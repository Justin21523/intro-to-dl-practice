{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型知識編輯 (Model Editing)\n",
    "\n",
    "本 notebook 對應李宏毅老師 2025 Spring ML HW8，探討如何精準修改 LLM 中儲存的知識。\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 理解知識在 LLM 中的儲存方式\n",
    "2. 學習 ROME（Rank-One Model Editing）方法\n",
    "3. 了解 MEMIT（Mass-Editing Memory In a Transformer）\n",
    "4. 實作簡單的知識編輯\n",
    "5. 評估編輯效果與副作用\n",
    "\n",
    "## 參考資源\n",
    "\n",
    "- [ROME Paper](https://arxiv.org/abs/2202.05262) - Locating and Editing Factual Associations in GPT\n",
    "- [MEMIT Paper](https://arxiv.org/abs/2210.07229) - Mass-Editing Memory in a Transformer\n",
    "- [EasyEdit](https://github.com/zjunlp/EasyEdit) - 知識編輯工具庫\n",
    "- [2025 Spring HW8](https://speech.ee.ntu.edu.tw/~hylee/ml/2025-spring.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 為什麼需要模型編輯？\n",
    "\n",
    "### 1.1 LLM 知識更新的挑戰\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                    LLM 知識更新的問題                                     │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  問題場景：                                                              │\n",
    "│  ─────────                                                              │\n",
    "│  • LLM 訓練資料有截止日期（knowledge cutoff）                             │\n",
    "│  • 世界知識會隨時間改變                                                   │\n",
    "│  • 模型可能學到錯誤資訊                                                   │\n",
    "│  • 需要移除有害或敏感資訊                                                 │\n",
    "│                                                                         │\n",
    "│  範例：                                                                  │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐       │\n",
    "│  │  Q: Who is the Prime Minister of UK?                        │       │\n",
    "│  │                                                             │       │\n",
    "│  │  舊模型: Boris Johnson (2019-2022)                          │       │\n",
    "│  │  需更新: Keir Starmer (2024-)                               │       │\n",
    "│  └─────────────────────────────────────────────────────────────┘       │\n",
    "│                                                                         │\n",
    "│  傳統方法的問題：                                                        │\n",
    "│  ─────────────────                                                      │\n",
    "│  1. 重新訓練：成本極高（數百萬美元）                                       │\n",
    "│  2. Fine-tuning：可能導致災難性遺忘                                       │\n",
    "│  3. RAG：只能補充，無法修正模型內部知識                                    │\n",
    "│                                                                         │\n",
    "│  模型編輯的目標：                                                        │\n",
    "│  ───────────────                                                        │\n",
    "│  精準修改特定知識，同時保持其他能力不變                                    │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 1.2 編輯目標的三個維度\n",
    "\n",
    "| 維度 | 定義 | 範例 |\n",
    "|------|------|------|\n",
    "| **Efficacy** | 編輯後正確回答目標問題 | Q: UK PM? → A: Keir Starmer |\n",
    "| **Generalization** | 能泛化到相關問題 | Q: Who leads UK? → A: Keir Starmer |\n",
    "| **Specificity** | 不影響不相關知識 | Q: France PM? → A: (保持不變) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from copy import deepcopy\n",
    "\n",
    "# 設定\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 知識在 Transformer 中的儲存\n",
    "\n",
    "### 2.1 知識定位 (Knowledge Localization)\n",
    "\n",
    "研究發現，事實知識主要儲存在 Transformer 的 **MLP 層**中。\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                   Transformer Layer 中的知識儲存                          │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  Transformer Block:                                                     │\n",
    "│  ┌─────────────────────────────────────────────────────────────────┐   │\n",
    "│  │                                                                 │   │\n",
    "│  │   Input x                                                       │   │\n",
    "│  │      │                                                          │   │\n",
    "│  │      ▼                                                          │   │\n",
    "│  │  ┌─────────────┐                                                │   │\n",
    "│  │  │  Attention  │ ← 主要處理 token 間的關係                        │   │\n",
    "│  │  │   (QKV)     │                                                │   │\n",
    "│  │  └──────┬──────┘                                                │   │\n",
    "│  │         │ + x (residual)                                        │   │\n",
    "│  │         ▼                                                       │   │\n",
    "│  │  ┌─────────────┐                                                │   │\n",
    "│  │  │    MLP      │ ← 主要儲存事實知識 ⭐                            │   │\n",
    "│  │  │  W_up, W_down                                                │   │\n",
    "│  │  └──────┬──────┘                                                │   │\n",
    "│  │         │ + (residual)                                          │   │\n",
    "│  │         ▼                                                       │   │\n",
    "│  │      Output                                                     │   │\n",
    "│  │                                                                 │   │\n",
    "│  └─────────────────────────────────────────────────────────────────┘   │\n",
    "│                                                                         │\n",
    "│  MLP 可以看作 Key-Value Memory:                                         │\n",
    "│  ──────────────────────────────                                         │\n",
    "│  MLP(x) = W_down · σ(W_up · x)                                         │\n",
    "│                                                                         │\n",
    "│  其中：                                                                  │\n",
    "│  • W_up 的列向量 = Keys（輸入模式）                                       │\n",
    "│  • W_down 的行向量 = Values（輸出表示）                                   │\n",
    "│  • σ = 激活函數                                                         │\n",
    "│                                                                         │\n",
    "│  知識編輯 = 修改特定的 Key-Value 對應關係                                 │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    簡化的 MLP 層，展示知識儲存概念\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, d_hidden: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_hidden = d_hidden\n",
    "        \n",
    "        # Up projection: d_model -> d_hidden\n",
    "        self.W_up = nn.Linear(d_model, d_hidden, bias=False)\n",
    "        \n",
    "        # Down projection: d_hidden -> d_model\n",
    "        self.W_down = nn.Linear(d_hidden, d_model, bias=False)\n",
    "        \n",
    "        # 激活函數\n",
    "        self.activation = nn.GELU()\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [batch, seq, d_model]\n",
    "        \"\"\"\n",
    "        hidden = self.activation(self.W_up(x))  # [batch, seq, d_hidden]\n",
    "        output = self.W_down(hidden)            # [batch, seq, d_model]\n",
    "        return output\n",
    "    \n",
    "    def get_key_value_interpretation(self):\n",
    "        \"\"\"\n",
    "        將 MLP 解釋為 Key-Value memory\n",
    "        \"\"\"\n",
    "        # Keys: W_up 的列向量 (d_hidden 個 keys，每個維度 d_model)\n",
    "        keys = self.W_up.weight.data  # [d_hidden, d_model]\n",
    "        \n",
    "        # Values: W_down 的行向量 (d_hidden 個 values，每個維度 d_model)\n",
    "        values = self.W_down.weight.data.T  # [d_hidden, d_model]\n",
    "        \n",
    "        return keys, values\n",
    "\n",
    "\n",
    "# 示範\n",
    "d_model = 128\n",
    "d_hidden = 512\n",
    "\n",
    "mlp = SimpleMLP(d_model, d_hidden)\n",
    "keys, values = mlp.get_key_value_interpretation()\n",
    "\n",
    "print(f\"MLP 維度: d_model={d_model}, d_hidden={d_hidden}\")\n",
    "print(f\"Keys shape: {keys.shape} (每個 key 是一個 {d_model} 維向量)\")\n",
    "print(f\"Values shape: {values.shape} (每個 value 是一個 {d_model} 維向量)\")\n",
    "print(f\"\\n可以把 MLP 看作 {d_hidden} 個 Key-Value pairs 的記憶體\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ROME: Rank-One Model Editing\n",
    "\n",
    "### 3.1 ROME 核心思想\n",
    "\n",
    "ROME 透過 **rank-one update** 來修改 MLP 的權重，使其輸出新的知識。\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                         ROME 方法概覽                                    │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  目標：將 (subject, relation) → old_object 改為 → new_object            │\n",
    "│  例如：(UK Prime Minister, is) → Boris Johnson → Keir Starmer          │\n",
    "│                                                                         │\n",
    "│  步驟：                                                                  │\n",
    "│  ──────                                                                 │\n",
    "│                                                                         │\n",
    "│  Step 1: 找到關鍵層 (Causal Tracing)                                    │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐       │\n",
    "│  │  透過因果介入實驗，找出哪一層對特定事實最重要                     │       │\n",
    "│  │  通常是中間層 (e.g., layer 15-20 in GPT-2 XL)                 │       │\n",
    "│  └─────────────────────────────────────────────────────────────┘       │\n",
    "│                                                                         │\n",
    "│  Step 2: 計算 key vector k*                                            │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐       │\n",
    "│  │  k* = average hidden state at subject's last token          │       │\n",
    "│  │       across different prompts containing the subject       │       │\n",
    "│  └─────────────────────────────────────────────────────────────┘       │\n",
    "│                                                                         │\n",
    "│  Step 3: 計算 target value v*                                          │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐       │\n",
    "│  │  找到使模型輸出新答案所需的 hidden state                       │       │\n",
    "│  │  v* = argmin ||h - v|| s.t. model outputs new_object       │       │\n",
    "│  └─────────────────────────────────────────────────────────────┘       │\n",
    "│                                                                         │\n",
    "│  Step 4: Rank-One Update                                               │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐       │\n",
    "│  │  W_new = W_old + Δ                                          │       │\n",
    "│  │  Δ = (v* - W_old·k*) · k*ᵀ · (C + k*·k*ᵀ)⁻¹               │       │\n",
    "│  │                                                             │       │\n",
    "│  │  其中 C 是統計矩陣，確保更新最小化對其他 key 的影響           │       │\n",
    "│  └─────────────────────────────────────────────────────────────┘       │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EditRequest:\n",
    "    \"\"\"知識編輯請求\"\"\"\n",
    "    subject: str          # 主詞 (e.g., \"UK Prime Minister\")\n",
    "    relation: str         # 關係 (e.g., \"is\")\n",
    "    old_object: str       # 舊答案 (e.g., \"Boris Johnson\")\n",
    "    new_object: str       # 新答案 (e.g., \"Keir Starmer\")\n",
    "    prompts: List[str]    # 用於生成 key 的 prompts\n",
    "\n",
    "\n",
    "def compute_rank_one_update(\n",
    "    W: torch.Tensor,        # 原始權重 [out_dim, in_dim]\n",
    "    k_star: torch.Tensor,   # 目標 key [in_dim]\n",
    "    v_star: torch.Tensor,   # 目標 value [out_dim]\n",
    "    C: torch.Tensor = None, # 統計矩陣 [in_dim, in_dim]\n",
    "    lambda_reg: float = 1e-4\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    計算 ROME 的 rank-one update\n",
    "    \n",
    "    W_new = W + (v* - W·k*) · k*ᵀ · (C + k*·k*ᵀ)⁻¹\n",
    "    \n",
    "    這個更新確保：\n",
    "    - W_new · k* ≈ v* (新知識)\n",
    "    - W_new · k ≈ W · k for k ≠ k* (保持其他知識)\n",
    "    \"\"\"\n",
    "    in_dim = W.shape[1]\n",
    "    \n",
    "    # 如果沒有提供 C，使用單位矩陣\n",
    "    if C is None:\n",
    "        C = torch.eye(in_dim, device=W.device) * lambda_reg\n",
    "    \n",
    "    # 確保維度正確\n",
    "    k_star = k_star.view(-1)  # [in_dim]\n",
    "    v_star = v_star.view(-1)  # [out_dim]\n",
    "    \n",
    "    # 計算殘差：v* - W·k*\n",
    "    residual = v_star - W @ k_star  # [out_dim]\n",
    "    \n",
    "    # 計算 (C + k*·k*ᵀ)⁻¹ · k*\n",
    "    k_outer = torch.outer(k_star, k_star)  # [in_dim, in_dim]\n",
    "    inv_term = torch.linalg.solve(C + k_outer, k_star)  # [in_dim]\n",
    "    \n",
    "    # Rank-one update: residual · inv_termᵀ\n",
    "    delta = torch.outer(residual, inv_term)  # [out_dim, in_dim]\n",
    "    \n",
    "    return delta\n",
    "\n",
    "\n",
    "# 示範 rank-one update\n",
    "print(\"Rank-One Update 示範：\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 創建一個簡單的線性層\n",
    "in_dim, out_dim = 10, 8\n",
    "W = torch.randn(out_dim, in_dim)\n",
    "\n",
    "# 目標：讓特定輸入 k_star 產生特定輸出 v_star\n",
    "k_star = torch.randn(in_dim)\n",
    "v_star = torch.randn(out_dim)\n",
    "\n",
    "print(f\"原始輸出 W @ k_star: {(W @ k_star)[:5]}...\")\n",
    "print(f\"目標輸出 v_star: {v_star[:5]}...\")\n",
    "\n",
    "# 計算更新\n",
    "delta = compute_rank_one_update(W, k_star, v_star)\n",
    "W_new = W + delta\n",
    "\n",
    "print(f\"\\n更新後輸出 W_new @ k_star: {(W_new @ k_star)[:5]}...\")\n",
    "print(f\"誤差: {torch.norm(W_new @ k_star - v_star).item():.6f}\")\n",
    "\n",
    "# 檢查對其他 key 的影響\n",
    "k_other = torch.randn(in_dim)\n",
    "print(f\"\\n對其他 key 的影響:\")\n",
    "print(f\"原始: {(W @ k_other)[:5]}...\")\n",
    "print(f\"更新後: {(W_new @ k_other)[:5]}...\")\n",
    "print(f\"差異: {torch.norm(W_new @ k_other - W @ k_other).item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 簡化版知識編輯實作\n",
    "\n",
    "### 4.1 建立簡單的知識模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleKnowledgeModel(nn.Module):\n",
    "    \"\"\"\n",
    "    簡化的知識模型，用於展示編輯概念\n",
    "    \n",
    "    模型學習 (subject_embedding) → (object_embedding) 的映射\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab_size: int = 100,\n",
    "                 embed_dim: int = 64,\n",
    "                 hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Knowledge MLP (這是我們要編輯的層)\n",
    "        self.knowledge_mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output = nn.Linear(embed_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, subject_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        subject_ids: [batch_size]\n",
    "        Returns: logits [batch_size, vocab_size]\n",
    "        \"\"\"\n",
    "        # Embed subject\n",
    "        x = self.embedding(subject_ids)  # [batch, embed_dim]\n",
    "        \n",
    "        # Apply knowledge MLP\n",
    "        x = self.knowledge_mlp(x)  # [batch, embed_dim]\n",
    "        \n",
    "        # Output logits\n",
    "        logits = self.output(x)  # [batch, vocab_size]\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def predict(self, subject_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"預測 object id\"\"\"\n",
    "        logits = self.forward(subject_ids)\n",
    "        return logits.argmax(dim=-1)\n",
    "    \n",
    "    def get_hidden(self, subject_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"取得 MLP 輸入的 hidden state\"\"\"\n",
    "        return self.embedding(subject_ids)\n",
    "\n",
    "\n",
    "# 建立模型\n",
    "vocab_size = 100\n",
    "model = SimpleKnowledgeModel(vocab_size=vocab_size).to(device)\n",
    "print(f\"模型參數量: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立知識資料集\n",
    "knowledge_facts = {\n",
    "    # subject_id: object_id\n",
    "    0: 50,   # \"UK_PM\" -> \"Boris_Johnson\"\n",
    "    1: 51,   # \"France_PM\" -> \"Macron\"\n",
    "    2: 52,   # \"Germany_Chancellor\" -> \"Scholz\"\n",
    "    3: 53,   # \"US_President\" -> \"Biden\"\n",
    "    4: 54,   # \"Japan_PM\" -> \"Kishida\"\n",
    "}\n",
    "\n",
    "# 訓練模型學習這些知識\n",
    "def train_knowledge_model(model, facts, epochs=500, lr=0.01):\n",
    "    \"\"\"訓練模型記住知識\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    subjects = torch.tensor(list(facts.keys())).to(device)\n",
    "    objects = torch.tensor(list(facts.values())).to(device)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(subjects)\n",
    "        loss = criterion(logits, objects)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            acc = (model.predict(subjects) == objects).float().mean()\n",
    "            print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}, Accuracy = {acc.item():.2%}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "print(\"訓練模型記住知識...\")\n",
    "losses = train_knowledge_model(model, knowledge_facts)\n",
    "\n",
    "# 驗證\n",
    "print(\"\\n驗證知識：\")\n",
    "for subject, expected_object in knowledge_facts.items():\n",
    "    predicted = model.predict(torch.tensor([subject]).to(device)).item()\n",
    "    print(f\"Subject {subject} -> Predicted: {predicted}, Expected: {expected_object}, \"\n",
    "          f\"{'✓' if predicted == expected_object else '✗'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeEditor:\n",
    "    \"\"\"\n",
    "    簡化版知識編輯器\n",
    "    \"\"\"\n",
    "    def __init__(self, model: SimpleKnowledgeModel):\n",
    "        self.model = model\n",
    "        self.original_state = deepcopy(model.state_dict())\n",
    "    \n",
    "    def edit(self, \n",
    "             subject_id: int, \n",
    "             new_object_id: int,\n",
    "             method: str = \"fine_tune\") -> Dict:\n",
    "        \"\"\"\n",
    "        編輯知識\n",
    "        \n",
    "        Args:\n",
    "            subject_id: 要編輯的 subject\n",
    "            new_object_id: 新的 object\n",
    "            method: 編輯方法 ('fine_tune' 或 'rank_one')\n",
    "        \"\"\"\n",
    "        if method == \"fine_tune\":\n",
    "            return self._edit_finetune(subject_id, new_object_id)\n",
    "        elif method == \"rank_one\":\n",
    "            return self._edit_rank_one(subject_id, new_object_id)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    def _edit_finetune(self, subject_id: int, new_object_id: int, \n",
    "                       steps: int = 100, lr: float = 0.01) -> Dict:\n",
    "        \"\"\"\n",
    "        使用 fine-tuning 編輯（基線方法）\n",
    "        問題：可能導致災難性遺忘\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        subject = torch.tensor([subject_id]).to(device)\n",
    "        target = torch.tensor([new_object_id]).to(device)\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            optimizer.zero_grad()\n",
    "            logits = self.model(subject)\n",
    "            loss = criterion(logits, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return {\"method\": \"fine_tune\", \"steps\": steps}\n",
    "    \n",
    "    def _edit_rank_one(self, subject_id: int, new_object_id: int) -> Dict:\n",
    "        \"\"\"\n",
    "        使用 rank-one update 編輯（類似 ROME）\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # 取得 subject 的 hidden representation (作為 key)\n",
    "            subject = torch.tensor([subject_id]).to(device)\n",
    "            k_star = self.model.embedding(subject).squeeze()  # [embed_dim]\n",
    "            \n",
    "            # 取得目標 object 的 embedding (作為 target output)\n",
    "            # 注意：這是簡化版，實際 ROME 會用更複雜的方法計算 v*\n",
    "            target_embedding = self.model.output.weight[new_object_id]  # [embed_dim]\n",
    "            v_star = target_embedding\n",
    "            \n",
    "            # 編輯 knowledge_mlp 的最後一層\n",
    "            # 取得最後一個 Linear 層\n",
    "            last_linear = self.model.knowledge_mlp[-1]\n",
    "            W = last_linear.weight.data  # [embed_dim, hidden_dim]\n",
    "            \n",
    "            # 取得進入最後一層的 hidden state\n",
    "            x = self.model.embedding(subject)\n",
    "            for layer in list(self.model.knowledge_mlp)[:-1]:\n",
    "                x = layer(x)\n",
    "            h = x.squeeze()  # [hidden_dim] - 這是實際的 key\n",
    "            \n",
    "            # 計算 rank-one update\n",
    "            delta = compute_rank_one_update(W, h, v_star)\n",
    "            \n",
    "            # 應用更新\n",
    "            last_linear.weight.data = W + delta\n",
    "        \n",
    "        return {\"method\": \"rank_one\"}\n",
    "    \n",
    "    def restore(self):\n",
    "        \"\"\"恢復原始模型\"\"\"\n",
    "        self.model.load_state_dict(deepcopy(self.original_state))\n",
    "\n",
    "\n",
    "# 測試知識編輯\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"知識編輯測試\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "editor = KnowledgeEditor(model)\n",
    "\n",
    "# 編輯前\n",
    "print(\"\\n編輯前預測：\")\n",
    "for subject, expected in knowledge_facts.items():\n",
    "    pred = model.predict(torch.tensor([subject]).to(device)).item()\n",
    "    print(f\"Subject {subject}: {pred}\")\n",
    "\n",
    "# 執行編輯：將 subject 0 的答案從 50 改為 60\n",
    "print(\"\\n執行編輯：Subject 0: 50 -> 60\")\n",
    "edit_result = editor.edit(subject_id=0, new_object_id=60, method=\"rank_one\")\n",
    "print(f\"編輯方法: {edit_result['method']}\")\n",
    "\n",
    "# 編輯後\n",
    "print(\"\\n編輯後預測：\")\n",
    "for subject, expected in knowledge_facts.items():\n",
    "    pred = model.predict(torch.tensor([subject]).to(device)).item()\n",
    "    if subject == 0:\n",
    "        status = \"✓ (已編輯)\" if pred == 60 else \"✗\"\n",
    "    else:\n",
    "        status = \"✓ (保持)\" if pred == expected else \"✗ (受影響)\"\n",
    "    print(f\"Subject {subject}: {pred} {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MEMIT: 批量知識編輯\n",
    "\n",
    "### 5.1 MEMIT 概念\n",
    "\n",
    "MEMIT 擴展了 ROME，可以同時編輯多條知識。\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                         MEMIT vs ROME                                    │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  ROME:                                                                  │\n",
    "│  • 一次編輯一條知識                                                      │\n",
    "│  • 修改單一層                                                           │\n",
    "│  • 適合小規模編輯                                                        │\n",
    "│                                                                         │\n",
    "│  MEMIT:                                                                 │\n",
    "│  • 一次編輯多條知識                                                      │\n",
    "│  • 將編輯分散到多層                                                      │\n",
    "│  • 更好的 scalability                                                   │\n",
    "│                                                                         │\n",
    "│  MEMIT 更新公式：                                                        │\n",
    "│  ───────────────                                                        │\n",
    "│  對於 N 條編輯，在 L 層分散更新：                                          │\n",
    "│                                                                         │\n",
    "│  W_new^l = W_old^l + R · K^T · (C + K·K^T)^{-1}                        │\n",
    "│                                                                         │\n",
    "│  其中：                                                                  │\n",
    "│  • K = [k_1, k_2, ..., k_N] 是所有編輯的 key 向量                        │\n",
    "│  • R = [r_1, r_2, ..., r_N] 是對應的殘差向量                             │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_rank_update(\n",
    "    W: torch.Tensor,          # [out_dim, in_dim]\n",
    "    K: torch.Tensor,          # [num_edits, in_dim] - 多個 keys\n",
    "    V: torch.Tensor,          # [num_edits, out_dim] - 多個 target values\n",
    "    lambda_reg: float = 1e-4\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    批量 rank 更新（簡化版 MEMIT）\n",
    "    \n",
    "    W_new = W + (V - W·K^T)^T · K · (λI + K^T·K)^{-1}\n",
    "    \"\"\"\n",
    "    num_edits, in_dim = K.shape\n",
    "    out_dim = W.shape[0]\n",
    "    \n",
    "    # 計算當前輸出\n",
    "    current_output = W @ K.T  # [out_dim, num_edits]\n",
    "    \n",
    "    # 計算殘差\n",
    "    residual = V.T - current_output  # [out_dim, num_edits]\n",
    "    \n",
    "    # 計算 (λI + K^T·K)^{-1}\n",
    "    KtK = K.T @ K  # [in_dim, in_dim]\n",
    "    reg_matrix = lambda_reg * torch.eye(in_dim, device=W.device)\n",
    "    inv_term = torch.linalg.inv(reg_matrix + KtK)  # [in_dim, in_dim]\n",
    "    \n",
    "    # 計算更新\n",
    "    delta = residual @ K @ inv_term  # [out_dim, in_dim]\n",
    "    \n",
    "    return delta\n",
    "\n",
    "\n",
    "# 示範批量編輯\n",
    "print(\"批量編輯示範：\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 創建權重矩陣\n",
    "in_dim, out_dim = 10, 8\n",
    "W = torch.randn(out_dim, in_dim)\n",
    "\n",
    "# 3 個編輯請求\n",
    "K = torch.randn(3, in_dim)  # 3 個 keys\n",
    "V = torch.randn(3, out_dim)  # 3 個 target values\n",
    "\n",
    "print(f\"權重矩陣: {W.shape}\")\n",
    "print(f\"Keys: {K.shape}\")\n",
    "print(f\"Target Values: {V.shape}\")\n",
    "\n",
    "# 計算更新\n",
    "delta = batch_rank_update(W, K, V)\n",
    "W_new = W + delta\n",
    "\n",
    "# 驗證\n",
    "print(f\"\\n驗證編輯效果：\")\n",
    "for i in range(3):\n",
    "    original = W @ K[i]\n",
    "    edited = W_new @ K[i]\n",
    "    target = V[i]\n",
    "    error = torch.norm(edited - target).item()\n",
    "    print(f\"Edit {i+1}: Error = {error:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 編輯效果評估\n",
    "\n",
    "### 6.1 評估指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EditEvaluator:\n",
    "    \"\"\"\n",
    "    知識編輯效果評估器\n",
    "    \"\"\"\n",
    "    def __init__(self, model, editor, original_facts: Dict):\n",
    "        self.model = model\n",
    "        self.editor = editor\n",
    "        self.original_facts = original_facts\n",
    "    \n",
    "    def evaluate_edit(\n",
    "        self, \n",
    "        subject_id: int, \n",
    "        new_object_id: int,\n",
    "        method: str = \"rank_one\"\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        評估單次編輯的效果\n",
    "        \n",
    "        Returns:\n",
    "            - efficacy: 編輯是否成功\n",
    "            - specificity: 對其他知識的影響\n",
    "        \"\"\"\n",
    "        # 恢復原始模型\n",
    "        self.editor.restore()\n",
    "        \n",
    "        # 執行編輯\n",
    "        self.editor.edit(subject_id, new_object_id, method=method)\n",
    "        \n",
    "        results = {\n",
    "            \"method\": method,\n",
    "            \"subject\": subject_id,\n",
    "            \"old_object\": self.original_facts[subject_id],\n",
    "            \"new_object\": new_object_id,\n",
    "        }\n",
    "        \n",
    "        # 1. Efficacy: 編輯的 subject 是否正確預測新 object\n",
    "        pred = self.model.predict(torch.tensor([subject_id]).to(device)).item()\n",
    "        results[\"efficacy\"] = pred == new_object_id\n",
    "        results[\"predicted\"] = pred\n",
    "        \n",
    "        # 2. Specificity: 其他 subject 是否保持正確\n",
    "        other_correct = 0\n",
    "        other_total = 0\n",
    "        other_results = []\n",
    "        \n",
    "        for s, expected_o in self.original_facts.items():\n",
    "            if s != subject_id:\n",
    "                pred = self.model.predict(torch.tensor([s]).to(device)).item()\n",
    "                is_correct = pred == expected_o\n",
    "                other_correct += int(is_correct)\n",
    "                other_total += 1\n",
    "                other_results.append({\n",
    "                    \"subject\": s,\n",
    "                    \"expected\": expected_o,\n",
    "                    \"predicted\": pred,\n",
    "                    \"correct\": is_correct\n",
    "                })\n",
    "        \n",
    "        results[\"specificity\"] = other_correct / other_total if other_total > 0 else 1.0\n",
    "        results[\"other_subjects\"] = other_results\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# 重新訓練模型\n",
    "print(\"重新訓練模型...\")\n",
    "model = SimpleKnowledgeModel(vocab_size=vocab_size).to(device)\n",
    "train_knowledge_model(model, knowledge_facts, epochs=500)\n",
    "\n",
    "# 評估不同編輯方法\n",
    "editor = KnowledgeEditor(model)\n",
    "evaluator = EditEvaluator(model, editor, knowledge_facts)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"編輯方法比較\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for method in [\"fine_tune\", \"rank_one\"]:\n",
    "    print(f\"\\n--- {method.upper()} ---\")\n",
    "    results = evaluator.evaluate_edit(\n",
    "        subject_id=0, \n",
    "        new_object_id=60, \n",
    "        method=method\n",
    "    )\n",
    "    \n",
    "    print(f\"Efficacy: {'✓' if results['efficacy'] else '✗'} \"\n",
    "          f\"(Predicted: {results['predicted']}, Target: {results['new_object']})\")\n",
    "    print(f\"Specificity: {results['specificity']:.2%}\")\n",
    "    \n",
    "    print(\"Other subjects:\")\n",
    "    for other in results['other_subjects']:\n",
    "        status = '✓' if other['correct'] else '✗'\n",
    "        print(f\"  Subject {other['subject']}: {other['predicted']} \"\n",
    "              f\"(expected {other['expected']}) {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化編輯效果\n",
    "def visualize_edit_comparison():\n",
    "    \"\"\"比較不同編輯方法的效果\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    methods = ['Fine-tune', 'Rank-One (ROME-like)']\n",
    "    efficacy_scores = [0.9, 0.95]  # 模擬數據\n",
    "    specificity_scores = [0.6, 0.85]\n",
    "    \n",
    "    # 1. Efficacy vs Specificity\n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[0].bar(x - width/2, efficacy_scores, width, label='Efficacy', color='steelblue')\n",
    "    bars2 = axes[0].bar(x + width/2, specificity_scores, width, label='Specificity', color='coral')\n",
    "    \n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].set_title('Edit Quality Comparison')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(methods)\n",
    "    axes[0].legend()\n",
    "    axes[0].set_ylim([0, 1.1])\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 加上數值標籤\n",
    "    for bar, val in zip(bars1, efficacy_scores):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                     f'{val:.0%}', ha='center', va='bottom')\n",
    "    for bar, val in zip(bars2, specificity_scores):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                     f'{val:.0%}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Number of edits vs Performance\n",
    "    num_edits = [1, 5, 10, 20, 50, 100]\n",
    "    finetune_perf = [0.9, 0.7, 0.5, 0.3, 0.15, 0.1]  # Fine-tune 隨編輯增加而下降\n",
    "    rome_perf = [0.95, 0.92, 0.88, 0.82, 0.75, 0.68]  # ROME 更穩定\n",
    "    memit_perf = [0.95, 0.93, 0.91, 0.88, 0.85, 0.82]  # MEMIT 最穩定\n",
    "    \n",
    "    axes[1].plot(num_edits, finetune_perf, 'o-', label='Fine-tune', linewidth=2)\n",
    "    axes[1].plot(num_edits, rome_perf, 's-', label='ROME', linewidth=2)\n",
    "    axes[1].plot(num_edits, memit_perf, '^-', label='MEMIT', linewidth=2)\n",
    "    \n",
    "    axes[1].set_xlabel('Number of Edits')\n",
    "    axes[1].set_ylabel('Overall Performance')\n",
    "    axes[1].set_title('Scalability of Edit Methods')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xscale('log')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n關鍵觀察：\")\n",
    "    print(\"1. Fine-tuning 的 efficacy 高但 specificity 低（災難性遺忘）\")\n",
    "    print(\"2. ROME 在單次編輯時表現優異\")\n",
    "    print(\"3. MEMIT 在大量編輯時仍能保持較好效果\")\n",
    "\n",
    "visualize_edit_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 使用 EasyEdit 進行實作\n",
    "\n",
    "### 7.1 EasyEdit 簡介\n",
    "\n",
    "EasyEdit 是一個統一的知識編輯框架，支援多種編輯方法。\n",
    "\n",
    "```python\n",
    "# EasyEdit 使用範例（概念展示）\n",
    "# 安裝：pip install easyeditor\n",
    "\n",
    "from easyeditor import BaseEditor, ROMEHyperParams\n",
    "\n",
    "# 載入預訓練模型\n",
    "hparams = ROMEHyperParams.from_hparams('hparams/ROME/gpt2-xl.yaml')\n",
    "editor = BaseEditor.from_hparams(hparams)\n",
    "\n",
    "# 定義編輯\n",
    "prompts = ['The Prime Minister of UK is']\n",
    "ground_truth = ['Keir Starmer']\n",
    "target_new = ['Keir Starmer']\n",
    "subject = ['UK']\n",
    "\n",
    "# 執行編輯\n",
    "metrics, edited_model, _ = editor.edit(\n",
    "    prompts=prompts,\n",
    "    ground_truth=ground_truth,\n",
    "    target_new=target_new,\n",
    "    subject=subject,\n",
    "    keep_original_weight=True\n",
    ")\n",
    "\n",
    "print(metrics)  # {'efficacy': 1.0, 'generalization': 0.95, ...}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模擬 EasyEdit 風格的 API\n",
    "class SimpleEditor:\n",
    "    \"\"\"\n",
    "    簡化版知識編輯器（模擬 EasyEdit API）\n",
    "    \"\"\"\n",
    "    def __init__(self, model, method: str = \"ROME\"):\n",
    "        self.model = model\n",
    "        self.method = method\n",
    "        self.original_state = deepcopy(model.state_dict())\n",
    "        \n",
    "    def edit(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        target_new: List[str],\n",
    "        subject: List[str],\n",
    "        keep_original_weight: bool = True\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        執行知識編輯\n",
    "        \n",
    "        Args:\n",
    "            prompts: 輸入 prompts\n",
    "            target_new: 新的目標答案\n",
    "            subject: 主詞\n",
    "            keep_original_weight: 是否保留原始權重（用於比較）\n",
    "        \n",
    "        Returns:\n",
    "            metrics: 編輯效果指標\n",
    "        \"\"\"\n",
    "        print(f\"\\nEditing with method: {self.method}\")\n",
    "        print(f\"Number of edits: {len(prompts)}\")\n",
    "        \n",
    "        for i, (prompt, target, subj) in enumerate(zip(prompts, target_new, subject)):\n",
    "            print(f\"  Edit {i+1}: '{prompt}' -> '{target}' (subject: {subj})\")\n",
    "        \n",
    "        # 模擬編輯過程\n",
    "        metrics = {\n",
    "            \"efficacy\": 0.95,\n",
    "            \"generalization\": 0.88,\n",
    "            \"specificity\": 0.92,\n",
    "            \"fluency\": 0.97,\n",
    "            \"method\": self.method\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def restore(self):\n",
    "        \"\"\"恢復原始權重\"\"\"\n",
    "        self.model.load_state_dict(deepcopy(self.original_state))\n",
    "\n",
    "\n",
    "# 使用範例\n",
    "print(\"模擬 EasyEdit 使用流程：\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "editor = SimpleEditor(model, method=\"ROME\")\n",
    "\n",
    "metrics = editor.edit(\n",
    "    prompts=[\"The Prime Minister of UK is\"],\n",
    "    target_new=[\"Keir Starmer\"],\n",
    "    subject=[\"UK Prime Minister\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nEdit Metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2%}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 練習題\n",
    "\n",
    "### 練習 1：實作 Causal Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1：實作簡化版 Causal Tracing\n",
    "def causal_trace(\n",
    "    model: nn.Module,\n",
    "    subject_id: int,\n",
    "    clean_answer: int,\n",
    "    corrupted_subject_id: int\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    TODO: 實作 Causal Tracing 來找出哪一層對知識最重要\n",
    "    \n",
    "    步驟：\n",
    "    1. 用 clean input 得到正確答案的機率\n",
    "    2. 用 corrupted input 得到機率（應該下降）\n",
    "    3. 逐層恢復 clean 的 activation，看哪層恢復效果最大\n",
    "    \n",
    "    Args:\n",
    "        model: 要分析的模型\n",
    "        subject_id: 原始 subject\n",
    "        clean_answer: 正確答案\n",
    "        corrupted_subject_id: 被干擾的 subject\n",
    "    \n",
    "    Returns:\n",
    "        每一層的「重要性分數」\n",
    "    \"\"\"\n",
    "    # 提示：\n",
    "    # 1. 使用 forward hook 來擷取中間 activation\n",
    "    # 2. 計算恢復特定層後的輸出變化\n",
    "    pass\n",
    "\n",
    "print(\"練習 1：實作 causal_trace 函數\")\n",
    "print(\"這個函數可以幫助我們找出模型中儲存特定知識的關鍵層\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：多層編輯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2：實作將編輯分散到多層的方法\n",
    "class MultiLayerEditor:\n",
    "    \"\"\"\n",
    "    TODO: 實作將編輯分散到多層的編輯器（類似 MEMIT）\n",
    "    \n",
    "    想法：\n",
    "    - 不要把整個編輯放在單一層\n",
    "    - 將 target value 分解，分散到多層的小更新\n",
    "    - 這樣可以減少對單一層的干擾\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Module, layers_to_edit: List[int]):\n",
    "        self.model = model\n",
    "        self.layers_to_edit = layers_to_edit\n",
    "    \n",
    "    def edit(self, subject_id: int, target_value: torch.Tensor) -> Dict:\n",
    "        \"\"\"\n",
    "        TODO: 將編輯分散到多層\n",
    "        \n",
    "        提示：\n",
    "        1. 將 target_value 分解為 len(layers_to_edit) 個部分\n",
    "        2. 對每一層應用較小的更新\n",
    "        3. 返回編輯的統計資訊\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "print(\"練習 2：實作 MultiLayerEditor\")\n",
    "print(\"這種方法可以提高大量編輯時的穩定性\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3：編輯效果可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 3：分析編輯對模型權重的影響\n",
    "def analyze_weight_change(\n",
    "    original_weights: Dict[str, torch.Tensor],\n",
    "    edited_weights: Dict[str, torch.Tensor]\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    TODO: 分析編輯前後權重的變化\n",
    "    \n",
    "    分析項目：\n",
    "    1. 每一層權重的 L2 變化量\n",
    "    2. 變化最大的層\n",
    "    3. 權重變化的分布（直方圖）\n",
    "    \n",
    "    這有助於理解編輯的「代價」和「範圍」\n",
    "    \"\"\"\n",
    "    # 提示：\n",
    "    # 1. 遍歷所有層的權重\n",
    "    # 2. 計算差異的各種統計量\n",
    "    # 3. 繪製視覺化圖表\n",
    "    pass\n",
    "\n",
    "print(\"練習 3：實作 analyze_weight_change 函數\")\n",
    "print(\"這可以幫助我們理解不同編輯方法的影響範圍\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 總結\n",
    "\n",
    "### 9.1 模型編輯方法總覽\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                     知識編輯方法比較                                      │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  方法          │ 優點                    │ 缺點                         │\n",
    "│  ───────────────────────────────────────────────────────────────────── │\n",
    "│  Fine-tuning  │ 簡單直接                │ 災難性遺忘                    │\n",
    "│               │ 不需要特殊實作          │ 需要大量資料                  │\n",
    "│  ───────────────────────────────────────────────────────────────────── │\n",
    "│  ROME         │ 精準編輯單一知識         │ 一次只能編輯一條              │\n",
    "│               │ 保持其他知識             │ 需要找到關鍵層                │\n",
    "│  ───────────────────────────────────────────────────────────────────── │\n",
    "│  MEMIT        │ 可批量編輯               │ 計算較複雜                    │\n",
    "│               │ 擴展性好                 │ 需要更多記憶體                │\n",
    "│  ───────────────────────────────────────────────────────────────────── │\n",
    "│  MEND         │ 學習如何編輯             │ 需要訓練 hypernetwork         │\n",
    "│               │ 快速適應                 │ 泛化能力有限                  │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 9.2 實際應用建議\n",
    "\n",
    "1. **小規模編輯**（< 10 條）：ROME\n",
    "2. **中等規模編輯**（10-100 條）：MEMIT\n",
    "3. **頻繁更新場景**：考慮 RAG 結合編輯\n",
    "4. **安全性要求高**：編輯後要仔細評估副作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"模型知識編輯 - 學習完成！\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n你已經學會：\")\n",
    "print(\"✓ 理解知識在 Transformer 中的儲存方式\")\n",
    "print(\"✓ ROME 的核心原理（Rank-One Update）\")\n",
    "print(\"✓ MEMIT 的批量編輯方法\")\n",
    "print(\"✓ 編輯效果的評估指標\")\n",
    "print(\"✓ 實作簡單的知識編輯器\")\n",
    "print(\"\\n下一步學習建議：\")\n",
    "print(\"1. 使用 EasyEdit 在真實 LLM 上實驗\")\n",
    "print(\"2. 研究 Causal Tracing 的細節\")\n",
    "print(\"3. 探索編輯的可逆性和持久性\")\n",
    "print(\"4. 了解編輯與對齊的關係\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
