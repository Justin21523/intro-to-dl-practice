{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è¿´æ­¸åˆ†æ (Regression)\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "- ç†è§£è¿´æ­¸å•é¡Œçš„å®šç¾©èˆ‡æ‡‰ç”¨\n",
    "- æŒæ¡ç‰¹å¾µå·¥ç¨‹æŠ€å·§\n",
    "- å­¸æœƒæ¨¡å‹é¸æ“‡èˆ‡è¶…åƒæ•¸èª¿å„ª\n",
    "- å¯¦ä½œ COVID-19 ç¢ºè¨ºæ•¸é æ¸¬ï¼ˆæå®æ¯… HW1ï¼‰\n",
    "\n",
    "## å°æ‡‰èª²ç¨‹\n",
    "- [æå®æ¯… ML 2021 - Lecture 1: Regression](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php)\n",
    "- [HW1: COVID-19 Cases Prediction](https://github.com/ga642381/ML2021-Spring/tree/main/HW01)\n",
    "\n",
    "## è¿´æ­¸å•é¡Œæ¦‚è¿°\n",
    "\n",
    "```\n",
    "è¿´æ­¸ vs åˆ†é¡\n",
    "â”œâ”€â”€ è¿´æ­¸ (Regression): è¼¸å‡ºé€£çºŒå€¼\n",
    "â”‚   ä¾‹: æˆ¿åƒ¹é æ¸¬ã€æº«åº¦é æ¸¬ã€éŠ·é‡é æ¸¬\n",
    "â”‚\n",
    "â””â”€â”€ åˆ†é¡ (Classification): è¼¸å‡ºé›¢æ•£é¡åˆ¥\n",
    "    ä¾‹: åƒåœ¾éƒµä»¶æª¢æ¸¬ã€åœ–ç‰‡åˆ†é¡\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "# å›ºå®šéš¨æ©Ÿç¨®å­\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: ç·šæ€§è¿´æ­¸åŸºç¤\n",
    "\n",
    "### æ•¸å­¸è¡¨ç¤º\n",
    "\n",
    "ç·šæ€§è¿´æ­¸æ¨¡å‹ï¼š\n",
    "$$\\hat{y} = w_1 x_1 + w_2 x_2 + ... + w_n x_n + b = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "\n",
    "æå¤±å‡½æ•¸ï¼ˆMSEï¼‰ï¼š\n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== å¾é›¶å¯¦ç¾ç·šæ€§è¿´æ­¸ ==========\n",
    "\n",
    "# ç”Ÿæˆç¯„ä¾‹è³‡æ–™\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 1) * 2\n",
    "y = 3 * X + 2 + np.random.randn(100, 1) * 0.5  # y = 3x + 2 + noise\n",
    "\n",
    "# è½‰ç‚º Tensor\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "# åˆå§‹åŒ–åƒæ•¸\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# è¨“ç·´\n",
    "lr = 0.01\n",
    "epochs = 100\n",
    "\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    # å‰å‘å‚³æ’­\n",
    "    y_pred = X_tensor * w + b\n",
    "    \n",
    "    # è¨ˆç®— MSE æå¤±\n",
    "    loss = ((y_pred - y_tensor) ** 2).mean()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # åå‘å‚³æ’­\n",
    "    loss.backward()\n",
    "    \n",
    "    # æ›´æ–°åƒæ•¸ï¼ˆæ‰‹å‹•æ¢¯åº¦ä¸‹é™ï¼‰\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        b -= lr * b.grad\n",
    "        \n",
    "        # æ¸…é›¶æ¢¯åº¦\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()\n",
    "\n",
    "print(f\"å­¸ç¿’åˆ°çš„åƒæ•¸: w = {w.item():.4f}, b = {b.item():.4f}\")\n",
    "print(f\"çœŸå¯¦åƒæ•¸: w = 3, b = 2\")\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].scatter(X, y, alpha=0.5, label='è³‡æ–™')\n",
    "axes[0].plot(X, X_tensor.detach().numpy() * w.item() + b.item(), 'r-', label='é æ¸¬ç·š')\n",
    "axes[0].set_xlabel('X')\n",
    "axes[0].set_ylabel('y')\n",
    "axes[0].legend()\n",
    "axes[0].set_title('ç·šæ€§è¿´æ­¸æ“¬åˆ')\n",
    "\n",
    "axes[1].plot(losses)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MSE Loss')\n",
    "axes[1].set_title('è¨“ç·´æå¤±')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: æ·±åº¦ç¥ç¶“ç¶²è·¯è¿´æ­¸\n",
    "\n",
    "å°æ–¼è¤‡é›œçš„éç·šæ€§é—œä¿‚ï¼Œæˆ‘å€‘ä½¿ç”¨æ·±åº¦ç¥ç¶“ç¶²è·¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DNN è¿´æ­¸æ¨¡å‹ ==========\n",
    "\n",
    "class DNNRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    æ·±åº¦ç¥ç¶“ç¶²è·¯è¿´æ­¸æ¨¡å‹\n",
    "    \n",
    "    é©ç”¨æ–¼ COVID-19 é æ¸¬ç­‰è¤‡é›œè¿´æ­¸ä»»å‹™\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[64, 32, 16], dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # è¼¸å‡ºå±¤ï¼ˆè¿´æ­¸è¼¸å‡º 1 å€‹å€¼ï¼‰\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)\n",
    "\n",
    "# æ¸¬è©¦æ¨¡å‹\n",
    "model = DNNRegressor(input_dim=10, hidden_dims=[64, 32, 16])\n",
    "print(model)\n",
    "\n",
    "x = torch.randn(4, 10)\n",
    "out = model(x)\n",
    "print(f\"\\nè¼¸å…¥: {x.shape}\")\n",
    "print(f\"è¼¸å‡º: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: COVID-19 é æ¸¬å¯¦æˆ°ï¼ˆæå®æ¯… HW1ï¼‰\n",
    "\n",
    "### ä»»å‹™èªªæ˜\n",
    "\n",
    "æ ¹æ“šå‰å¹¾å¤©çš„ COVID-19 ç›¸é—œç‰¹å¾µï¼Œé æ¸¬æœªä¾†çš„ç¢ºè¨ºæ•¸ã€‚\n",
    "\n",
    "**ç‰¹å¾µåŒ…æ‹¬**ï¼š\n",
    "- å„å·çš„ COVID ç›¸é—œçµ±è¨ˆï¼ˆç¢ºè¨ºã€æ­»äº¡ã€æª¢æ¸¬é™½æ€§ç‡ç­‰ï¼‰\n",
    "- äººå£çµ±è¨ˆç‰¹å¾µ\n",
    "- è¡Œç‚ºæŒ‡æ¨™ï¼ˆé…æˆ´å£ç½©æ¯”ä¾‹ç­‰ï¼‰\n",
    "\n",
    "**ç›®æ¨™**ï¼šé æ¸¬ç¬¬ N+1 å¤©çš„ç¢ºè¨ºæ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ¨¡æ“¬ COVID-19 è³‡æ–™é›† ==========\n",
    "\n",
    "def generate_covid_like_data(n_samples=3000, n_features=93, n_days=5):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆé¡ä¼¼ COVID-19 HW1 çš„è³‡æ–™\n",
    "    \n",
    "    å¯¦éš› HW1 è³‡æ–™çµæ§‹:\n",
    "    - æ¯å€‹æ¨£æœ¬æœ‰ 93 å€‹ç‰¹å¾µ\n",
    "    - åŒ…å« 5 å¤©çš„æ­·å²æ•¸æ“š\n",
    "    - ç›®æ¨™æ˜¯é æ¸¬ä¸‹ä¸€å¤©çš„ç¢ºè¨ºæ•¸\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # æ¨¡æ“¬ç‰¹å¾µ\n",
    "    # ç‰¹å¾µé¡å‹: å·ç·¨ç¢¼(one-hot), ç¢ºè¨ºæ•¸, æ­»äº¡æ•¸, é™½æ€§ç‡ç­‰\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # æ¨¡æ“¬ç›®æ¨™ï¼ˆä½¿ç”¨éƒ¨åˆ†ç‰¹å¾µçš„éç·šæ€§çµ„åˆï¼‰\n",
    "    # å¯¦éš›ä¸Šç¢ºè¨ºæ•¸èˆ‡å‰å¹¾å¤©çš„ç¢ºè¨ºæ•¸é«˜åº¦ç›¸é—œ\n",
    "    y = (\n",
    "        0.5 * X[:, 0] +\n",
    "        0.3 * X[:, 1] ** 2 +\n",
    "        0.2 * np.sin(X[:, 2]) +\n",
    "        0.1 * X[:, 3] * X[:, 4] +\n",
    "        np.random.randn(n_samples) * 0.1\n",
    "    )\n",
    "    \n",
    "    return X.astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "# ç”Ÿæˆè³‡æ–™\n",
    "X, y = generate_covid_like_data(n_samples=3000, n_features=93)\n",
    "print(f\"ç‰¹å¾µç¶­åº¦: {X.shape}\")\n",
    "print(f\"æ¨™ç±¤ç¶­åº¦: {y.shape}\")\n",
    "print(f\"æ¨™ç±¤ç¯„åœ: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Dataset é¡åˆ¥ ==========\n",
    "\n",
    "class COVID19Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    COVID-19 é æ¸¬è³‡æ–™é›†\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y=None, mode='train'):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y) if y is not None else None\n",
    "        self.mode = mode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        return self.X[idx]\n",
    "\n",
    "# åˆ†å‰²è³‡æ–™\n",
    "train_ratio = 0.8\n",
    "n_train = int(len(X) * train_ratio)\n",
    "\n",
    "X_train, X_val = X[:n_train], X[n_train:]\n",
    "y_train, y_val = y[:n_train], y[n_train:]\n",
    "\n",
    "print(f\"è¨“ç·´é›†: {X_train.shape[0]} æ¨£æœ¬\")\n",
    "print(f\"é©—è­‰é›†: {X_val.shape[0]} æ¨£æœ¬\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ç‰¹å¾µå·¥ç¨‹ ==========\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    ç‰¹å¾µå·¥ç¨‹é¡åˆ¥\n",
    "    \n",
    "    åŒ…å«:\n",
    "    - ç‰¹å¾µæ¨™æº–åŒ–\n",
    "    - ç‰¹å¾µé¸æ“‡\n",
    "    - ç¼ºå¤±å€¼è™•ç†\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.selected_features = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"æ“¬åˆæ¨™æº–åŒ–å™¨\"\"\"\n",
    "        self.scaler.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"è½‰æ›ç‰¹å¾µ\"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return X_scaled\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"æ“¬åˆä¸¦è½‰æ›\"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    @staticmethod\n",
    "    def select_features(X, y, method='correlation', k=40):\n",
    "        \"\"\"\n",
    "        ç‰¹å¾µé¸æ“‡\n",
    "        \n",
    "        æå®æ¯… HW1 æŠ€å·§:\n",
    "        - é¸æ“‡èˆ‡ç›®æ¨™ç›¸é—œæ€§é«˜çš„ç‰¹å¾µ\n",
    "        - ç§»é™¤å†—é¤˜ç‰¹å¾µ\n",
    "        \"\"\"\n",
    "        if method == 'correlation':\n",
    "            correlations = []\n",
    "            for i in range(X.shape[1]):\n",
    "                corr = np.corrcoef(X[:, i], y)[0, 1]\n",
    "                correlations.append(abs(corr) if not np.isnan(corr) else 0)\n",
    "            \n",
    "            # é¸æ“‡ç›¸é—œæ€§æœ€é«˜çš„ k å€‹ç‰¹å¾µ\n",
    "            top_indices = np.argsort(correlations)[-k:]\n",
    "            return sorted(top_indices)\n",
    "        \n",
    "        return list(range(X.shape[1]))\n",
    "\n",
    "# æ‡‰ç”¨ç‰¹å¾µå·¥ç¨‹\n",
    "fe = FeatureEngineer()\n",
    "\n",
    "# æ¨™æº–åŒ–\n",
    "X_train_scaled = fe.fit_transform(X_train)\n",
    "X_val_scaled = fe.transform(X_val)\n",
    "\n",
    "# ç‰¹å¾µé¸æ“‡ï¼ˆé¸æ“‡æœ€ç›¸é—œçš„ 40 å€‹ç‰¹å¾µï¼‰\n",
    "selected_idx = FeatureEngineer.select_features(X_train_scaled, y_train, k=40)\n",
    "print(f\"é¸æ“‡çš„ç‰¹å¾µæ•¸: {len(selected_idx)}\")\n",
    "\n",
    "X_train_selected = X_train_scaled[:, selected_idx]\n",
    "X_val_selected = X_val_scaled[:, selected_idx]\n",
    "\n",
    "print(f\"ç‰¹å¾µé¸æ“‡å¾Œç¶­åº¦: {X_train_selected.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è¨“ç·´é…ç½® ==========\n",
    "\n",
    "config = {\n",
    "    'batch_size': 256,\n",
    "    'epochs': 100,\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'hidden_dims': [64, 32, 16],\n",
    "    'dropout': 0.2,\n",
    "    'early_stopping_patience': 10,\n",
    "}\n",
    "\n",
    "# å»ºç«‹ DataLoader\n",
    "train_dataset = COVID19Dataset(X_train_selected, y_train)\n",
    "val_dataset = COVID19Dataset(X_val_selected, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "print(f\"è¨“ç·´æ‰¹æ¬¡æ•¸: {len(train_loader)}\")\n",
    "print(f\"é©—è­‰æ‰¹æ¬¡æ•¸: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è¨“ç·´å‡½æ•¸ ==========\n",
    "\n",
    "def train_regression_model(model, train_loader, val_loader, config):\n",
    "    \"\"\"\n",
    "    è¨“ç·´è¿´æ­¸æ¨¡å‹\n",
    "    \n",
    "    åŒ…å«:\n",
    "    - Early Stopping\n",
    "    - å­¸ç¿’ç‡èª¿åº¦\n",
    "    - æœ€ä½³æ¨¡å‹ä¿å­˜\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # å„ªåŒ–å™¨\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # å­¸ç¿’ç‡èª¿åº¦\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # æå¤±å‡½æ•¸\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # æ­·å²è¨˜éŒ„\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # ========== è¨“ç·´ ==========\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_batch)\n",
    "            loss = criterion(pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * len(X_batch)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # ========== é©—è­‰ ==========\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                \n",
    "                pred = model(X_batch)\n",
    "                loss = criterion(pred, y_batch)\n",
    "                \n",
    "                val_loss += loss.item() * len(X_batch)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        # æ›´æ–°å­¸ç¿’ç‡\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # è¨˜éŒ„\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        # Early Stopping æª¢æŸ¥\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{config['epochs']}\")\n",
    "            print(f\"  Train Loss: {train_loss:.6f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.6f}\")\n",
    "            print(f\"  Best Val Loss: {best_val_loss:.6f}\")\n",
    "        \n",
    "        if patience_counter >= config['early_stopping_patience']:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # è¼‰å…¥æœ€ä½³æ¨¡å‹\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# è¨“ç·´æ¨¡å‹\n",
    "model = DNNRegressor(\n",
    "    input_dim=X_train_selected.shape[1],\n",
    "    hidden_dims=config['hidden_dims'],\n",
    "    dropout=config['dropout']\n",
    ")\n",
    "\n",
    "model, history = train_regression_model(model, train_loader, val_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è¨“ç·´çµæœå¯è¦–åŒ– ==========\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# æå¤±æ›²ç·š\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].set_title('Training History')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# é æ¸¬ vs å¯¦éš›\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_val_tensor = torch.FloatTensor(X_val_selected).to(device)\n",
    "    y_pred = model(X_val_tensor).cpu().numpy()\n",
    "\n",
    "axes[1].scatter(y_val, y_pred, alpha=0.5)\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', label='Perfect')\n",
    "axes[1].set_xlabel('Actual')\n",
    "axes[1].set_ylabel('Predicted')\n",
    "axes[1].set_title('Prediction vs Actual')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# è¨ˆç®—æŒ‡æ¨™\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(f\"\\nè©•ä¼°æŒ‡æ¨™:\")\n",
    "print(f\"  MSE: {mse:.6f}\")\n",
    "print(f\"  RMSE: {rmse:.6f}\")\n",
    "print(f\"  MAE: {mae:.6f}\")\n",
    "print(f\"  RÂ²: {r2:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: é€²éšæŠ€å·§\n",
    "\n",
    "### 4.1 K-Fold äº¤å‰é©—è­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== K-Fold äº¤å‰é©—è­‰ ==========\n",
    "\n",
    "def cross_validate(X, y, n_folds=5, config=None):\n",
    "    \"\"\"\n",
    "    K-Fold äº¤å‰é©—è­‰\n",
    "    \n",
    "    æ›´å¯é çš„æ¨¡å‹è©•ä¼°æ–¹å¼\n",
    "    \"\"\"\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n",
    "        print(f\"\\n=== Fold {fold+1}/{n_folds} ===\")\n",
    "        \n",
    "        # åˆ†å‰²è³‡æ–™\n",
    "        X_tr, X_vl = X[train_idx], X[val_idx]\n",
    "        y_tr, y_vl = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # æ¨™æº–åŒ–\n",
    "        scaler = StandardScaler()\n",
    "        X_tr = scaler.fit_transform(X_tr)\n",
    "        X_vl = scaler.transform(X_vl)\n",
    "        \n",
    "        # å»ºç«‹ DataLoader\n",
    "        train_ds = COVID19Dataset(X_tr, y_tr)\n",
    "        val_ds = COVID19Dataset(X_vl, y_vl)\n",
    "        \n",
    "        train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "        val_dl = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
    "        \n",
    "        # è¨“ç·´\n",
    "        model = DNNRegressor(input_dim=X.shape[1], hidden_dims=[64, 32], dropout=0.2)\n",
    "        \n",
    "        # ç°¡åŒ–è¨“ç·´\n",
    "        model = model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        for epoch in range(30):\n",
    "            model.train()\n",
    "            for X_batch, y_batch in train_dl:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(X_batch), y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        # è©•ä¼°\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_vl_tensor = torch.FloatTensor(X_vl).to(device)\n",
    "            y_pred = model(X_vl_tensor).cpu().numpy()\n",
    "        \n",
    "        mse = mean_squared_error(y_vl, y_pred)\n",
    "        fold_results.append(mse)\n",
    "        print(f\"  Fold MSE: {mse:.6f}\")\n",
    "    \n",
    "    print(f\"\\nå¹³å‡ MSE: {np.mean(fold_results):.6f} Â± {np.std(fold_results):.6f}\")\n",
    "    return fold_results\n",
    "\n",
    "# åŸ·è¡Œäº¤å‰é©—è­‰\n",
    "cv_results = cross_validate(X, y, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 æ­£å‰‡åŒ–æŠ€å·§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ­£å‰‡åŒ–æŠ€å·§ ==========\n",
    "\n",
    "class RegularizedRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    å¸¶æ­£å‰‡åŒ–çš„è¿´æ­¸æ¨¡å‹\n",
    "    \n",
    "    åŒ…å«å¤šç¨®æ­£å‰‡åŒ–æŠ€å·§ï¼š\n",
    "    - L2 æ­£å‰‡åŒ– (weight_decay)\n",
    "    - Dropout\n",
    "    - Batch Normalization\n",
    "    - Early Stopping\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.LeakyReLU(0.1))  # LeakyReLU é˜²æ­¢æ­»ç¥ç¶“å…ƒ\n",
    "            \n",
    "            # æ¼¸é€²å¼ dropoutï¼ˆå‰é¢å±¤å°‘ï¼Œå¾Œé¢å±¤å¤šï¼‰\n",
    "            layers.append(nn.Dropout(dropout * (i + 1) / len(hidden_dims)))\n",
    "            \n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)\n",
    "\n",
    "print(\"æ­£å‰‡åŒ–è¿´æ­¸æ¨¡å‹ï¼š\")\n",
    "reg_model = RegularizedRegressor(input_dim=40, hidden_dims=[64, 32, 16], dropout=0.3)\n",
    "print(reg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¯ ç¸½çµ\n",
    "\n",
    "### è¿´æ­¸ä»»å‹™é‡é»\n",
    "\n",
    "| æŠ€å·§ | èªªæ˜ |\n",
    "|------|------|\n",
    "| **ç‰¹å¾µæ¨™æº–åŒ–** | åŠ é€Ÿæ”¶æ–‚ï¼Œå¿…åš |\n",
    "| **ç‰¹å¾µé¸æ“‡** | ç§»é™¤ç„¡é—œç‰¹å¾µï¼Œæ¸›å°‘éæ“¬åˆ |\n",
    "| **Early Stopping** | é˜²æ­¢éæ“¬åˆ |\n",
    "| **Learning Rate Scheduling** | å¾ŒæœŸç²¾ç´°èª¿æ•´ |\n",
    "| **Cross Validation** | æ›´å¯é çš„è©•ä¼° |\n",
    "\n",
    "### è©•ä¼°æŒ‡æ¨™\n",
    "\n",
    "| æŒ‡æ¨™ | å…¬å¼ | èªªæ˜ |\n",
    "|------|------|------|\n",
    "| **MSE** | $\\frac{1}{n}\\sum(y-\\hat{y})^2$ | å‡æ–¹èª¤å·®ï¼Œå°å¤§èª¤å·®æ•æ„Ÿ |\n",
    "| **RMSE** | $\\sqrt{MSE}$ | èˆ‡åŸå§‹å°ºåº¦ç›¸åŒ |\n",
    "| **MAE** | $\\frac{1}{n}\\sum|y-\\hat{y}|$ | å¹³å‡çµ•å°èª¤å·® |\n",
    "| **RÂ²** | $1 - \\frac{SS_{res}}{SS_{tot}}$ | æ±ºå®šä¿‚æ•¸ï¼Œè¶Šæ¥è¿‘ 1 è¶Šå¥½ |\n",
    "\n",
    "### æå®æ¯… HW1 æŠ€å·§\n",
    "\n",
    "```\n",
    "Strong Baseline é”æˆæ–¹æ³•:\n",
    "1. ç‰¹å¾µé¸æ“‡ï¼šé¸æ“‡èˆ‡ç¢ºè¨ºæ•¸ç›¸é—œçš„ç‰¹å¾µ\n",
    "2. é©ç•¶çš„ç¶²è·¯æ·±åº¦ï¼šä¸è¦å¤ªæ·±ä¹Ÿä¸è¦å¤ªæ·º\n",
    "3. æ­£å‰‡åŒ–ï¼šDropout + Weight Decay\n",
    "4. æ—©åœï¼šç›£æ§é©—è­‰é›†æå¤±\n",
    "5. å­¸ç¿’ç‡èª¿åº¦ï¼šReduceLROnPlateau\n",
    "```\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å‰å¾€ `supervised_learning/classification.ipynb` å­¸ç¿’åˆ†é¡ä»»å‹™ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## ç·´ç¿’é¡Œ\n\n### ç·´ç¿’ 1: æ¯”è¼ƒä¸åŒç‰¹å¾µæ•¸é‡çš„æ•ˆæœ\n\n**ç›®æ¨™**: å¯¦é©—ä¸åŒç‰¹å¾µé¸æ“‡æ•¸é‡å°æ¨¡å‹æ€§èƒ½çš„å½±éŸ¿",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ç·´ç¿’ 1: ç‰¹å¾µé¸æ“‡å¯¦é©—\n\ndef experiment_feature_selection(X_train, y_train, X_val, y_val, k_values=[10, 20, 40, 60, 80]):\n    \"\"\"æ¯”è¼ƒä¸åŒç‰¹å¾µæ•¸é‡çš„æ•ˆæœ\"\"\"\n    results = []\n    \n    # æ¨™æº–åŒ–\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    for k in k_values:\n        # ç‰¹å¾µé¸æ“‡\n        selected_idx = FeatureEngineer.select_features(X_train_scaled, y_train, k=min(k, X_train.shape[1]))\n        X_tr_sel = X_train_scaled[:, selected_idx]\n        X_vl_sel = X_val_scaled[:, selected_idx]\n        \n        # è¨“ç·´æ¨¡å‹\n        model = DNNRegressor(input_dim=len(selected_idx), hidden_dims=[32, 16], dropout=0.2).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n        criterion = nn.MSELoss()\n        \n        train_ds = COVID19Dataset(X_tr_sel, y_train)\n        train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n        \n        for epoch in range(50):\n            model.train()\n            for X_batch, y_batch in train_dl:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                optimizer.zero_grad()\n                loss = criterion(model(X_batch), y_batch)\n                loss.backward()\n                optimizer.step()\n        \n        # è©•ä¼°\n        model.eval()\n        with torch.no_grad():\n            y_pred = model(torch.FloatTensor(X_vl_sel).to(device)).cpu().numpy()\n        \n        mse = mean_squared_error(y_val, y_pred)\n        results.append({'k': k, 'mse': mse})\n        print(f\"k={k}: MSE = {mse:.6f}\")\n    \n    return results\n\n# åŸ·è¡Œå¯¦é©—\nresults = experiment_feature_selection(X_train, y_train, X_val, y_val)\n\n# è¦–è¦ºåŒ–\nplt.figure(figsize=(8, 5))\nplt.plot([r['k'] for r in results], [r['mse'] for r in results], 'bo-', linewidth=2, markersize=8)\nplt.xlabel('Number of Features (k)')\nplt.ylabel('Validation MSE')\nplt.title('Feature Selection: MSE vs Number of Features')\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"\\næœ€ä½³ç‰¹å¾µæ•¸: k={min(results, key=lambda x: x['mse'])['k']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### ç·´ç¿’ 2: æ¯”è¼ƒä¸åŒæå¤±å‡½æ•¸\n\n**ç›®æ¨™**: æ¯”è¼ƒ MSEã€MAEã€Huber Loss å°ç•°å¸¸å€¼çš„æ•æ„Ÿåº¦",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ç·´ç¿’ 2: æ¯”è¼ƒæå¤±å‡½æ•¸\n\n# ç”Ÿæˆå¸¶ç•°å¸¸å€¼çš„è³‡æ–™\nnp.random.seed(42)\nX_clean = np.random.randn(200, 5).astype(np.float32)\ny_clean = (2 * X_clean[:, 0] + 3 * X_clean[:, 1] + np.random.randn(200) * 0.3).astype(np.float32)\n\n# åŠ å…¥ 10% ç•°å¸¸å€¼\noutlier_idx = np.random.choice(200, 20, replace=False)\ny_with_outliers = y_clean.copy()\ny_with_outliers[outlier_idx] += np.random.choice([-10, 10], 20)\n\nprint(f\"ç•°å¸¸å€¼æ•¸é‡: {len(outlier_idx)}\")\nprint(f\"æ­£å¸¸ y ç¯„åœ: [{y_clean.min():.2f}, {y_clean.max():.2f}]\")\nprint(f\"ç•°å¸¸ y ç¯„åœ: [{y_with_outliers.min():.2f}, {y_with_outliers.max():.2f}]\")\n\ndef train_with_loss(X, y, loss_fn, loss_name, epochs=100):\n    \"\"\"ä½¿ç”¨æŒ‡å®šæå¤±å‡½æ•¸è¨“ç·´\"\"\"\n    model = nn.Sequential(nn.Linear(5, 32), nn.ReLU(), nn.Linear(32, 1)).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    \n    X_t = torch.FloatTensor(X).to(device)\n    y_t = torch.FloatTensor(y).to(device)\n    \n    for _ in range(epochs):\n        optimizer.zero_grad()\n        pred = model(X_t).squeeze()\n        loss = loss_fn(pred, y_t)\n        loss.backward()\n        optimizer.step()\n    \n    with torch.no_grad():\n        pred = model(X_t).squeeze().cpu().numpy()\n    \n    # è¨ˆç®—åœ¨ä¹¾æ·¨è³‡æ–™ä¸Šçš„ MSE\n    mse_clean = mean_squared_error(y_clean, pred)\n    return mse_clean, pred\n\n# æ¯”è¼ƒä¸‰ç¨®æå¤±å‡½æ•¸\nloss_fns = {\n    'MSE': nn.MSELoss(),\n    'MAE (L1)': nn.L1Loss(),\n    'Huber': nn.HuberLoss(delta=1.0),\n}\n\nresults_losses = {}\nfor name, loss_fn in loss_fns.items():\n    mse, pred = train_with_loss(X_clean, y_with_outliers, loss_fn, name)\n    results_losses[name] = {'mse': mse, 'pred': pred}\n    print(f\"{name}: Clean MSE = {mse:.4f}\")\n\n# è¦–è¦ºåŒ–\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\nfor i, (name, res) in enumerate(results_losses.items()):\n    axes[i].scatter(y_clean, res['pred'], alpha=0.5)\n    axes[i].plot([y_clean.min(), y_clean.max()], [y_clean.min(), y_clean.max()], 'r--')\n    axes[i].set_xlabel('True y (clean)')\n    axes[i].set_ylabel('Predicted y')\n    axes[i].set_title(f\"{name}\\nMSE on clean data: {res['mse']:.4f}\")\n    axes[i].grid(True, alpha=0.3)\n\nplt.suptitle('Loss Function Comparison with Outliers in Training Data')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nçµè«–: MAE å’Œ Huber Loss å°ç•°å¸¸å€¼æ›´é­¯æ£’\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}