{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¨¡å‹å£“ç¸® (Model Compression)\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "- ç†è§£æ¨¡å‹å£“ç¸®çš„å¿…è¦æ€§\n",
    "- å¯¦ä½œçŸ¥è­˜è’¸é¤¾ (Knowledge Distillation)\n",
    "- å­¸ç¿’ç¶²è·¯å‰ªæ (Pruning)\n",
    "- æŒæ¡æ¨¡å‹é‡åŒ– (Quantization)\n",
    "\n",
    "## å°æ‡‰èª²ç¨‹\n",
    "- [æå®æ¯… ML 2021 - Network Compression](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php)\n",
    "- [HW13: Network Compression](https://github.com/ga642381/ML2021-Spring/tree/main/HW13)\n",
    "\n",
    "## æ¨¡å‹å£“ç¸®æ¦‚è¿°\n",
    "\n",
    "```\n",
    "ç‚ºä»€éº¼éœ€è¦å£“ç¸®ï¼Ÿ\n",
    "â”œâ”€â”€ é‚Šç·£è¨­å‚™è³‡æºæœ‰é™\n",
    "â”œâ”€â”€ é™ä½æ¨ç†å»¶é²\n",
    "â”œâ”€â”€ æ¸›å°‘èƒ½è€—\n",
    "â””â”€â”€ é™ä½éƒ¨ç½²æˆæœ¬\n",
    "\n",
    "ä¸»è¦æ–¹æ³•:\n",
    "â”œâ”€â”€ Knowledge Distillationï¼ˆçŸ¥è­˜è’¸é¤¾ï¼‰\n",
    "â”‚   ç”¨å°æ¨¡å‹å­¸ç¿’å¤§æ¨¡å‹çš„çŸ¥è­˜\n",
    "â”œâ”€â”€ Pruningï¼ˆå‰ªæï¼‰\n",
    "â”‚   ç§»é™¤ä¸é‡è¦çš„æ¬Šé‡æˆ–çµæ§‹\n",
    "â”œâ”€â”€ Quantizationï¼ˆé‡åŒ–ï¼‰\n",
    "â”‚   é™ä½æ•¸å€¼ç²¾åº¦\n",
    "â””â”€â”€ Architecture Design\n",
    "    è¨­è¨ˆé«˜æ•ˆçš„ç¶²è·¯çµæ§‹\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "# å›ºå®šéš¨æ©Ÿç¨®å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: æº–å‚™å·¥ä½œ - Teacher å’Œ Student æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== å®šç¾©æ¨¡å‹ ==========\n",
    "\n",
    "class TeacherModel(nn.Module):\n",
    "    \"\"\"\n",
    "    å¤§å‹ Teacher æ¨¡å‹\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 4 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class StudentModel(nn.Module):\n",
    "    \"\"\"\n",
    "    å°å‹ Student æ¨¡å‹\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# æ¯”è¼ƒæ¨¡å‹å¤§å°\n",
    "teacher = TeacherModel()\n",
    "student = StudentModel()\n",
    "\n",
    "teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "student_params = sum(p.numel() for p in student.parameters())\n",
    "\n",
    "print(f\"Teacher æ¨¡å‹åƒæ•¸é‡: {teacher_params:,}\")\n",
    "print(f\"Student æ¨¡å‹åƒæ•¸é‡: {student_params:,}\")\n",
    "print(f\"å£“ç¸®æ¯”: {teacher_params / student_params:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æº–å‚™è³‡æ–™ ==========\n",
    "\n",
    "# ä½¿ç”¨ CIFAR-10 å­é›†\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# æ¨¡æ“¬è³‡æ–™ï¼ˆé¿å…ä¸‹è¼‰ï¼‰\n",
    "class FakeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, size=1000):\n",
    "        self.size = size\n",
    "        self.data = torch.randn(size, 3, 32, 32)\n",
    "        self.targets = torch.randint(0, 10, (size,))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "train_dataset = FakeDataset(2000)\n",
    "test_dataset = FakeDataset(500)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"è¨“ç·´é›†: {len(train_dataset)} æ¨£æœ¬\")\n",
    "print(f\"æ¸¬è©¦é›†: {len(test_dataset)} æ¨£æœ¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Knowledge Distillationï¼ˆçŸ¥è­˜è’¸é¤¾ï¼‰\n",
    "\n",
    "è®“ Student æ¨¡å‹å­¸ç¿’ Teacher æ¨¡å‹çš„ã€Œè»Ÿæ¨™ç±¤ã€ï¼ˆsoft labelsï¼‰ã€‚\n",
    "\n",
    "### æå¤±å‡½æ•¸\n",
    "\n",
    "$$L = \\alpha \\cdot L_{CE}(y, p_s) + (1-\\alpha) \\cdot T^2 \\cdot L_{KL}(p_t^{(T)}, p_s^{(T)})$$\n",
    "\n",
    "- $L_{CE}$: èˆ‡çœŸå¯¦æ¨™ç±¤çš„äº¤å‰ç†µ\n",
    "- $L_{KL}$: èˆ‡ Teacher è»Ÿæ¨™ç±¤çš„ KL æ•£åº¦\n",
    "- $T$: æº«åº¦åƒæ•¸\n",
    "- $p^{(T)} = \\text{softmax}(z/T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== çŸ¥è­˜è’¸é¤¾æå¤± ==========\n",
    "\n",
    "class DistillationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    çŸ¥è­˜è’¸é¤¾æå¤±å‡½æ•¸\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=4.0, alpha=0.7):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "    \n",
    "    def forward(self, student_logits, teacher_logits, labels):\n",
    "        # ç¡¬æ¨™ç±¤æå¤±\n",
    "        hard_loss = self.ce_loss(student_logits, labels)\n",
    "        \n",
    "        # è»Ÿæ¨™ç±¤æå¤±\n",
    "        soft_student = F.log_softmax(student_logits / self.temperature, dim=1)\n",
    "        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=1)\n",
    "        soft_loss = self.kl_loss(soft_student, soft_teacher) * (self.temperature ** 2)\n",
    "        \n",
    "        # ç¸½æå¤±\n",
    "        total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n",
    "        \n",
    "        return total_loss, hard_loss.item(), soft_loss.item()\n",
    "\n",
    "# æ¸¬è©¦\n",
    "distill_loss = DistillationLoss(temperature=4.0, alpha=0.7)\n",
    "student_out = torch.randn(4, 10)\n",
    "teacher_out = torch.randn(4, 10)\n",
    "labels = torch.randint(0, 10, (4,))\n",
    "\n",
    "loss, hard, soft = distill_loss(student_out, teacher_out, labels)\n",
    "print(f\"è’¸é¤¾æå¤±: {loss.item():.4f}\")\n",
    "print(f\"  ç¡¬æ¨™ç±¤æå¤±: {hard:.4f}\")\n",
    "print(f\"  è»Ÿæ¨™ç±¤æå¤±: {soft:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è¨“ç·´ Teacher ==========\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs=10, lr=1e-3):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'test_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # æ¸¬è©¦\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                out = model(x)\n",
    "                correct += (out.argmax(1) == y).sum().item()\n",
    "        \n",
    "        acc = correct / len(test_loader.dataset)\n",
    "        history['train_loss'].append(total_loss / len(train_loader))\n",
    "        history['test_acc'].append(acc)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Acc={acc:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# è¨“ç·´ Teacher\n",
    "print(\"è¨“ç·´ Teacher æ¨¡å‹...\")\n",
    "teacher, teacher_history = train_model(teacher, train_loader, test_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== çŸ¥è­˜è’¸é¤¾è¨“ç·´ Student ==========\n",
    "\n",
    "def train_with_distillation(student, teacher, train_loader, test_loader, epochs=10, lr=1e-3):\n",
    "    student = student.to(device)\n",
    "    teacher = teacher.to(device)\n",
    "    teacher.eval()  # Teacher å›ºå®š\n",
    "    \n",
    "    optimizer = torch.optim.Adam(student.parameters(), lr=lr)\n",
    "    criterion = DistillationLoss(temperature=4.0, alpha=0.3)\n",
    "    \n",
    "    history = {'train_loss': [], 'test_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # Teacher è¼¸å‡ºï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰\n",
    "            with torch.no_grad():\n",
    "                teacher_out = teacher(x)\n",
    "            \n",
    "            # Student è¼¸å‡º\n",
    "            optimizer.zero_grad()\n",
    "            student_out = student(x)\n",
    "            \n",
    "            # è’¸é¤¾æå¤±\n",
    "            loss, _, _ = criterion(student_out, teacher_out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # æ¸¬è©¦\n",
    "        student.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                out = student(x)\n",
    "                correct += (out.argmax(1) == y).sum().item()\n",
    "        \n",
    "        acc = correct / len(test_loader.dataset)\n",
    "        history['train_loss'].append(total_loss / len(train_loader))\n",
    "        history['test_acc'].append(acc)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Acc={acc:.4f}\")\n",
    "    \n",
    "    return student, history\n",
    "\n",
    "# çŸ¥è­˜è’¸é¤¾è¨“ç·´\n",
    "print(\"\\nä½¿ç”¨çŸ¥è­˜è’¸é¤¾è¨“ç·´ Student...\")\n",
    "student_distilled = StudentModel()\n",
    "student_distilled, distill_history = train_with_distillation(\n",
    "    student_distilled, teacher, train_loader, test_loader, epochs=10\n",
    ")\n",
    "\n",
    "# å°æ¯”ï¼šç›´æ¥è¨“ç·´ Student\n",
    "print(\"\\nç›´æ¥è¨“ç·´ Student...\")\n",
    "student_direct = StudentModel()\n",
    "student_direct, direct_history = train_model(student_direct, train_loader, test_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ¯”è¼ƒçµæœ ==========\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# æº–ç¢ºç‡æ¯”è¼ƒ\n",
    "axes[0].plot(teacher_history['test_acc'], label='Teacher', marker='o')\n",
    "axes[0].plot(distill_history['test_acc'], label='Student (Distilled)', marker='s')\n",
    "axes[0].plot(direct_history['test_acc'], label='Student (Direct)', marker='^')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Test Accuracy')\n",
    "axes[0].set_title('Accuracy Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# æ¨¡å‹å¤§å°å’Œæº–ç¢ºç‡\n",
    "models = ['Teacher', 'Student\\n(Distilled)', 'Student\\n(Direct)']\n",
    "params = [teacher_params, student_params, student_params]\n",
    "accs = [teacher_history['test_acc'][-1], distill_history['test_acc'][-1], direct_history['test_acc'][-1]]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "ax2 = axes[1]\n",
    "bars1 = ax2.bar(x - width/2, [p/1000 for p in params], width, label='Parameters (K)')\n",
    "ax2.set_ylabel('Parameters (K)')\n",
    "ax2.set_xlabel('Model')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models)\n",
    "\n",
    "ax3 = ax2.twinx()\n",
    "bars2 = ax3.bar(x + width/2, [a*100 for a in accs], width, color='orange', label='Accuracy (%)')\n",
    "ax3.set_ylabel('Accuracy (%)')\n",
    "\n",
    "ax2.legend(loc='upper left')\n",
    "ax3.legend(loc='upper right')\n",
    "ax2.set_title('Model Size vs Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nçµæœç¸½çµ:\")\n",
    "print(f\"  Teacher æº–ç¢ºç‡: {teacher_history['test_acc'][-1]:.4f}\")\n",
    "print(f\"  Distilled Student: {distill_history['test_acc'][-1]:.4f}\")\n",
    "print(f\"  Direct Student: {direct_history['test_acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Pruningï¼ˆå‰ªæï¼‰\n",
    "\n",
    "ç§»é™¤æ¨¡å‹ä¸­ä¸é‡è¦çš„æ¬Šé‡æˆ–çµæ§‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== éçµæ§‹åŒ–å‰ªæ ==========\n",
    "\n",
    "def apply_unstructured_pruning(model, amount=0.3):\n",
    "    \"\"\"\n",
    "    æ‡‰ç”¨å…¨å±€éçµæ§‹åŒ–å‰ªæ\n",
    "    \n",
    "    ç§»é™¤æœ€å°çš„ amount æ¯”ä¾‹çš„æ¬Šé‡\n",
    "    \"\"\"\n",
    "    parameters_to_prune = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    \n",
    "    # å…¨å±€å‰ªæ\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=amount,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_sparsity(model):\n",
    "    \"\"\"\n",
    "    è¨ˆç®—æ¨¡å‹ç¨€ç–åº¦\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    zeros = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            total += param.numel()\n",
    "            zeros += (param == 0).sum().item()\n",
    "    \n",
    "    return zeros / total\n",
    "\n",
    "# æ¸¬è©¦å‰ªæ\n",
    "pruned_model = copy.deepcopy(student_distilled)\n",
    "\n",
    "print(f\"å‰ªæå‰ç¨€ç–åº¦: {get_sparsity(pruned_model):.2%}\")\n",
    "\n",
    "apply_unstructured_pruning(pruned_model, amount=0.5)\n",
    "\n",
    "print(f\"å‰ªæå¾Œç¨€ç–åº¦: {get_sparsity(pruned_model):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è©•ä¼°å‰ªææ•ˆæœ ==========\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    return accuracy, inference_time\n",
    "\n",
    "# ä¸åŒå‰ªæç‡çš„æ•ˆæœ\n",
    "sparsity_levels = [0.0, 0.3, 0.5, 0.7, 0.9]\n",
    "results = []\n",
    "\n",
    "for sparsity in sparsity_levels:\n",
    "    model = copy.deepcopy(student_distilled)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    if sparsity > 0:\n",
    "        apply_unstructured_pruning(model, amount=sparsity)\n",
    "    \n",
    "    acc, time_taken = evaluate_model(model, test_loader)\n",
    "    actual_sparsity = get_sparsity(model)\n",
    "    \n",
    "    results.append({\n",
    "        'target_sparsity': sparsity,\n",
    "        'actual_sparsity': actual_sparsity,\n",
    "        'accuracy': acc,\n",
    "        'time': time_taken\n",
    "    })\n",
    "    \n",
    "    print(f\"ç¨€ç–åº¦ {sparsity*100:.0f}%: Acc={acc:.4f}, Time={time_taken:.4f}s\")\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "sparsities = [r['actual_sparsity']*100 for r in results]\n",
    "accuracies = [r['accuracy']*100 for r in results]\n",
    "\n",
    "ax.plot(sparsities, accuracies, 'bo-', markersize=10)\n",
    "ax.set_xlabel('Sparsity (%)')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Accuracy vs Sparsity')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "for i, (s, a) in enumerate(zip(sparsities, accuracies)):\n",
    "    ax.annotate(f'{a:.1f}%', (s, a), textcoords=\"offset points\", xytext=(0,10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Quantizationï¼ˆé‡åŒ–ï¼‰\n",
    "\n",
    "å°‡æ¨¡å‹å¾ FP32 è½‰æ›ç‚ºæ›´ä½ç²¾åº¦ï¼ˆå¦‚ INT8ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== å‹•æ…‹é‡åŒ– ==========\n",
    "\n",
    "def dynamic_quantize(model):\n",
    "    \"\"\"\n",
    "    å‹•æ…‹é‡åŒ–ï¼ˆä¸»è¦é‡å°ç·šæ€§å±¤ï¼‰\n",
    "    \"\"\"\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Linear},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "    return quantized_model\n",
    "\n",
    "def get_model_size(model):\n",
    "    \"\"\"\n",
    "    ç²å–æ¨¡å‹å¤§å°ï¼ˆMBï¼‰\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), 'temp_model.pt')\n",
    "    import os\n",
    "    size_mb = os.path.getsize('temp_model.pt') / 1e6\n",
    "    os.remove('temp_model.pt')\n",
    "    return size_mb\n",
    "\n",
    "# é‡åŒ–æ¨¡å‹\n",
    "original_model = copy.deepcopy(student_distilled).cpu()\n",
    "quantized_model = dynamic_quantize(original_model)\n",
    "\n",
    "print(f\"åŸå§‹æ¨¡å‹å¤§å°: {get_model_size(original_model):.2f} MB\")\n",
    "print(f\"é‡åŒ–æ¨¡å‹å¤§å°: {get_model_size(quantized_model):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ¨¡æ“¬ INT8 é‡åŒ– ==========\n",
    "\n",
    "class QuantizedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    æ¨¡æ“¬çš„ INT8 é‡åŒ–ç·šæ€§å±¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, linear_layer):\n",
    "        super().__init__()\n",
    "        \n",
    "        # é‡åŒ–æ¬Šé‡\n",
    "        weight = linear_layer.weight.data\n",
    "        self.scale = weight.abs().max() / 127\n",
    "        self.weight_int8 = torch.round(weight / self.scale).clamp(-128, 127).to(torch.int8)\n",
    "        \n",
    "        # åç½®ä¿æŒ FP32\n",
    "        self.bias = linear_layer.bias\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # åé‡åŒ–æ¬Šé‡\n",
    "        weight_fp32 = self.weight_int8.float() * self.scale\n",
    "        return F.linear(x, weight_fp32, self.bias)\n",
    "\n",
    "def quantize_model_manual(model):\n",
    "    \"\"\"\n",
    "    æ‰‹å‹•é‡åŒ–æ¨¡å‹ä¸­çš„ç·šæ€§å±¤\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            setattr(model, name, QuantizedLinear(module))\n",
    "        elif len(list(module.children())) > 0:\n",
    "            quantize_model_manual(module)\n",
    "    return model\n",
    "\n",
    "print(\"æ‰‹å‹•é‡åŒ–ç¤ºç¯„å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¯ ç¸½çµ\n",
    "\n",
    "### å£“ç¸®æ–¹æ³•æ¯”è¼ƒ\n",
    "\n",
    "| æ–¹æ³• | å£“ç¸®ç‡ | ç²¾åº¦æå¤± | ç¡¬é«”è¦æ±‚ |\n",
    "|------|--------|----------|----------|\n",
    "| **Knowledge Distillation** | 2-10x | å° | ç„¡ç‰¹æ®Šè¦æ±‚ |\n",
    "| **Pruning** | 2-10x | ä¸­ç­‰ | ç¨€ç–é‹ç®—æ”¯æŒ |\n",
    "| **Quantization** | 2-4x | å° | INT8 æ”¯æŒ |\n",
    "\n",
    "### æå®æ¯… HW13 æŠ€å·§\n",
    "\n",
    "```\n",
    "Knowledge Distillation:\n",
    "1. é¸æ“‡é©ç•¶çš„æº«åº¦ Tï¼ˆé€šå¸¸ 3-10ï¼‰\n",
    "2. èª¿æ•´ alpha å¹³è¡¡ç¡¬/è»Ÿæ¨™ç±¤\n",
    "3. å¯ä»¥çµ„åˆå¤šå€‹ Teacher\n",
    "\n",
    "Pruning:\n",
    "1. é€æ­¥å‰ªææ¯”ä¸€æ¬¡æ€§å‰ªææ•ˆæœå¥½\n",
    "2. å‰ªæå¾Œéœ€è¦ fine-tune\n",
    "3. çµæ§‹åŒ–å‰ªææ›´å®¹æ˜“åŠ é€Ÿ\n",
    "\n",
    "Quantization:\n",
    "1. PTQ vs QAT æ¬Šè¡¡\n",
    "2. æ··åˆç²¾åº¦é‡åŒ–\n",
    "3. æ•æ„Ÿå±¤ä¿æŒé«˜ç²¾åº¦\n",
    "```\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å‰å¾€ `advanced_learning/meta_learning.ipynb` å­¸ç¿’å…ƒå­¸ç¿’ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## ğŸ‹ï¸ ç·´ç¿’\n\n### ç·´ç¿’ 1: æ¯”è¼ƒä¸åŒè’¸é¤¾æº«åº¦çš„æ•ˆæœ\n\næ¸¬è©¦æº«åº¦åƒæ•¸ T å°çŸ¥è­˜è’¸é¤¾æ•ˆæœçš„å½±éŸ¿ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== ç·´ç¿’ 1: æ¯”è¼ƒä¸åŒè’¸é¤¾æº«åº¦ ==========\n\ndef train_with_temperature(student, teacher, train_loader, test_loader, temperature, epochs=10):\n    \"\"\"\n    ä½¿ç”¨æŒ‡å®šæº«åº¦é€²è¡ŒçŸ¥è­˜è’¸é¤¾\n    \"\"\"\n    student = student.to(device)\n    teacher = teacher.to(device)\n    teacher.eval()\n    \n    optimizer = torch.optim.Adam(student.parameters(), lr=1e-3)\n    criterion = DistillationLoss(temperature=temperature, alpha=0.3)\n    \n    for epoch in range(epochs):\n        student.train()\n        for x, y in train_loader:\n            x, y = x.to(device), y.to(device)\n            with torch.no_grad():\n                teacher_out = teacher(x)\n            optimizer.zero_grad()\n            student_out = student(x)\n            loss, _, _ = criterion(student_out, teacher_out, y)\n            loss.backward()\n            optimizer.step()\n    \n    # è©•ä¼°\n    student.eval()\n    correct = 0\n    with torch.no_grad():\n        for x, y in test_loader:\n            x, y = x.to(device), y.to(device)\n            out = student(x)\n            correct += (out.argmax(1) == y).sum().item()\n    \n    return correct / len(test_loader.dataset)\n\ndef experiment_temperature():\n    \"\"\"\n    æ¯”è¼ƒä¸åŒæº«åº¦åƒæ•¸çš„æ•ˆæœ\n    \"\"\"\n    temperatures = [1, 2, 4, 8, 16]\n    results = []\n    \n    print(\"ç·´ç¿’ 1: è’¸é¤¾æº«åº¦å¯¦é©—\")\n    print(\"=\" * 50)\n    \n    for T in temperatures:\n        student = StudentModel()\n        acc = train_with_temperature(student, teacher, train_loader, test_loader, temperature=T, epochs=10)\n        results.append({'temperature': T, 'accuracy': acc})\n        print(f\"æº«åº¦ T={T:2d}: æº–ç¢ºç‡ = {acc:.4f}\")\n    \n    return results\n\n# åŸ·è¡Œå¯¦é©—\ntemp_results = experiment_temperature()\n\n# è¦–è¦ºåŒ–\nfig, ax = plt.subplots(figsize=(10, 6))\n\ntemps = [r['temperature'] for r in temp_results]\naccs = [r['accuracy'] * 100 for r in temp_results]\n\nbars = ax.bar(range(len(temps)), accs, color='steelblue')\nax.set_xticks(range(len(temps)))\nax.set_xticklabels([f'T={t}' for t in temps])\nax.set_xlabel('Temperature')\nax.set_ylabel('Accuracy (%)')\nax.set_title('Effect of Distillation Temperature')\n\nfor bar, acc in zip(bars, accs):\n    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n            f'{acc:.1f}%', ha='center', fontsize=10)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nçµè«–ï¼š\")\nprint(\"- T=1 ç­‰æ–¼æ™®é€šçš„ softmaxï¼Œè»Ÿæ¨™ç±¤ä¸å¤ ã€Œè»Ÿã€\")\nprint(\"- T éå¤§æœƒä½¿åˆ†ä½ˆéæ–¼å¹³æ»‘ï¼Œä¸Ÿå¤±é¡åˆ¥è³‡è¨Š\")\nprint(\"- é€šå¸¸ T=3~10 æ˜¯è¼ƒå¥½çš„é¸æ“‡\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}