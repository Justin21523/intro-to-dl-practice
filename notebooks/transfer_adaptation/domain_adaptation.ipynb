{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adaptation (é ˜åŸŸé©æ‡‰)\n",
    "\n",
    "**å°æ‡‰èª²ç¨‹**: æå®æ¯… 2021 ML HW11 - Domain Adaptation\n",
    "\n",
    "æœ¬ notebook ä»‹ç´¹å¦‚ä½•è™•ç†è¨“ç·´èˆ‡æ¸¬è©¦è³‡æ–™åˆ†ä½ˆä¸åŒçš„å•é¡Œï¼š\n",
    "- **Domain Shift å•é¡Œ**: ç†è§£åˆ†ä½ˆåç§»\n",
    "- **Discrepancy-based**: MMD, CORAL\n",
    "- **Adversarial-based**: DANN, ADDA\n",
    "- **Self-training**: Pseudo-labeling\n",
    "\n",
    "```\n",
    "Domain Adaptation å•é¡Œè¨­å®šï¼š\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Source Domain (æœ‰æ¨™ç±¤)        Target Domain (ç„¡/å°‘æ¨™ç±¤)    â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚  â”‚ ğŸ–¼ï¸ ğŸ–¼ï¸ ğŸ–¼ï¸ â”‚                   â”‚ ğŸ“· ğŸ“· ğŸ“· â”‚                  â”‚\n",
    "â”‚  â”‚ åˆæˆåœ–ç‰‡ â”‚      â”€â”€â”€â”€â”€â”€â–º     â”‚ çœŸå¯¦ç…§ç‰‡ â”‚                  â”‚\n",
    "â”‚  â”‚ (æœ‰æ¨™ç±¤) â”‚                   â”‚ (ç„¡æ¨™ç±¤) â”‚                  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  P_s(X, Y) â‰  P_t(X, Y)                                     â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  ç›®æ¨™ï¼šå­¸ç¿’ä¸€å€‹åœ¨ Target Domain è¡¨ç¾è‰¯å¥½çš„æ¨¡å‹              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Domain Adaptation é¡å‹ï¼š\n",
    "- Unsupervised DA: Target å®Œå…¨ç„¡æ¨™ç±¤\n",
    "- Semi-supervised DA: Target æœ‰å°‘é‡æ¨™ç±¤\n",
    "- Supervised DA: Target æœ‰æ¨™ç±¤ï¼ˆä½†é‡å°‘ï¼‰\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from torch.autograd import Function\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Domain Shift è¦–è¦ºåŒ–\n",
    "\n",
    "ç†è§£ä»€éº¼æ˜¯ Domain Shift ä»¥åŠå®ƒå°æ¨¡å‹çš„å½±éŸ¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_domain_shift_data(\n",
    "    n_samples: int = 500,\n",
    "    source_params: Tuple[float, float] = (0, 1),\n",
    "    target_params: Tuple[float, float] = (2, 1.5)\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    å‰µå»ºå…·æœ‰ domain shift çš„äºŒåˆ†é¡æ•¸æ“š\n",
    "    \n",
    "    Returns:\n",
    "        X_source, y_source, X_target, y_target\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Source domain\n",
    "    source_class0 = np.random.randn(n_samples // 2, 2) * source_params[1] + [source_params[0], 0]\n",
    "    source_class1 = np.random.randn(n_samples // 2, 2) * source_params[1] + [source_params[0], 3]\n",
    "    X_source = np.vstack([source_class0, source_class1])\n",
    "    y_source = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))\n",
    "    \n",
    "    # Target domain (shifted)\n",
    "    target_class0 = np.random.randn(n_samples // 2, 2) * target_params[1] + [target_params[0], 0]\n",
    "    target_class1 = np.random.randn(n_samples // 2, 2) * target_params[1] + [target_params[0], 3]\n",
    "    X_target = np.vstack([target_class0, target_class1])\n",
    "    y_target = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))\n",
    "    \n",
    "    return X_source, y_source, X_target, y_target\n",
    "\n",
    "\n",
    "def visualize_domain_shift():\n",
    "    \"\"\"è¦–è¦ºåŒ– domain shift\"\"\"\n",
    "    X_s, y_s, X_t, y_t = create_domain_shift_data()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Source domain\n",
    "    axes[0].scatter(X_s[y_s==0, 0], X_s[y_s==0, 1], c='blue', alpha=0.5, label='Class 0')\n",
    "    axes[0].scatter(X_s[y_s==1, 0], X_s[y_s==1, 1], c='red', alpha=0.5, label='Class 1')\n",
    "    axes[0].set_title('Source Domain (æœ‰æ¨™ç±¤)')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_xlim(-5, 8)\n",
    "    axes[0].set_ylim(-4, 8)\n",
    "    \n",
    "    # Target domain\n",
    "    axes[1].scatter(X_t[y_t==0, 0], X_t[y_t==0, 1], c='cyan', alpha=0.5, label='Class 0')\n",
    "    axes[1].scatter(X_t[y_t==1, 0], X_t[y_t==1, 1], c='orange', alpha=0.5, label='Class 1')\n",
    "    axes[1].set_title('Target Domain (ç„¡æ¨™ç±¤)')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xlim(-5, 8)\n",
    "    axes[1].set_ylim(-4, 8)\n",
    "    \n",
    "    # Both domains\n",
    "    axes[2].scatter(X_s[:, 0], X_s[:, 1], c='blue', alpha=0.3, label='Source', marker='o')\n",
    "    axes[2].scatter(X_t[:, 0], X_t[:, 1], c='red', alpha=0.3, label='Target', marker='x')\n",
    "    axes[2].set_title('Domain Shift è¦–è¦ºåŒ–')\n",
    "    axes[2].legend()\n",
    "    axes[2].set_xlim(-5, 8)\n",
    "    axes[2].set_ylim(-4, 8)\n",
    "    \n",
    "    # æ·»åŠ ç®­é ­è¡¨ç¤º shift\n",
    "    axes[2].annotate('', xy=(2, 1.5), xytext=(0, 1.5),\n",
    "                    arrowprops=dict(arrowstyle='->', color='green', lw=2))\n",
    "    axes[2].text(1, 2.2, 'Domain\\nShift', ha='center', fontsize=10, color='green')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_domain_shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_performance_drop():\n",
    "    \"\"\"å±•ç¤º domain shift å°æ¨¡å‹æ€§èƒ½çš„å½±éŸ¿\"\"\"\n",
    "    X_s, y_s, X_t, y_t = create_domain_shift_data()\n",
    "    \n",
    "    # è½‰æ›ç‚º tensor\n",
    "    X_s_t = torch.FloatTensor(X_s)\n",
    "    y_s_t = torch.LongTensor(y_s)\n",
    "    X_t_t = torch.FloatTensor(X_t)\n",
    "    y_t_t = torch.LongTensor(y_t)\n",
    "    \n",
    "    # ç°¡å–®çš„åˆ†é¡å™¨\n",
    "    class SimpleClassifier(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(2, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32, 2)\n",
    "            )\n",
    "            \n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "    \n",
    "    # åœ¨ source ä¸Šè¨“ç·´\n",
    "    model = SimpleClassifier()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_s_t)\n",
    "        loss = criterion(output, y_s_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # è©•ä¼°\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        source_pred = model(X_s_t).argmax(dim=1)\n",
    "        target_pred = model(X_t_t).argmax(dim=1)\n",
    "        \n",
    "        source_acc = (source_pred == y_s_t).float().mean().item()\n",
    "        target_acc = (target_pred == y_t_t).float().mean().item()\n",
    "    \n",
    "    print(f\"Source Domain æº–ç¢ºç‡: {source_acc:.2%}\")\n",
    "    print(f\"Target Domain æº–ç¢ºç‡: {target_acc:.2%}\")\n",
    "    print(f\"æ€§èƒ½ä¸‹é™: {source_acc - target_acc:.2%}\")\n",
    "    \n",
    "    return model, X_s, y_s, X_t, y_t\n",
    "\n",
    "model, X_s, y_s, X_t, y_t = demonstrate_performance_drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Discrepancy-based Methods\n",
    "\n",
    "é€éæœ€å°åŒ–é ˜åŸŸé–“çš„åˆ†ä½ˆå·®ç•°ä¾†é€²è¡Œé©æ‡‰ã€‚\n",
    "\n",
    "```\n",
    "Discrepancy-based æ–¹æ³•åŸç†ï¼š\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   Source Features â”€â”€â”¬â”€â”€â–º ç‰¹å¾µåˆ†ä½ˆ P_s(f)                       â”‚\n",
    "â”‚                     â”‚                                           â”‚\n",
    "â”‚                     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\n",
    "â”‚                     â”œâ”€â”€â”€â–ºâ”‚  Discrepancy â”‚ â—„â”€â”€ æœ€å°åŒ–            â”‚\n",
    "â”‚                     â”‚    â”‚  (MMD/CORAL) â”‚                      â”‚\n",
    "â”‚                     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n",
    "â”‚                     â”‚                                           â”‚\n",
    "â”‚   Target Features â”€â”€â”´â”€â”€â–º ç‰¹å¾µåˆ†ä½ˆ P_t(f)                       â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "MMD (Maximum Mean Discrepancy):\n",
    "MMDÂ²(P, Q) = E[k(x,x')] + E[k(y,y')] - 2E[k(x,y)]\n",
    "\n",
    "å…¶ä¸­ k æ˜¯æ ¸å‡½æ•¸ï¼ˆé€šå¸¸ç”¨ RBF kernelï¼‰\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMDLoss(nn.Module):\n",
    "    \"\"\"Maximum Mean Discrepancy Loss\"\"\"\n",
    "    \n",
    "    def __init__(self, kernel_type: str = 'rbf', kernel_mul: float = 2.0, kernel_num: int = 5):\n",
    "        super().__init__()\n",
    "        self.kernel_type = kernel_type\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.kernel_num = kernel_num\n",
    "        \n",
    "    def gaussian_kernel(self, source: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"è¨ˆç®—é«˜æ–¯æ ¸çŸ©é™£\"\"\"\n",
    "        n_samples = source.shape[0] + target.shape[0]\n",
    "        total = torch.cat([source, target], dim=0)\n",
    "        \n",
    "        # è¨ˆç®— L2 è·é›¢\n",
    "        total0 = total.unsqueeze(0).expand(n_samples, n_samples, -1)\n",
    "        total1 = total.unsqueeze(1).expand(n_samples, n_samples, -1)\n",
    "        L2_distance = ((total0 - total1) ** 2).sum(dim=2)\n",
    "        \n",
    "        # å¤šå°ºåº¦é«˜æ–¯æ ¸\n",
    "        bandwidth = torch.sum(L2_distance) / (n_samples ** 2 - n_samples)\n",
    "        bandwidth /= self.kernel_mul ** (self.kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (self.kernel_mul ** i) for i in range(self.kernel_num)]\n",
    "        \n",
    "        kernel_val = torch.zeros_like(L2_distance)\n",
    "        for bw in bandwidth_list:\n",
    "            kernel_val += torch.exp(-L2_distance / (2 * bw + 1e-8))\n",
    "            \n",
    "        return kernel_val\n",
    "    \n",
    "    def forward(self, source: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        è¨ˆç®— MMD æå¤±\n",
    "        \n",
    "        Args:\n",
    "            source: Source domain ç‰¹å¾µ [batch_s, dim]\n",
    "            target: Target domain ç‰¹å¾µ [batch_t, dim]\n",
    "        \"\"\"\n",
    "        batch_s = source.shape[0]\n",
    "        batch_t = target.shape[0]\n",
    "        \n",
    "        kernels = self.gaussian_kernel(source, target)\n",
    "        \n",
    "        # åˆ†å‰²æ ¸çŸ©é™£\n",
    "        XX = kernels[:batch_s, :batch_s]\n",
    "        YY = kernels[batch_s:, batch_s:]\n",
    "        XY = kernels[:batch_s, batch_s:]\n",
    "        YX = kernels[batch_s:, :batch_s]\n",
    "        \n",
    "        # MMD\n",
    "        mmd = XX.mean() + YY.mean() - XY.mean() - YX.mean()\n",
    "        \n",
    "        return mmd\n",
    "\n",
    "\n",
    "# æ¸¬è©¦\n",
    "mmd_loss = MMDLoss()\n",
    "\n",
    "source_features = torch.randn(32, 64)\n",
    "target_features = torch.randn(32, 64) + 1  # æœ‰åç§»\n",
    "\n",
    "loss = mmd_loss(source_features, target_features)\n",
    "print(f\"MMD Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ç›¸åŒåˆ†ä½ˆ\n",
    "same_features = torch.randn(32, 64)\n",
    "loss_same = mmd_loss(same_features, same_features + torch.randn(32, 64) * 0.1)\n",
    "print(f\"MMD Loss (ç›¸è¿‘åˆ†ä½ˆ): {loss_same.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CORALLoss(nn.Module):\n",
    "    \"\"\"Correlation Alignment Loss\"\"\"\n",
    "    \n",
    "    def forward(self, source: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        è¨ˆç®— CORAL æå¤±\n",
    "        å°é½Š source å’Œ target çš„å”æ–¹å·®çŸ©é™£\n",
    "        \n",
    "        Args:\n",
    "            source: Source domain ç‰¹å¾µ [batch_s, dim]\n",
    "            target: Target domain ç‰¹å¾µ [batch_t, dim]\n",
    "        \"\"\"\n",
    "        d = source.shape[1]\n",
    "        \n",
    "        # ä¸­å¿ƒåŒ–\n",
    "        source_centered = source - source.mean(dim=0, keepdim=True)\n",
    "        target_centered = target - target.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        # è¨ˆç®—å”æ–¹å·®çŸ©é™£\n",
    "        cov_source = (source_centered.T @ source_centered) / (source.shape[0] - 1)\n",
    "        cov_target = (target_centered.T @ target_centered) / (target.shape[0] - 1)\n",
    "        \n",
    "        # CORAL loss: Frobenius norm of covariance difference\n",
    "        loss = torch.sum((cov_source - cov_target) ** 2) / (4 * d * d)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "# æ¸¬è©¦\n",
    "coral_loss = CORALLoss()\n",
    "loss = coral_loss(source_features, target_features)\n",
    "print(f\"CORAL Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCORAL(nn.Module):\n",
    "    \"\"\"Deep CORAL ç¶²è·¯\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 2,\n",
    "        hidden_dim: int = 64,\n",
    "        feature_dim: int = 32,\n",
    "        num_classes: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ç‰¹å¾µæå–å™¨\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, feature_dim)\n",
    "        )\n",
    "        \n",
    "        # åˆ†é¡å™¨\n",
    "        self.classifier = nn.Linear(feature_dim, num_classes)\n",
    "        \n",
    "        # CORAL loss\n",
    "        self.coral_loss = CORALLoss()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        source: torch.Tensor,\n",
    "        target: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source: Source domain æ•¸æ“š\n",
    "            target: Target domain æ•¸æ“šï¼ˆè¨“ç·´æ™‚ä½¿ç”¨ï¼‰\n",
    "            \n",
    "        Returns:\n",
    "            source_output: Source åˆ†é¡è¼¸å‡º\n",
    "            source_features: Source ç‰¹å¾µ\n",
    "            coral_loss: CORAL æå¤±ï¼ˆå¦‚æœæœ‰ targetï¼‰\n",
    "        \"\"\"\n",
    "        # æå–ç‰¹å¾µ\n",
    "        source_features = self.feature_extractor(source)\n",
    "        source_output = self.classifier(source_features)\n",
    "        \n",
    "        coral = None\n",
    "        if target is not None:\n",
    "            target_features = self.feature_extractor(target)\n",
    "            coral = self.coral_loss(source_features, target_features)\n",
    "            \n",
    "        return source_output, source_features, coral\n",
    "\n",
    "\n",
    "# æ¸¬è©¦\n",
    "model = DeepCORAL(input_dim=2)\n",
    "source = torch.randn(32, 2)\n",
    "target = torch.randn(32, 2)\n",
    "\n",
    "output, features, coral = model(source, target)\n",
    "print(f\"è¼¸å‡ºå½¢ç‹€: {output.shape}\")\n",
    "print(f\"ç‰¹å¾µå½¢ç‹€: {features.shape}\")\n",
    "print(f\"CORAL Loss: {coral.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Adversarial-based Methods\n",
    "\n",
    "ä½¿ç”¨å°æŠ—å­¸ç¿’è®“ç‰¹å¾µæå–å™¨ç”¢ç”Ÿé ˜åŸŸä¸è®Šçš„ç‰¹å¾µã€‚\n",
    "\n",
    "```\n",
    "DANN (Domain-Adversarial Neural Network) æ¶æ§‹ï¼š\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   Input X â”€â”€â–º Feature Extractor (G_f) â”€â”€â–º Features f            â”‚\n",
    "â”‚                                              â”‚                  â”‚\n",
    "â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚                                    â”‚                 â”‚          â”‚\n",
    "â”‚                                    â–¼                 â–¼          â”‚\n",
    "â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚                          â”‚ Label Predictor â”‚  â”‚   Domain     â”‚  â”‚\n",
    "â”‚                          â”‚     (G_y)       â”‚  â”‚ Discriminatorâ”‚  â”‚\n",
    "â”‚                          â”‚                 â”‚  â”‚    (G_d)     â”‚  â”‚\n",
    "â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                   â”‚                  â”‚          â”‚\n",
    "â”‚                                   â–¼                  â–¼          â”‚\n",
    "â”‚                           Class Labels         Domain Label     â”‚\n",
    "â”‚                                                (Source/Target)  â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   é—œéµï¼šGradient Reversal Layer (GRL)                           â”‚\n",
    "â”‚   - å‰å‘å‚³æ’­ï¼šæ†ç­‰å‡½æ•¸                                          â”‚\n",
    "â”‚   - åå‘å‚³æ’­ï¼šæ¢¯åº¦åè½‰ (Ã— -Î»)                                   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(Function):\n",
    "    \"\"\"Gradient Reversal Layer çš„è‡ªå‹•å¾®åˆ†å‡½æ•¸\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # åå‘å‚³æ’­æ™‚åè½‰æ¢¯åº¦\n",
    "        return grad_output.neg() * ctx.lambda_, None\n",
    "\n",
    "\n",
    "class GradientReversalLayer(nn.Module):\n",
    "    \"\"\"Gradient Reversal Layer\"\"\"\n",
    "    \n",
    "    def __init__(self, lambda_: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.lambda_ = lambda_\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)\n",
    "    \n",
    "    def set_lambda(self, lambda_: float):\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "\n",
    "class DANN(nn.Module):\n",
    "    \"\"\"Domain-Adversarial Neural Network\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 2,\n",
    "        hidden_dim: int = 64,\n",
    "        feature_dim: int = 32,\n",
    "        num_classes: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ç‰¹å¾µæå–å™¨ G_f\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, feature_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # æ¨™ç±¤é æ¸¬å™¨ G_y\n",
    "        self.label_predictor = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # é ˜åŸŸåˆ¤åˆ¥å™¨ G_d\n",
    "        self.domain_discriminator = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 2)  # Source vs Target\n",
    "        )\n",
    "        \n",
    "        # Gradient Reversal Layer\n",
    "        self.grl = GradientReversalLayer()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        lambda_: float = 1.0\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: è¼¸å…¥æ•¸æ“š\n",
    "            lambda_: GRL çš„ lambda åƒæ•¸\n",
    "            \n",
    "        Returns:\n",
    "            class_output: é¡åˆ¥é æ¸¬\n",
    "            domain_output: é ˜åŸŸé æ¸¬\n",
    "            features: ç‰¹å¾µ\n",
    "        \"\"\"\n",
    "        self.grl.set_lambda(lambda_)\n",
    "        \n",
    "        # æå–ç‰¹å¾µ\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # é¡åˆ¥é æ¸¬\n",
    "        class_output = self.label_predictor(features)\n",
    "        \n",
    "        # é ˜åŸŸé æ¸¬ï¼ˆé€šé GRLï¼‰\n",
    "        reversed_features = self.grl(features)\n",
    "        domain_output = self.domain_discriminator(reversed_features)\n",
    "        \n",
    "        return class_output, domain_output, features\n",
    "\n",
    "\n",
    "# æ¸¬è©¦\n",
    "dann = DANN()\n",
    "x = torch.randn(32, 2)\n",
    "class_out, domain_out, feat = dann(x, lambda_=1.0)\n",
    "print(f\"é¡åˆ¥è¼¸å‡º: {class_out.shape}\")\n",
    "print(f\"é ˜åŸŸè¼¸å‡º: {domain_out.shape}\")\n",
    "print(f\"ç‰¹å¾µ: {feat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dann(\n",
    "    model: DANN,\n",
    "    source_data: Tuple[torch.Tensor, torch.Tensor],\n",
    "    target_data: torch.Tensor,\n",
    "    num_epochs: int = 100,\n",
    "    lr: float = 0.01\n",
    ") -> List[Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    è¨“ç·´ DANN\n",
    "    \n",
    "    Args:\n",
    "        model: DANN æ¨¡å‹\n",
    "        source_data: (X_source, y_source)\n",
    "        target_data: X_target\n",
    "        num_epochs: è¨“ç·´è¼ªæ•¸\n",
    "        lr: å­¸ç¿’ç‡\n",
    "        \n",
    "    Returns:\n",
    "        è¨“ç·´æ­·å²\n",
    "    \"\"\"\n",
    "    X_s, y_s = source_data\n",
    "    X_t = target_data\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    class_criterion = nn.CrossEntropyLoss()\n",
    "    domain_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # é ˜åŸŸæ¨™ç±¤\n",
    "    source_domain_label = torch.zeros(X_s.shape[0]).long()  # 0 = source\n",
    "    target_domain_label = torch.ones(X_t.shape[0]).long()   # 1 = target\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # è¨ˆç®— lambdaï¼ˆé€æ¼¸å¢åŠ ï¼‰\n",
    "        p = epoch / num_epochs\n",
    "        lambda_ = 2 / (1 + np.exp(-10 * p)) - 1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Source domain\n",
    "        class_out_s, domain_out_s, _ = model(X_s, lambda_)\n",
    "        class_loss = class_criterion(class_out_s, y_s)\n",
    "        domain_loss_s = domain_criterion(domain_out_s, source_domain_label)\n",
    "        \n",
    "        # Target domain\n",
    "        _, domain_out_t, _ = model(X_t, lambda_)\n",
    "        domain_loss_t = domain_criterion(domain_out_t, target_domain_label)\n",
    "        \n",
    "        # ç¸½æå¤±\n",
    "        domain_loss = (domain_loss_s + domain_loss_t) / 2\n",
    "        total_loss = class_loss + domain_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        history.append({\n",
    "            'epoch': epoch,\n",
    "            'class_loss': class_loss.item(),\n",
    "            'domain_loss': domain_loss.item(),\n",
    "            'lambda': lambda_\n",
    "        })\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}: class_loss={class_loss.item():.4f}, \"\n",
    "                  f\"domain_loss={domain_loss.item():.4f}, Î»={lambda_:.4f}\")\n",
    "            \n",
    "    return history\n",
    "\n",
    "\n",
    "# è¨“ç·´ DANN\n",
    "X_s, y_s, X_t, y_t = create_domain_shift_data()\n",
    "X_s_t = torch.FloatTensor(X_s)\n",
    "y_s_t = torch.LongTensor(y_s)\n",
    "X_t_t = torch.FloatTensor(X_t)\n",
    "y_t_t = torch.LongTensor(y_t)\n",
    "\n",
    "dann_model = DANN(input_dim=2)\n",
    "history = train_dann(dann_model, (X_s_t, y_s_t), X_t_t, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dann(model, X_s, y_s, X_t, y_t):\n",
    "    \"\"\"è©•ä¼° DANN æ¨¡å‹\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X_s_t = torch.FloatTensor(X_s)\n",
    "        X_t_t = torch.FloatTensor(X_t)\n",
    "        \n",
    "        class_out_s, _, feat_s = model(X_s_t)\n",
    "        class_out_t, _, feat_t = model(X_t_t)\n",
    "        \n",
    "        source_pred = class_out_s.argmax(dim=1).numpy()\n",
    "        target_pred = class_out_t.argmax(dim=1).numpy()\n",
    "        \n",
    "        source_acc = (source_pred == y_s).mean()\n",
    "        target_acc = (target_pred == y_t).mean()\n",
    "        \n",
    "    print(f\"\\nDANN çµæœ:\")\n",
    "    print(f\"Source Domain æº–ç¢ºç‡: {source_acc:.2%}\")\n",
    "    print(f\"Target Domain æº–ç¢ºç‡: {target_acc:.2%}\")\n",
    "    \n",
    "    # è¦–è¦ºåŒ–ç‰¹å¾µç©ºé–“\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    feat_s_np = feat_s.numpy()\n",
    "    feat_t_np = feat_t.numpy()\n",
    "    \n",
    "    # ä½¿ç”¨ PCA é™ç¶­åˆ° 2Dï¼ˆå¦‚æœç‰¹å¾µç¶­åº¦ > 2ï¼‰\n",
    "    if feat_s_np.shape[1] > 2:\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        all_feat = np.vstack([feat_s_np, feat_t_np])\n",
    "        all_feat_2d = pca.fit_transform(all_feat)\n",
    "        feat_s_2d = all_feat_2d[:len(feat_s_np)]\n",
    "        feat_t_2d = all_feat_2d[len(feat_s_np):]\n",
    "    else:\n",
    "        feat_s_2d = feat_s_np\n",
    "        feat_t_2d = feat_t_np\n",
    "    \n",
    "    # æŒ‰é ˜åŸŸè‘—è‰²\n",
    "    axes[0].scatter(feat_s_2d[:, 0], feat_s_2d[:, 1], c='blue', alpha=0.5, label='Source')\n",
    "    axes[0].scatter(feat_t_2d[:, 0], feat_t_2d[:, 1], c='red', alpha=0.5, label='Target')\n",
    "    axes[0].set_title('ç‰¹å¾µç©ºé–“ï¼ˆæŒ‰é ˜åŸŸè‘—è‰²ï¼‰')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # æŒ‰é¡åˆ¥è‘—è‰²\n",
    "    colors_s = ['blue' if y == 0 else 'cyan' for y in y_s]\n",
    "    colors_t = ['red' if y == 0 else 'orange' for y in y_t]\n",
    "    axes[1].scatter(feat_s_2d[:, 0], feat_s_2d[:, 1], c=colors_s, alpha=0.5, marker='o')\n",
    "    axes[1].scatter(feat_t_2d[:, 0], feat_t_2d[:, 1], c=colors_t, alpha=0.5, marker='x')\n",
    "    axes[1].set_title('ç‰¹å¾µç©ºé–“ï¼ˆæŒ‰é¡åˆ¥è‘—è‰²ï¼‰')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "evaluate_dann(dann_model, X_s, y_s, X_t, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Self-Training (Pseudo-Labeling)\n",
    "\n",
    "ä½¿ç”¨æ¨¡å‹é æ¸¬çš„å½æ¨™ç±¤ä¾†é€²è¡Œè‡ªè¨“ç·´ã€‚\n",
    "\n",
    "```\n",
    "Self-Training æµç¨‹ï¼š\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Step 1: åœ¨ Source ä¸Šè¨“ç·´åˆå§‹æ¨¡å‹                               â”‚\n",
    "â”‚          Model_0 â† train(Source_labeled)                        â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Step 2: å° Target ç”Ÿæˆå½æ¨™ç±¤                                   â”‚\n",
    "â”‚          pseudo_labels â† Model_t(Target_unlabeled)              â”‚\n",
    "â”‚          é¸æ“‡é«˜ç½®ä¿¡åº¦æ¨£æœ¬                                       â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Step 3: åˆä½µè¨“ç·´                                               â”‚\n",
    "â”‚          Model_{t+1} â† train(Source + Target_pseudo)            â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Step 4: é‡è¤‡ Step 2-3 ç›´åˆ°æ”¶æ–‚                                 â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  ç½®ä¿¡åº¦é–¾å€¼é¸æ“‡ï¼š                                               â”‚\n",
    "â”‚  â”œâ”€ å¤ªé«˜ï¼šåˆ©ç”¨çš„ target æ¨£æœ¬å¤ªå°‘                               â”‚\n",
    "â”‚  â””â”€ å¤ªä½ï¼šå¼•å…¥å¤ªå¤šéŒ¯èª¤æ¨™ç±¤                                     â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfTrainer:\n",
    "    \"\"\"Self-Training Domain Adaptation\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        confidence_threshold: float = 0.9,\n",
    "        max_iterations: int = 10\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.max_iterations = max_iterations\n",
    "        \n",
    "    def generate_pseudo_labels(\n",
    "        self,\n",
    "        X_target: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆå½æ¨™ç±¤\n",
    "        \n",
    "        Returns:\n",
    "            pseudo_labels: å½æ¨™ç±¤\n",
    "            confidence: ç½®ä¿¡åº¦\n",
    "            mask: é«˜ç½®ä¿¡åº¦æ¨£æœ¬é®ç½©\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(X_target)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            confidence, pseudo_labels = probs.max(dim=1)\n",
    "            mask = confidence >= self.confidence_threshold\n",
    "            \n",
    "        return pseudo_labels, confidence, mask\n",
    "    \n",
    "    def train_step(\n",
    "        self,\n",
    "        X_source: torch.Tensor,\n",
    "        y_source: torch.Tensor,\n",
    "        X_target_pseudo: torch.Tensor,\n",
    "        y_target_pseudo: torch.Tensor,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        source_weight: float = 1.0,\n",
    "        target_weight: float = 1.0\n",
    "    ) -> float:\n",
    "        \"\"\"å–®æ­¥è¨“ç·´\"\"\"\n",
    "        self.model.train()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Source loss\n",
    "        source_out = self.model(X_source)\n",
    "        source_loss = criterion(source_out, y_source)\n",
    "        \n",
    "        # Target pseudo loss\n",
    "        if len(X_target_pseudo) > 0:\n",
    "            target_out = self.model(X_target_pseudo)\n",
    "            target_loss = criterion(target_out, y_target_pseudo)\n",
    "            total_loss = source_weight * source_loss + target_weight * target_loss\n",
    "        else:\n",
    "            total_loss = source_loss\n",
    "            \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return total_loss.item()\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        X_source: torch.Tensor,\n",
    "        y_source: torch.Tensor,\n",
    "        X_target: torch.Tensor,\n",
    "        epochs_per_iteration: int = 50,\n",
    "        lr: float = 0.01\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        å®Œæ•´çš„ self-training æµç¨‹\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        history = []\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            print(f\"\\nIteration {iteration + 1}/{self.max_iterations}\")\n",
    "            \n",
    "            # ç”Ÿæˆå½æ¨™ç±¤\n",
    "            pseudo_labels, confidence, mask = self.generate_pseudo_labels(X_target)\n",
    "            \n",
    "            num_selected = mask.sum().item()\n",
    "            print(f\"  é¸æ“‡äº† {num_selected}/{len(X_target)} å€‹é«˜ç½®ä¿¡åº¦æ¨£æœ¬\")\n",
    "            \n",
    "            if num_selected == 0:\n",
    "                print(\"  æ²’æœ‰é«˜ç½®ä¿¡åº¦æ¨£æœ¬ï¼Œé™ä½é–¾å€¼æˆ–åœæ­¢\")\n",
    "                break\n",
    "                \n",
    "            # è¨“ç·´\n",
    "            X_target_pseudo = X_target[mask]\n",
    "            y_target_pseudo = pseudo_labels[mask]\n",
    "            \n",
    "            for epoch in range(epochs_per_iteration):\n",
    "                loss = self.train_step(\n",
    "                    X_source, y_source,\n",
    "                    X_target_pseudo, y_target_pseudo,\n",
    "                    optimizer\n",
    "                )\n",
    "                \n",
    "            print(f\"  æœ€çµ‚ Loss: {loss:.4f}\")\n",
    "            \n",
    "            history.append({\n",
    "                'iteration': iteration,\n",
    "                'num_pseudo': num_selected,\n",
    "                'loss': loss\n",
    "            })\n",
    "            \n",
    "        return history\n",
    "\n",
    "\n",
    "# æ¸¬è©¦\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model_st = SimpleModel()\n",
    "trainer = SelfTrainer(model_st, confidence_threshold=0.8, max_iterations=5)\n",
    "history = trainer.fit(X_s_t, y_s_t, X_t_t, epochs_per_iteration=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è©•ä¼° Self-Training\n",
    "model_st.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model_st(X_t_t).argmax(dim=1).numpy()\n",
    "    acc = (pred == y_t).mean()\n",
    "    \n",
    "print(f\"Self-Training Target æº–ç¢ºç‡: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: æ–¹æ³•æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comparison_table():\n",
    "    \"\"\"å°å‡º Domain Adaptation æ–¹æ³•æ¯”è¼ƒè¡¨\"\"\"\n",
    "    print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                     Domain Adaptation æ–¹æ³•æ¯”è¼ƒ                                â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘       æ–¹æ³•        â•‘                      ç‰¹é»                                 â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                   â•‘ - æœ€å°åŒ–é ˜åŸŸé–“çš„åˆ†ä½ˆå·®ç•°ï¼ˆMMD, CORALï¼‰                    â•‘\n",
    "â•‘  Discrepancy      â•‘ - ç°¡å–®ç›´è§€ï¼Œæ˜“æ–¼å¯¦ç¾                                      â•‘\n",
    "â•‘  -based           â•‘ - ä¸éœ€è¦å°æŠ—è¨“ç·´                                          â•‘\n",
    "â•‘                   â•‘ - å°æ–¼å¤§çš„ domain shift æ•ˆæœæœ‰é™                          â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                   â•‘ - ä½¿ç”¨å°æŠ—å­¸ç¿’ç”¢ç”Ÿé ˜åŸŸä¸è®Šç‰¹å¾µï¼ˆDANN, ADDAï¼‰              â•‘\n",
    "â•‘  Adversarial      â•‘ - ç†è«–åŸºç¤ç´®å¯¦ï¼ˆH-divergenceï¼‰                            â•‘\n",
    "â•‘  -based           â•‘ - æ•ˆæœé€šå¸¸è¼ƒå¥½                                            â•‘\n",
    "â•‘                   â•‘ - è¨“ç·´ä¸ç©©å®šï¼Œéœ€è¦ä»”ç´°èª¿åƒ                                â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                   â•‘ - ä½¿ç”¨å½æ¨™ç±¤é€²è¡Œè‡ªè¨“ç·´                                    â•‘\n",
    "â•‘  Self-Training    â•‘ - ç°¡å–®ï¼Œä¸éœ€è¦ä¿®æ”¹ç¶²è·¯æ¶æ§‹                                â•‘\n",
    "â•‘                   â•‘ - å¯èƒ½ç´¯ç©éŒ¯èª¤                                            â•‘\n",
    "â•‘                   â•‘ - éœ€è¦åˆé©çš„ç½®ä¿¡åº¦é–¾å€¼                                    â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                   â•‘ - ç”Ÿæˆç›®æ¨™é¢¨æ ¼çš„æ•¸æ“š                                      â•‘\n",
    "â•‘  Generation       â•‘ - CycleGAN, PixelDA                                       â•‘\n",
    "â•‘  -based           â•‘ - éœ€è¦é¡å¤–çš„ç”Ÿæˆæ¨¡å‹                                      â•‘\n",
    "â•‘                   â•‘ - é©ç”¨æ–¼è¦–è¦ºä»»å‹™                                          â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "é¸æ“‡æŒ‡å—ï¼š\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ å ´æ™¯                              â”‚ æ¨è–¦æ–¹æ³•                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ å° domain shift                   â”‚ Discrepancy-based (CORAL)  â”‚\n",
    "â”‚ ä¸­ç­‰ domain shift                 â”‚ DANN / ADDA                â”‚\n",
    "â”‚ æœ‰å°‘é‡ç›®æ¨™æ¨™ç±¤                    â”‚ Self-Training + Fine-tune  â”‚\n",
    "â”‚ è¦–è¦ºä»»å‹™ï¼ˆåœ–åƒé¢¨æ ¼å·®ç•°ï¼‰          â”‚ Generation-based           â”‚\n",
    "â”‚ éœ€è¦ç©©å®šè¨“ç·´                      â”‚ MMD + Self-Training        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "print_comparison_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç·´ç¿’\n",
    "\n",
    "### Exercise 1: å¯¦ä½œ ADDA (Adversarial Discriminative Domain Adaptation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADDA:\n",
    "    \"\"\"Adversarial Discriminative Domain Adaptation\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        source_encoder: nn.Module,\n",
    "        target_encoder: nn.Module,  # åˆå§‹åŒ–ç‚º source_encoder çš„æ‹·è²\n",
    "        classifier: nn.Module,\n",
    "        discriminator: nn.Module\n",
    "    ):\n",
    "        # TODO: å¯¦ä½œ ADDA\n",
    "        # ADDA èˆ‡ DANN çš„ä¸»è¦å€åˆ¥ï¼š\n",
    "        # 1. æœ‰å…©å€‹ç¨ç«‹çš„ç·¨ç¢¼å™¨ï¼ˆsource å’Œ targetï¼‰\n",
    "        # 2. è¨“ç·´åˆ†å…©éšæ®µï¼š\n",
    "        #    - å…ˆåœ¨ source ä¸Šè¨“ç·´ source_encoder + classifier\n",
    "        #    - å†å›ºå®š source_encoderï¼Œç”¨å°æŠ—å­¸ç¿’è¨“ç·´ target_encoder\n",
    "        pass\n",
    "    \n",
    "    def pretrain_source(self, X_source, y_source, epochs=100):\n",
    "        \"\"\"åœ¨ source ä¸Šé è¨“ç·´\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def adapt_target(self, X_source, X_target, epochs=100):\n",
    "        \"\"\"å°æŠ—é©æ‡‰ target encoder\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: å¯¦ä½œ Domain-Specific Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainSpecificBatchNorm(nn.Module):\n",
    "    \"\"\"Domain-Specific Batch Normalization\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features: int, num_domains: int = 2):\n",
    "        super().__init__()\n",
    "        # TODO: å¯¦ä½œ DSBN\n",
    "        # ç‚ºæ¯å€‹ domain ç¶­è­·ç¨ç«‹çš„ BN çµ±è¨ˆé‡\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, domain_idx: int) -> torch.Tensor:\n",
    "        # æ ¹æ“š domain_idx é¸æ“‡å°æ‡‰çš„ BN\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: æ¯”è¼ƒä¸åŒæ–¹æ³•åœ¨ä¸åŒ domain shift ç¨‹åº¦ä¸‹çš„è¡¨ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_methods_across_shifts():\n",
    "    \"\"\"\n",
    "    æ¯”è¼ƒä¸åŒ DA æ–¹æ³•åœ¨ä¸åŒ domain shift ç¨‹åº¦ä¸‹çš„è¡¨ç¾\n",
    "    \n",
    "    TODO:\n",
    "    1. å‰µå»ºä¸åŒ shift ç¨‹åº¦çš„æ•¸æ“šï¼ˆshift = [0.5, 1.0, 2.0, 3.0]ï¼‰\n",
    "    2. è¨“ç·´å„ç¨®æ–¹æ³•ï¼šBaseline, CORAL, DANN, Self-Training\n",
    "    3. è¨˜éŒ„ target æº–ç¢ºç‡\n",
    "    4. ç¹ªè£½æ¯”è¼ƒåœ–\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¸½çµ\n",
    "\n",
    "```\n",
    "Domain Adaptation é‡é»å›é¡§ï¼š\n",
    "\n",
    "1. å•é¡Œå®šç¾©\n",
    "   â”œâ”€ Source Domain: æœ‰æ¨™ç±¤\n",
    "   â”œâ”€ Target Domain: ç„¡/å°‘æ¨™ç±¤\n",
    "   â””â”€ ç›®æ¨™: åœ¨ Target ä¸Šè¡¨ç¾è‰¯å¥½\n",
    "\n",
    "2. Discrepancy-based\n",
    "   â”œâ”€ MMD: æœ€å°åŒ–å‡å€¼å·®ç•°\n",
    "   â”œâ”€ CORAL: å°é½Šå”æ–¹å·®çŸ©é™£\n",
    "   â””â”€ ç°¡å–®ä½†å°å¤§ shift æ•ˆæœæœ‰é™\n",
    "\n",
    "3. Adversarial-based\n",
    "   â”œâ”€ DANN: GRL + Domain Discriminator\n",
    "   â”œâ”€ ADDA: å…©éšæ®µè¨“ç·´\n",
    "   â””â”€ æ•ˆæœå¥½ä½†è¨“ç·´ä¸ç©©å®š\n",
    "\n",
    "4. Self-Training\n",
    "   â”œâ”€ å½æ¨™ç±¤è¿­ä»£è¨“ç·´\n",
    "   â”œâ”€ ç½®ä¿¡åº¦é–¾å€¼é¸æ“‡é‡è¦\n",
    "   â””â”€ å¯èƒ½ç´¯ç©éŒ¯èª¤\n",
    "\n",
    "å¯¦éš›æ‡‰ç”¨å»ºè­°ï¼š\n",
    "- å…ˆå˜—è©¦ç°¡å–®æ–¹æ³•ï¼ˆCORAL, Self-Trainingï¼‰\n",
    "- æ•ˆæœä¸ä½³å†è€ƒæ…® DANN\n",
    "- çµåˆå¤šç¨®æ–¹æ³•é€šå¸¸æ•ˆæœæ›´å¥½\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
