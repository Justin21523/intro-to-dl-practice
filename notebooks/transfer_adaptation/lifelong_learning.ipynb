{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifelong Learning (終身學習 / 持續學習)\n",
    "\n",
    "**對應課程**: 李宏毅 2021 ML HW14 - Life-Long Learning\n",
    "\n",
    "本 notebook 介紹如何讓模型持續學習新任務而不遺忘舊知識：\n",
    "- **問題定義**: 持續學習場景與挑戰\n",
    "- **Replay-based**: 經驗回放、生成式回放\n",
    "- **Architecture-based**: PackNet, Progressive Neural Networks\n",
    "- **評估指標**: Average Accuracy, Forgetting, Forward Transfer\n",
    "\n",
    "```\n",
    "持續學習場景：\n",
    "\n",
    "時間軸\n",
    "──────────────────────────────────────────────────────────►\n",
    "    Task 1        Task 2        Task 3        Task 4\n",
    "   (貓狗分類)   (車輛識別)    (花卉分類)    (手寫數字)\n",
    "      │            │            │            │\n",
    "      ▼            ▼            ▼            ▼\n",
    "  ┌───────┐    ┌───────┐    ┌───────┐    ┌───────┐\n",
    "  │ Model │ ─► │ Model │ ─► │ Model │ ─► │ Model │\n",
    "  │  M₁   │    │  M₂   │    │  M₃   │    │  M₄   │\n",
    "  └───────┘    └───────┘    └───────┘    └───────┘\n",
    "\n",
    "挑戰：\n",
    "- 災難性遺忘：學習 Task 2 後忘記 Task 1\n",
    "- 無法存取舊任務數據\n",
    "- 需要在新舊任務間取得平衡\n",
    "\n",
    "相關 Notebook:\n",
    "- catastrophic_forgetting.ipynb: EWC, Synaptic Intelligence 等正則化方法\n",
    "- 本 notebook 聚焦於 Replay-based 和 Architecture-based 方法\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List, Dict, Tuple, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: 持續學習問題設定\n",
    "\n",
    "創建一個多任務序列學習的場景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Task:\n",
    "    \"\"\"任務定義\"\"\"\n",
    "    task_id: int\n",
    "    name: str\n",
    "    X_train: torch.Tensor\n",
    "    y_train: torch.Tensor\n",
    "    X_test: torch.Tensor\n",
    "    y_test: torch.Tensor\n",
    "    num_classes: int\n",
    "\n",
    "\n",
    "def create_permuted_mnist_tasks(\n",
    "    num_tasks: int = 5,\n",
    "    num_samples: int = 1000,\n",
    "    input_dim: int = 784\n",
    ") -> List[Task]:\n",
    "    \"\"\"\n",
    "    創建 Permuted MNIST 風格的任務序列\n",
    "    每個任務使用不同的像素排列\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    tasks = []\n",
    "    \n",
    "    for task_id in range(num_tasks):\n",
    "        # 生成隨機排列（第一個任務使用原始順序）\n",
    "        if task_id == 0:\n",
    "            perm = np.arange(input_dim)\n",
    "        else:\n",
    "            perm = np.random.permutation(input_dim)\n",
    "            \n",
    "        # 生成模擬數據（簡化版）\n",
    "        num_classes = 10\n",
    "        \n",
    "        # 訓練數據\n",
    "        X_train = torch.randn(num_samples, input_dim)\n",
    "        y_train = torch.randint(0, num_classes, (num_samples,))\n",
    "        # 應用排列\n",
    "        X_train = X_train[:, perm]\n",
    "        \n",
    "        # 測試數據\n",
    "        X_test = torch.randn(num_samples // 5, input_dim)\n",
    "        y_test = torch.randint(0, num_classes, (num_samples // 5,))\n",
    "        X_test = X_test[:, perm]\n",
    "        \n",
    "        task = Task(\n",
    "            task_id=task_id,\n",
    "            name=f\"Permuted Task {task_id}\",\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            num_classes=num_classes\n",
    "        )\n",
    "        tasks.append(task)\n",
    "        \n",
    "    return tasks\n",
    "\n",
    "\n",
    "def create_split_tasks(\n",
    "    num_tasks: int = 5,\n",
    "    classes_per_task: int = 2,\n",
    "    num_samples: int = 500\n",
    ") -> List[Task]:\n",
    "    \"\"\"\n",
    "    創建 Split 風格的任務序列\n",
    "    每個任務只包含部分類別\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    tasks = []\n",
    "    input_dim = 64\n",
    "    \n",
    "    for task_id in range(num_tasks):\n",
    "        # 每個任務的類別範圍\n",
    "        start_class = task_id * classes_per_task\n",
    "        \n",
    "        # 生成數據\n",
    "        X_train_list = []\n",
    "        y_train_list = []\n",
    "        \n",
    "        for c in range(classes_per_task):\n",
    "            # 每個類別有不同的均值\n",
    "            class_mean = torch.randn(input_dim) * 2\n",
    "            X_c = torch.randn(num_samples // classes_per_task, input_dim) + class_mean\n",
    "            y_c = torch.full((num_samples // classes_per_task,), c, dtype=torch.long)\n",
    "            X_train_list.append(X_c)\n",
    "            y_train_list.append(y_c)\n",
    "            \n",
    "        X_train = torch.cat(X_train_list)\n",
    "        y_train = torch.cat(y_train_list)\n",
    "        \n",
    "        # 打亂順序\n",
    "        perm = torch.randperm(len(X_train))\n",
    "        X_train = X_train[perm]\n",
    "        y_train = y_train[perm]\n",
    "        \n",
    "        # 測試數據（類似方式生成）\n",
    "        X_test_list = []\n",
    "        y_test_list = []\n",
    "        for c in range(classes_per_task):\n",
    "            class_mean = torch.randn(input_dim) * 2\n",
    "            X_c = torch.randn(num_samples // classes_per_task // 5, input_dim) + class_mean\n",
    "            y_c = torch.full((num_samples // classes_per_task // 5,), c, dtype=torch.long)\n",
    "            X_test_list.append(X_c)\n",
    "            y_test_list.append(y_c)\n",
    "            \n",
    "        X_test = torch.cat(X_test_list)\n",
    "        y_test = torch.cat(y_test_list)\n",
    "        \n",
    "        task = Task(\n",
    "            task_id=task_id,\n",
    "            name=f\"Split Task {task_id} (classes {start_class}-{start_class+classes_per_task-1})\",\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            num_classes=classes_per_task\n",
    "        )\n",
    "        tasks.append(task)\n",
    "        \n",
    "    return tasks\n",
    "\n",
    "\n",
    "# 創建任務\n",
    "tasks = create_split_tasks(num_tasks=5, classes_per_task=2)\n",
    "print(\"創建的任務：\")\n",
    "for task in tasks:\n",
    "    print(f\"  {task.name}: {len(task.X_train)} 訓練樣本, {task.num_classes} 類別\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Replay-based Methods\n",
    "\n",
    "通過保存或生成舊任務的樣本來防止遺忘。\n",
    "\n",
    "```\n",
    "Replay 方法分類：\n",
    "\n",
    "1. Experience Replay (ER)\n",
    "   ├─ 保存部分舊樣本到記憶緩衝區\n",
    "   ├─ 訓練時混合新舊樣本\n",
    "   └─ 簡單有效，但需要存儲空間\n",
    "\n",
    "2. Generative Replay (GR)\n",
    "   ├─ 訓練生成器生成舊任務樣本\n",
    "   ├─ 不需要存儲真實樣本\n",
    "   └─ 生成品質影響效果\n",
    "\n",
    "3. Gradient Episodic Memory (GEM)\n",
    "   ├─ 保存樣本用於約束梯度方向\n",
    "   ├─ 確保更新不損害舊任務\n",
    "   └─ 計算成本較高\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"經驗回放緩衝區\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        buffer_size: int = 1000,\n",
    "        strategy: str = 'reservoir'  # 'reservoir' or 'ring'\n",
    "    ):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.strategy = strategy\n",
    "        \n",
    "        self.buffer_x: List[torch.Tensor] = []\n",
    "        self.buffer_y: List[torch.Tensor] = []\n",
    "        self.buffer_task: List[int] = []\n",
    "        \n",
    "        self.num_seen = 0\n",
    "        \n",
    "    def add(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        y: torch.Tensor,\n",
    "        task_id: int\n",
    "    ):\n",
    "        \"\"\"添加樣本到緩衝區\"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            self.num_seen += 1\n",
    "            \n",
    "            if self.strategy == 'reservoir':\n",
    "                # Reservoir Sampling：保持均勻分佈\n",
    "                if len(self.buffer_x) < self.buffer_size:\n",
    "                    self.buffer_x.append(x[i].clone())\n",
    "                    self.buffer_y.append(y[i].clone())\n",
    "                    self.buffer_task.append(task_id)\n",
    "                else:\n",
    "                    # 以 buffer_size / num_seen 的概率替換\n",
    "                    j = np.random.randint(0, self.num_seen)\n",
    "                    if j < self.buffer_size:\n",
    "                        self.buffer_x[j] = x[i].clone()\n",
    "                        self.buffer_y[j] = y[i].clone()\n",
    "                        self.buffer_task[j] = task_id\n",
    "                        \n",
    "            elif self.strategy == 'ring':\n",
    "                # Ring Buffer：FIFO\n",
    "                if len(self.buffer_x) < self.buffer_size:\n",
    "                    self.buffer_x.append(x[i].clone())\n",
    "                    self.buffer_y.append(y[i].clone())\n",
    "                    self.buffer_task.append(task_id)\n",
    "                else:\n",
    "                    idx = self.num_seen % self.buffer_size\n",
    "                    self.buffer_x[idx] = x[i].clone()\n",
    "                    self.buffer_y[idx] = y[i].clone()\n",
    "                    self.buffer_task[idx] = task_id\n",
    "                    \n",
    "    def sample(self, batch_size: int) -> Tuple[torch.Tensor, torch.Tensor, List[int]]:\n",
    "        \"\"\"從緩衝區採樣\"\"\"\n",
    "        if len(self.buffer_x) == 0:\n",
    "            return None, None, None\n",
    "            \n",
    "        indices = np.random.choice(\n",
    "            len(self.buffer_x),\n",
    "            size=min(batch_size, len(self.buffer_x)),\n",
    "            replace=False\n",
    "        )\n",
    "        \n",
    "        x = torch.stack([self.buffer_x[i] for i in indices])\n",
    "        y = torch.stack([self.buffer_y[i] for i in indices])\n",
    "        task_ids = [self.buffer_task[i] for i in indices]\n",
    "        \n",
    "        return x, y, task_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer_x)\n",
    "    \n",
    "    def get_task_distribution(self) -> Dict[int, int]:\n",
    "        \"\"\"獲取緩衝區中各任務的樣本數量\"\"\"\n",
    "        dist = defaultdict(int)\n",
    "        for t in self.buffer_task:\n",
    "            dist[t] += 1\n",
    "        return dict(dist)\n",
    "\n",
    "\n",
    "# 測試\n",
    "buffer = ReplayBuffer(buffer_size=100)\n",
    "\n",
    "# 添加多個任務的樣本\n",
    "for task in tasks[:3]:\n",
    "    buffer.add(task.X_train[:50], task.y_train[:50], task.task_id)\n",
    "    \n",
    "print(f\"緩衝區大小: {len(buffer)}\")\n",
    "print(f\"任務分佈: {buffer.get_task_distribution()}\")\n",
    "\n",
    "# 採樣\n",
    "x, y, t = buffer.sample(16)\n",
    "print(f\"採樣形狀: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    \"\"\"經驗回放訓練器\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        buffer_size: int = 500,\n",
    "        replay_batch_size: int = 16,\n",
    "        replay_freq: int = 1  # 每幾個 batch 做一次 replay\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "        self.replay_batch_size = replay_batch_size\n",
    "        self.replay_freq = replay_freq\n",
    "        \n",
    "    def train_task(\n",
    "        self,\n",
    "        task: Task,\n",
    "        epochs: int = 10,\n",
    "        batch_size: int = 32,\n",
    "        lr: float = 0.01\n",
    "    ) -> List[float]:\n",
    "        \"\"\"訓練單個任務（帶經驗回放）\"\"\"\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        losses = []\n",
    "        batch_count = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # 打亂數據\n",
    "            perm = torch.randperm(len(task.X_train))\n",
    "            X = task.X_train[perm]\n",
    "            y = task.y_train[perm]\n",
    "            \n",
    "            for i in range(0, len(X), batch_size):\n",
    "                x_batch = X[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 當前任務損失\n",
    "                output = self.model(x_batch)\n",
    "                loss = criterion(output, y_batch)\n",
    "                \n",
    "                # 經驗回放損失\n",
    "                if batch_count % self.replay_freq == 0 and len(self.buffer) > 0:\n",
    "                    x_replay, y_replay, _ = self.buffer.sample(self.replay_batch_size)\n",
    "                    if x_replay is not None:\n",
    "                        output_replay = self.model(x_replay)\n",
    "                        loss_replay = criterion(output_replay, y_replay)\n",
    "                        loss = loss + loss_replay\n",
    "                        \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "                batch_count += 1\n",
    "                \n",
    "        # 將當前任務的部分樣本加入緩衝區\n",
    "        num_to_add = min(100, len(task.X_train))\n",
    "        indices = np.random.choice(len(task.X_train), num_to_add, replace=False)\n",
    "        self.buffer.add(\n",
    "            task.X_train[indices],\n",
    "            task.y_train[indices],\n",
    "            task.task_id\n",
    "        )\n",
    "        \n",
    "        return losses\n",
    "\n",
    "\n",
    "# 測試\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim=64, hidden_dim=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model_er = SimpleMLP()\n",
    "er_trainer = ExperienceReplay(model_er, buffer_size=200)\n",
    "\n",
    "print(\"使用經驗回放訓練...\")\n",
    "for i, task in enumerate(tasks[:3]):\n",
    "    print(f\"\\n訓練 {task.name}\")\n",
    "    losses = er_trainer.train_task(task, epochs=5)\n",
    "    print(f\"  最終 Loss: {np.mean(losses[-10:]):.4f}\")\n",
    "    print(f\"  緩衝區分佈: {er_trainer.buffer.get_task_distribution()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeReplay:\n",
    "    \"\"\"生成式回放（使用 VAE）\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier: nn.Module,\n",
    "        generator: nn.Module,  # VAE or GAN\n",
    "        input_dim: int = 64,\n",
    "        latent_dim: int = 32\n",
    "    ):\n",
    "        self.classifier = classifier\n",
    "        self.generator = generator\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # 舊的分類器和生成器（用於生成舊樣本）\n",
    "        self.old_classifier = None\n",
    "        self.old_generator = None\n",
    "        \n",
    "    def generate_old_samples(\n",
    "        self,\n",
    "        num_samples: int\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"生成舊任務的樣本\"\"\"\n",
    "        if self.old_generator is None:\n",
    "            return None, None\n",
    "            \n",
    "        self.old_generator.eval()\n",
    "        self.old_classifier.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # 從隱空間採樣\n",
    "            z = torch.randn(num_samples, self.latent_dim)\n",
    "            \n",
    "            # 生成樣本\n",
    "            x_generated = self.old_generator.decode(z)\n",
    "            \n",
    "            # 使用舊分類器產生軟標籤\n",
    "            y_soft = F.softmax(self.old_classifier(x_generated), dim=1)\n",
    "            \n",
    "        return x_generated, y_soft\n",
    "    \n",
    "    def update_old_models(self):\n",
    "        \"\"\"更新舊模型\"\"\"\n",
    "        self.old_classifier = copy.deepcopy(self.classifier)\n",
    "        self.old_generator = copy.deepcopy(self.generator)\n",
    "        \n",
    "        # 凍結舊模型\n",
    "        for param in self.old_classifier.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.old_generator.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "class SimpleVAE(nn.Module):\n",
    "    \"\"\"簡單的 VAE 用於生成式回放\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 64, hidden_dim: int = 128, latent_dim: int = 32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_var(h)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var\n",
    "\n",
    "\n",
    "print(\"生成式回放架構已定義\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Architecture-based Methods\n",
    "\n",
    "通過修改網路架構來分配不同任務的參數。\n",
    "\n",
    "```\n",
    "Architecture-based 方法：\n",
    "\n",
    "1. PackNet\n",
    "   ├─ 訓練後剪枝，釋放參數給新任務\n",
    "   ├─ 凍結舊任務參數\n",
    "   └─ 無遺忘，但容量有限\n",
    "\n",
    "2. Progressive Neural Networks\n",
    "   ├─ 為每個任務添加新的網路列\n",
    "   ├─ 使用橫向連接傳遞知識\n",
    "   └─ 無遺忘，但模型不斷增長\n",
    "\n",
    "3. Dynamic Expandable Network (DEN)\n",
    "   ├─ 選擇性地擴展網路\n",
    "   ├─ 重用相關神經元\n",
    "   └─ 平衡容量和效率\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackNet:\n",
    "    \"\"\"PackNet: 通過剪枝和凍結實現持續學習\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        prune_ratio: float = 0.5  # 每個任務使用的參數比例\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.prune_ratio = prune_ratio\n",
    "        \n",
    "        # 記錄每個任務使用的參數遮罩\n",
    "        self.task_masks: Dict[int, Dict[str, torch.Tensor]] = {}\n",
    "        \n",
    "        # 已使用的參數遮罩（累積）\n",
    "        self.used_mask: Dict[str, torch.Tensor] = {}\n",
    "        \n",
    "        # 初始化遮罩\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                self.used_mask[name] = torch.zeros_like(param.data)\n",
    "                \n",
    "    def train_task(\n",
    "        self,\n",
    "        task: Task,\n",
    "        epochs: int = 20,\n",
    "        batch_size: int = 32,\n",
    "        lr: float = 0.01\n",
    "    ):\n",
    "        \"\"\"訓練任務並進行剪枝\"\"\"\n",
    "        # 獲取可用參數遮罩\n",
    "        available_mask = {}\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.used_mask:\n",
    "                available_mask[name] = 1 - self.used_mask[name]\n",
    "                \n",
    "        # 訓練\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            perm = torch.randperm(len(task.X_train))\n",
    "            X = task.X_train[perm]\n",
    "            y = task.y_train[perm]\n",
    "            \n",
    "            for i in range(0, len(X), batch_size):\n",
    "                x_batch = X[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 應用遮罩（只更新可用參數）\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    if name in available_mask:\n",
    "                        param.data *= available_mask[name]\n",
    "                        \n",
    "                output = self.model(x_batch)\n",
    "                loss = criterion(output, y_batch)\n",
    "                loss.backward()\n",
    "                \n",
    "                # 遮罩梯度\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    if name in available_mask and param.grad is not None:\n",
    "                        param.grad *= available_mask[name]\n",
    "                        \n",
    "                optimizer.step()\n",
    "                \n",
    "        # 剪枝：選擇最重要的參數\n",
    "        task_mask = self._prune_and_freeze(task.task_id, available_mask)\n",
    "        self.task_masks[task.task_id] = task_mask\n",
    "        \n",
    "        # 更新已使用遮罩\n",
    "        for name in task_mask:\n",
    "            self.used_mask[name] = self.used_mask[name] + task_mask[name]\n",
    "            self.used_mask[name] = torch.clamp(self.used_mask[name], 0, 1)\n",
    "            \n",
    "    def _prune_and_freeze(\n",
    "        self,\n",
    "        task_id: int,\n",
    "        available_mask: Dict[str, torch.Tensor]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"剪枝並凍結參數\"\"\"\n",
    "        task_mask = {}\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name not in available_mask:\n",
    "                continue\n",
    "                \n",
    "            # 計算可用參數數量\n",
    "            available = available_mask[name]\n",
    "            num_available = int(available.sum().item())\n",
    "            \n",
    "            if num_available == 0:\n",
    "                task_mask[name] = torch.zeros_like(param.data)\n",
    "                continue\n",
    "                \n",
    "            # 選擇最重要的參數（按絕對值）\n",
    "            num_to_keep = int(num_available * self.prune_ratio)\n",
    "            \n",
    "            # 只考慮可用參數\n",
    "            masked_weights = param.data.abs() * available\n",
    "            threshold = torch.topk(masked_weights.flatten(), num_to_keep).values.min()\n",
    "            \n",
    "            # 創建任務遮罩\n",
    "            mask = (masked_weights >= threshold).float() * available\n",
    "            task_mask[name] = mask\n",
    "            \n",
    "        return task_mask\n",
    "    \n",
    "    def get_capacity_usage(self) -> float:\n",
    "        \"\"\"獲取已使用的容量比例\"\"\"\n",
    "        total = 0\n",
    "        used = 0\n",
    "        for name, mask in self.used_mask.items():\n",
    "            total += mask.numel()\n",
    "            used += mask.sum().item()\n",
    "        return used / total if total > 0 else 0\n",
    "\n",
    "\n",
    "# 測試\n",
    "model_packnet = SimpleMLP()\n",
    "packnet = PackNet(model_packnet, prune_ratio=0.3)\n",
    "\n",
    "print(\"使用 PackNet 訓練...\")\n",
    "for task in tasks[:3]:\n",
    "    print(f\"\\n訓練 {task.name}\")\n",
    "    packnet.train_task(task, epochs=10)\n",
    "    print(f\"  容量使用: {packnet.get_capacity_usage():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressiveNeuralNetwork(nn.Module):\n",
    "    \"\"\"Progressive Neural Networks\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 64,\n",
    "        hidden_dim: int = 64,\n",
    "        num_classes: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # 存儲每個任務的網路列\n",
    "        self.columns: nn.ModuleList = nn.ModuleList()\n",
    "        \n",
    "        # 橫向連接\n",
    "        self.lateral_connections: nn.ModuleList = nn.ModuleList()\n",
    "        \n",
    "        # 任務特定的輸出頭\n",
    "        self.heads: nn.ModuleList = nn.ModuleList()\n",
    "        \n",
    "        self.num_tasks = 0\n",
    "        \n",
    "    def add_task(self):\n",
    "        \"\"\"為新任務添加網路列\"\"\"\n",
    "        task_id = self.num_tasks\n",
    "        \n",
    "        # 新的網路列（2 層）\n",
    "        column = nn.ModuleList([\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        ])\n",
    "        self.columns.append(column)\n",
    "        \n",
    "        # 橫向連接（從舊列到新列）\n",
    "        if task_id > 0:\n",
    "            lateral = nn.ModuleList([\n",
    "                nn.Linear(self.hidden_dim * task_id, self.hidden_dim),  # 連接第一層\n",
    "                nn.Linear(self.hidden_dim * task_id, self.hidden_dim)   # 連接第二層\n",
    "            ])\n",
    "        else:\n",
    "            lateral = nn.ModuleList([None, None])\n",
    "        self.lateral_connections.append(lateral)\n",
    "        \n",
    "        # 輸出頭\n",
    "        head = nn.Linear(self.hidden_dim, self.num_classes)\n",
    "        self.heads.append(head)\n",
    "        \n",
    "        self.num_tasks += 1\n",
    "        \n",
    "        # 凍結舊列\n",
    "        for i in range(task_id):\n",
    "            for param in self.columns[i].parameters():\n",
    "                param.requires_grad = False\n",
    "            if self.lateral_connections[i][0] is not None:\n",
    "                for param in self.lateral_connections[i].parameters():\n",
    "                    param.requires_grad = False\n",
    "            for param in self.heads[i].parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        return task_id\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, task_id: int) -> torch.Tensor:\n",
    "        \"\"\"前向傳播（指定任務）\"\"\"\n",
    "        if task_id >= self.num_tasks:\n",
    "            raise ValueError(f\"Task {task_id} not added yet\")\n",
    "            \n",
    "        # 計算所有列的隱藏狀態（到指定任務）\n",
    "        hidden_states = []  # [layer][column]\n",
    "        \n",
    "        for layer_idx in range(2):  # 2 層\n",
    "            layer_hiddens = []\n",
    "            \n",
    "            for col_idx in range(task_id + 1):\n",
    "                column = self.columns[col_idx]\n",
    "                \n",
    "                if layer_idx == 0:\n",
    "                    # 第一層：輸入是 x\n",
    "                    h = F.relu(column[layer_idx](x))\n",
    "                else:\n",
    "                    # 後續層：輸入是上一層的輸出\n",
    "                    h_prev = hidden_states[layer_idx - 1][col_idx]\n",
    "                    h = F.relu(column[layer_idx](h_prev))\n",
    "                    \n",
    "                # 添加橫向連接（如果存在）\n",
    "                if col_idx > 0 and self.lateral_connections[col_idx][layer_idx] is not None:\n",
    "                    # 收集所有舊列的隱藏狀態\n",
    "                    if layer_idx == 0:\n",
    "                        prev_hiddens = torch.cat([hidden_states[0][i] for i in range(col_idx)], dim=1)\n",
    "                    else:\n",
    "                        prev_hiddens = torch.cat([hidden_states[layer_idx-1][i] for i in range(col_idx)], dim=1)\n",
    "                    \n",
    "                    lateral_contribution = self.lateral_connections[col_idx][layer_idx](prev_hiddens)\n",
    "                    h = h + lateral_contribution\n",
    "                    \n",
    "                layer_hiddens.append(h)\n",
    "                \n",
    "            hidden_states.append(layer_hiddens)\n",
    "            \n",
    "        # 使用對應任務的輸出頭\n",
    "        final_hidden = hidden_states[-1][task_id]\n",
    "        output = self.heads[task_id](final_hidden)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# 測試\n",
    "pnn = ProgressiveNeuralNetwork(input_dim=64, hidden_dim=32, num_classes=2)\n",
    "\n",
    "# 添加任務並測試\n",
    "for i in range(3):\n",
    "    task_id = pnn.add_task()\n",
    "    x = torch.randn(16, 64)\n",
    "    out = pnn(x, task_id)\n",
    "    print(f\"Task {task_id}: 輸出形狀 {out.shape}\")\n",
    "    \n",
    "# 統計參數\n",
    "total_params = sum(p.numel() for p in pnn.parameters())\n",
    "print(f\"\\n總參數量: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: 評估指標\n",
    "\n",
    "持續學習的標準評估指標。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ContinualLearningMetrics:\n",
    "    \"\"\"持續學習評估指標\"\"\"\n",
    "    \n",
    "    # 準確率矩陣 R[i,j] = 訓練完任務 i 後在任務 j 上的準確率\n",
    "    accuracy_matrix: np.ndarray = field(default_factory=lambda: np.array([]))\n",
    "    \n",
    "    def compute_average_accuracy(self) -> float:\n",
    "        \"\"\"\n",
    "        平均準確率 (Average Accuracy)\n",
    "        訓練完所有任務後，在所有任務上的平均準確率\n",
    "        \"\"\"\n",
    "        if len(self.accuracy_matrix) == 0:\n",
    "            return 0.0\n",
    "        return self.accuracy_matrix[-1].mean()\n",
    "    \n",
    "    def compute_forgetting(self) -> float:\n",
    "        \"\"\"\n",
    "        遺忘度 (Forgetting)\n",
    "        F = (1/T-1) * Σ max_{l<T}(R[l,j]) - R[T,j]\n",
    "        \"\"\"\n",
    "        T = len(self.accuracy_matrix)\n",
    "        if T < 2:\n",
    "            return 0.0\n",
    "            \n",
    "        forgetting = 0.0\n",
    "        for j in range(T - 1):  # 除了最後一個任務\n",
    "            # 找到任務 j 的最高歷史準確率\n",
    "            max_acc = max(self.accuracy_matrix[l, j] for l in range(j, T))\n",
    "            # 當前準確率\n",
    "            curr_acc = self.accuracy_matrix[-1, j]\n",
    "            forgetting += max_acc - curr_acc\n",
    "            \n",
    "        return forgetting / (T - 1)\n",
    "    \n",
    "    def compute_forward_transfer(self) -> float:\n",
    "        \"\"\"\n",
    "        前向遷移 (Forward Transfer)\n",
    "        FWT = (1/T-1) * Σ R[i-1,i] - b_i\n",
    "        其中 b_i 是隨機初始化模型在任務 i 上的準確率\n",
    "        （簡化版：假設 b_i = 1/num_classes）\n",
    "        \"\"\"\n",
    "        T = len(self.accuracy_matrix)\n",
    "        if T < 2:\n",
    "            return 0.0\n",
    "            \n",
    "        # 假設隨機基線\n",
    "        random_baseline = 0.5  # 二分類\n",
    "        \n",
    "        fwt = 0.0\n",
    "        for i in range(1, T):\n",
    "            # 訓練任務 i 之前在任務 i 上的準確率\n",
    "            acc_before = self.accuracy_matrix[i-1, i] if i < len(self.accuracy_matrix[i-1]) else random_baseline\n",
    "            fwt += acc_before - random_baseline\n",
    "            \n",
    "        return fwt / (T - 1)\n",
    "    \n",
    "    def compute_backward_transfer(self) -> float:\n",
    "        \"\"\"\n",
    "        後向遷移 (Backward Transfer)\n",
    "        BWT = (1/T-1) * Σ R[T,j] - R[j,j]\n",
    "        \"\"\"\n",
    "        T = len(self.accuracy_matrix)\n",
    "        if T < 2:\n",
    "            return 0.0\n",
    "            \n",
    "        bwt = 0.0\n",
    "        for j in range(T - 1):\n",
    "            # 最終準確率 - 剛訓練完的準確率\n",
    "            bwt += self.accuracy_matrix[-1, j] - self.accuracy_matrix[j, j]\n",
    "            \n",
    "        return bwt / (T - 1)\n",
    "    \n",
    "    def summary(self) -> Dict[str, float]:\n",
    "        \"\"\"返回所有指標的摘要\"\"\"\n",
    "        return {\n",
    "            'Average Accuracy': self.compute_average_accuracy(),\n",
    "            'Forgetting': self.compute_forgetting(),\n",
    "            'Forward Transfer': self.compute_forward_transfer(),\n",
    "            'Backward Transfer': self.compute_backward_transfer()\n",
    "        }\n",
    "\n",
    "\n",
    "def evaluate_continual_learner(\n",
    "    model: nn.Module,\n",
    "    tasks: List[Task],\n",
    "    train_fn: Callable,  # 訓練函數\n",
    "    eval_fn: Callable    # 評估函數\n",
    ") -> ContinualLearningMetrics:\n",
    "    \"\"\"\n",
    "    評估持續學習者\n",
    "    \n",
    "    Args:\n",
    "        model: 模型\n",
    "        tasks: 任務列表\n",
    "        train_fn: 訓練函數 train_fn(model, task)\n",
    "        eval_fn: 評估函數 eval_fn(model, task) -> accuracy\n",
    "    \"\"\"\n",
    "    num_tasks = len(tasks)\n",
    "    accuracy_matrix = np.zeros((num_tasks, num_tasks))\n",
    "    \n",
    "    for i, task in enumerate(tasks):\n",
    "        print(f\"\\n訓練 Task {i}: {task.name}\")\n",
    "        train_fn(model, task)\n",
    "        \n",
    "        # 評估所有已見過的任務\n",
    "        for j in range(i + 1):\n",
    "            acc = eval_fn(model, tasks[j])\n",
    "            accuracy_matrix[i, j] = acc\n",
    "            print(f\"  Task {j} 準確率: {acc:.2%}\")\n",
    "            \n",
    "    metrics = ContinualLearningMetrics(accuracy_matrix=accuracy_matrix)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# 範例準確率矩陣（模擬）\n",
    "example_matrix = np.array([\n",
    "    [0.90, 0.00, 0.00, 0.00, 0.00],  # 訓練完 Task 0\n",
    "    [0.70, 0.85, 0.00, 0.00, 0.00],  # 訓練完 Task 1\n",
    "    [0.60, 0.75, 0.88, 0.00, 0.00],  # 訓練完 Task 2\n",
    "    [0.55, 0.70, 0.80, 0.92, 0.00],  # 訓練完 Task 3\n",
    "    [0.50, 0.65, 0.75, 0.85, 0.90],  # 訓練完 Task 4\n",
    "])\n",
    "\n",
    "metrics = ContinualLearningMetrics(accuracy_matrix=example_matrix)\n",
    "print(\"範例評估指標:\")\n",
    "for name, value in metrics.summary().items():\n",
    "    print(f\"  {name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_accuracy_matrix(accuracy_matrix: np.ndarray):\n",
    "    \"\"\"視覺化準確率矩陣\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # 只顯示非零部分\n",
    "    mask = accuracy_matrix == 0\n",
    "    masked_matrix = np.ma.array(accuracy_matrix, mask=mask)\n",
    "    \n",
    "    im = ax.imshow(masked_matrix, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "    \n",
    "    # 添加數值標籤\n",
    "    for i in range(len(accuracy_matrix)):\n",
    "        for j in range(len(accuracy_matrix[i])):\n",
    "            if accuracy_matrix[i, j] > 0:\n",
    "                ax.text(j, i, f'{accuracy_matrix[i, j]:.2f}',\n",
    "                       ha='center', va='center', fontsize=10)\n",
    "                \n",
    "    ax.set_xlabel('任務 j (評估)')\n",
    "    ax.set_ylabel('任務 i (訓練完成後)')\n",
    "    ax.set_title('準確率矩陣 R[i,j]')\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, label='準確率')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_accuracy_matrix(example_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: 方法比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_method_comparison():\n",
    "    \"\"\"印出持續學習方法比較\"\"\"\n",
    "    print(\"\"\"\n",
    "╔═══════════════════════════════════════════════════════════════════════════════════╗\n",
    "║                          持續學習方法比較                                          ║\n",
    "╠════════════════════╦══════════════════════════════════════════════════════════════╣\n",
    "║       類別         ║                      方法與特點                               ║\n",
    "╠════════════════════╬══════════════════════════════════════════════════════════════╣\n",
    "║                    ║ EWC: 用 Fisher Information 保護重要參數                      ║\n",
    "║  Regularization    ║ SI: 追蹤參數對損失的貢獻度                                   ║\n",
    "║  -based            ║ LwF: 知識蒸餾保留舊知識                                      ║\n",
    "║                    ║ 優點: 不需存儲，模型大小固定                                 ║\n",
    "║                    ║ 缺點: 長序列任務效果下降                                     ║\n",
    "╠════════════════════╬══════════════════════════════════════════════════════════════╣\n",
    "║                    ║ Experience Replay: 存儲並重播舊樣本                          ║\n",
    "║  Replay            ║ Generative Replay: 用生成器生成舊樣本                        ║\n",
    "║  -based            ║ GEM: 約束梯度不損害舊任務                                    ║\n",
    "║                    ║ 優點: 效果好，簡單                                           ║\n",
    "║                    ║ 缺點: 需要存儲空間或生成器                                   ║\n",
    "╠════════════════════╬══════════════════════════════════════════════════════════════╣\n",
    "║                    ║ PackNet: 剪枝+凍結                                           ║\n",
    "║  Architecture      ║ PNN: 為每個任務添加網路列                                    ║\n",
    "║  -based            ║ DEN: 動態擴展網路                                            ║\n",
    "║                    ║ 優點: 無遺忘                                                 ║\n",
    "║                    ║ 缺點: 模型增長或容量受限                                     ║\n",
    "╠════════════════════╬══════════════════════════════════════════════════════════════╣\n",
    "║                    ║ HAL: 結合 Replay + EWC                                       ║\n",
    "║  Hybrid            ║ ER-EWC: Replay + 正則化                                      ║\n",
    "║                    ║ 通常效果最好，但也最複雜                                     ║\n",
    "╚════════════════════╩══════════════════════════════════════════════════════════════╝\n",
    "\n",
    "選擇指南：\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│ 場景                                │ 推薦方法                     │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│ 記憶體受限                          │ EWC / SI                     │\n",
    "│ 可存儲少量樣本                      │ Experience Replay            │\n",
    "│ 任務數量已知且固定                  │ PackNet                      │\n",
    "│ 需要零遺忘                          │ PNN                          │\n",
    "│ 任務相似度高                        │ Generative Replay            │\n",
    "│ 追求最佳效果                        │ Hybrid (ER + EWC)            │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "評估指標解讀：\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│ Average Accuracy ↑  │ 越高越好，整體性能                          │\n",
    "│ Forgetting ↓        │ 越低越好，遺忘程度                          │\n",
    "│ Forward Transfer ↑  │ > 0 表示有正向遷移                          │\n",
    "│ Backward Transfer ↑ │ > 0 表示學新知識提升了舊任務（罕見）        │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "\"\"\")\n",
    "\n",
    "print_method_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習\n",
    "\n",
    "### Exercise 1: 實作 A-GEM (Averaged Gradient Episodic Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGEM:\n",
    "    \"\"\"Averaged Gradient Episodic Memory\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, buffer_size: int = 256):\n",
    "        self.model = model\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "    def project_gradient(\n",
    "        self,\n",
    "        grad: torch.Tensor,\n",
    "        ref_grad: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        投影梯度以不損害舊任務\n",
    "        \n",
    "        如果 grad · ref_grad < 0，投影 grad 到 ref_grad 的正交補空間\n",
    "        \n",
    "        TODO: 實作梯度投影\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def train_step(self, x: torch.Tensor, y: torch.Tensor, task_id: int):\n",
    "        \"\"\"\n",
    "        單步訓練\n",
    "        \n",
    "        TODO: \n",
    "        1. 計算當前 batch 的梯度\n",
    "        2. 計算 buffer 樣本的參考梯度\n",
    "        3. 如果違反約束，投影梯度\n",
    "        4. 更新參數\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: 比較不同方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_continual_learning_methods(tasks: List[Task]):\n",
    "    \"\"\"\n",
    "    比較不同持續學習方法的效果\n",
    "    \n",
    "    TODO:\n",
    "    1. 實現基線（Fine-tuning）\n",
    "    2. 實現 Experience Replay\n",
    "    3. 實現 EWC（參見 catastrophic_forgetting.ipynb）\n",
    "    4. 比較各方法的評估指標\n",
    "    5. 繪製準確率矩陣比較圖\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: 實作 Memory-Aware Synapses (MAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAS:\n",
    "    \"\"\"Memory Aware Synapses\"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, lambda_: float = 1.0):\n",
    "        \"\"\"\n",
    "        MAS 使用參數對輸出的敏感度作為重要性度量\n",
    "        \n",
    "        Ω_i = E[ ||∂L/∂θ_i||² ]  （這裡 L 是輸出的 L2 範數）\n",
    "        \n",
    "        TODO: 實作 MAS\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def estimate_importance(self, data_loader):\n",
    "        \"\"\"估計參數重要性\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def compute_loss(self, task_loss: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"計算總損失 = 任務損失 + λ * 正則化損失\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "```\n",
    "持續學習重點回顧：\n",
    "\n",
    "1. 問題設定\n",
    "   ├─ 任務序列學習\n",
    "   ├─ 舊任務數據不可用\n",
    "   └─ 需要平衡新舊知識\n",
    "\n",
    "2. Replay-based\n",
    "   ├─ Experience Replay: 存儲真實樣本\n",
    "   ├─ Generative Replay: 生成偽樣本\n",
    "   └─ GEM/A-GEM: 梯度約束\n",
    "\n",
    "3. Architecture-based\n",
    "   ├─ PackNet: 剪枝+凍結\n",
    "   ├─ PNN: 網路擴展\n",
    "   └─ DEN: 動態擴展\n",
    "\n",
    "4. 評估指標\n",
    "   ├─ Average Accuracy: 整體性能\n",
    "   ├─ Forgetting: 遺忘程度\n",
    "   ├─ Forward Transfer: 正向遷移\n",
    "   └─ Backward Transfer: 後向遷移\n",
    "\n",
    "實際應用建議：\n",
    "- 優先嘗試 Experience Replay（簡單有效）\n",
    "- 記憶體受限時用 EWC/SI\n",
    "- 追求零遺忘用 PNN\n",
    "- 最佳效果通常是 Hybrid 方法\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
