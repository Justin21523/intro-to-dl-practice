{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 災難性遺忘與持續學習 (Catastrophic Forgetting & Continual Learning)\n",
    "\n",
    "**對應課程**: 李宏毅 2025 Spring ML HW6, 2025 Fall GenAI-ML HW8\n",
    "\n",
    "本 notebook 探討神經網路在連續學習多個任務時的災難性遺忘問題，以及相應的解決方案。\n",
    "\n",
    "## 學習目標\n",
    "1. 理解災難性遺忘的原因與現象\n",
    "2. 學會量化測量遺忘程度\n",
    "3. 實作 EWC (Elastic Weight Consolidation)\n",
    "4. 了解 LoRA 與遺忘的關係\n",
    "5. 掌握持續學習策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: 什麼是災難性遺忘？\n",
    "\n",
    "### 1.1 問題定義\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    災難性遺忘 (Catastrophic Forgetting)          │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  定義: 當神經網路學習新任務時，會快速且嚴重地忘記先前學習的任務  │\n",
    "│                                                                 │\n",
    "│  時間軸:                                                        │\n",
    "│  ┌──────────┐     ┌──────────┐     ┌──────────┐                │\n",
    "│  │ Task A   │ →   │ Task B   │ →   │ Task C   │                │\n",
    "│  │ (學習)   │     │ (學習)   │     │ (學習)   │                │\n",
    "│  └──────────┘     └──────────┘     └──────────┘                │\n",
    "│       ↓                 ↓                 ↓                     │\n",
    "│  A: 100%           A: 30%↓          A: 10%↓↓                   │\n",
    "│                    B: 100%          B: 40%↓                     │\n",
    "│                                     C: 100%                     │\n",
    "│                                                                 │\n",
    "│  問題: 學完 Task C 後，Task A 和 B 的效能嚴重下降                │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 1.2 為什麼會發生？\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    遺忘的根本原因                                 │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  1. 權重共享 (Shared Parameters)                                │\n",
    "│     • 神經網路使用相同的參數處理所有任務                         │\n",
    "│     • 新任務的梯度更新會覆蓋舊任務學到的特徵                     │\n",
    "│                                                                 │\n",
    "│  2. 參數空間干擾 (Interference)                                 │\n",
    "│                                                                 │\n",
    "│        參數空間                                                 │\n",
    "│           ▲                                                     │\n",
    "│           │      ★ Task A 最優解                                │\n",
    "│           │    ╱                                                │\n",
    "│           │   ╱  ← 梯度更新路徑                                  │\n",
    "│           │  ╱                                                  │\n",
    "│           │ ●────→ ◆ Task B 最優解                              │\n",
    "│           │     遠離 Task A 最優解                               │\n",
    "│           └──────────────────→                                 │\n",
    "│                                                                 │\n",
    "│  3. 分布偏移 (Distribution Shift)                               │\n",
    "│     • 不同任務的資料分布不同                                    │\n",
    "│     • 模型適應新分布，忘記舊分布                                │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境設置\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用設備: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立簡單的分類任務來示範災難性遺忘\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    \"\"\"簡單的 MLP 用於示範\"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dim=256, output_dim=10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "def create_permuted_mnist_tasks(num_tasks=3):\n",
    "    \"\"\"\n",
    "    創建 Permuted MNIST 任務\n",
    "    每個任務使用不同的像素排列\n",
    "    \"\"\"\n",
    "    from torchvision import datasets, transforms\n",
    "    \n",
    "    # 載入 MNIST\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "        test_data = datasets.MNIST('./data', train=False, transform=transform)\n",
    "    except:\n",
    "        print(\"無法下載 MNIST，使用合成資料\")\n",
    "        # 合成資料\n",
    "        X_train = torch.randn(10000, 1, 28, 28)\n",
    "        y_train = torch.randint(0, 10, (10000,))\n",
    "        X_test = torch.randn(2000, 1, 28, 28)\n",
    "        y_test = torch.randint(0, 10, (2000,))\n",
    "        train_data = TensorDataset(X_train, y_train)\n",
    "        test_data = TensorDataset(X_test, y_test)\n",
    "    \n",
    "    tasks = []\n",
    "    \n",
    "    for task_id in range(num_tasks):\n",
    "        if task_id == 0:\n",
    "            # 第一個任務使用原始排列\n",
    "            perm = torch.arange(784)\n",
    "        else:\n",
    "            # 其他任務使用隨機排列\n",
    "            perm = torch.randperm(784)\n",
    "        \n",
    "        tasks.append({\n",
    "            'name': f'Task {task_id}',\n",
    "            'permutation': perm,\n",
    "            'train_data': train_data,\n",
    "            'test_data': test_data\n",
    "        })\n",
    "    \n",
    "    return tasks\n",
    "\n",
    "def apply_permutation(x, perm):\n",
    "    \"\"\"應用像素排列\"\"\"\n",
    "    batch_size = x.size(0)\n",
    "    x_flat = x.view(batch_size, -1)\n",
    "    x_perm = x_flat[:, perm]\n",
    "    return x_perm.view(batch_size, 1, 28, 28)\n",
    "\n",
    "print(\"任務創建函數已定義\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: 觀察災難性遺忘現象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練與評估函數\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, perm, device):\n",
    "    \"\"\"訓練一個 epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x = apply_permutation(x, perm.to(device))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, perm, device):\n",
    "    \"\"\"評估準確率\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            x = apply_permutation(x, perm.to(device))\n",
    "            \n",
    "            output = model(x)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "def sequential_training(model, tasks, epochs_per_task=3, batch_size=128, lr=0.001):\n",
    "    \"\"\"\n",
    "    Sequential training: 依序訓練每個任務（會導致遺忘）\n",
    "    \"\"\"\n",
    "    history = {task['name']: [] for task in tasks}\n",
    "    \n",
    "    for task_idx, task in enumerate(tasks):\n",
    "        print(f\"\\n訓練 {task['name']}...\")\n",
    "        \n",
    "        # 建立 DataLoader（使用子集以加快示範）\n",
    "        subset_indices = list(range(min(5000, len(task['train_data']))))\n",
    "        train_subset = Subset(task['train_data'], subset_indices)\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        test_subset_indices = list(range(min(1000, len(task['test_data']))))\n",
    "        test_subsets = [Subset(t['test_data'], test_subset_indices) for t in tasks]\n",
    "        test_loaders = [DataLoader(ts, batch_size=batch_size) for ts in test_subsets]\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(epochs_per_task):\n",
    "            train_epoch(model, train_loader, optimizer, task['permutation'], device)\n",
    "            \n",
    "            # 評估所有任務\n",
    "            for t_idx, t in enumerate(tasks[:task_idx+1]):\n",
    "                acc = evaluate(model, test_loaders[t_idx], t['permutation'], device)\n",
    "                history[t['name']].append(acc)\n",
    "        \n",
    "        # 打印當前各任務準確率\n",
    "        print(f\"  各任務準確率:\")\n",
    "        for t_idx, t in enumerate(tasks[:task_idx+1]):\n",
    "            print(f\"    {t['name']}: {history[t['name']][-1]:.2%}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"訓練函數已定義\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示範災難性遺忘\n",
    "print(\"=== 示範災難性遺忘 ===\")\n",
    "\n",
    "# 創建任務\n",
    "tasks = create_permuted_mnist_tasks(num_tasks=3)\n",
    "\n",
    "# 建立模型\n",
    "model = SimpleMLP().to(device)\n",
    "\n",
    "# Sequential training（會導致遺忘）\n",
    "history = sequential_training(model, tasks, epochs_per_task=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化遺忘現象\n",
    "def visualize_forgetting(history, title=\"Catastrophic Forgetting\"):\n",
    "    \"\"\"視覺化各任務準確率變化\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    colors = ['steelblue', 'coral', 'seagreen', 'purple', 'orange']\n",
    "    \n",
    "    for i, (task_name, accs) in enumerate(history.items()):\n",
    "        if accs:  # 確保有資料\n",
    "            plt.plot(accs, 'o-', label=task_name, color=colors[i % len(colors)], markersize=4)\n",
    "    \n",
    "    # 標註任務切換點\n",
    "    task_boundaries = []\n",
    "    cumulative = 0\n",
    "    for i, (_, accs) in enumerate(history.items()):\n",
    "        if i > 0 and accs:\n",
    "            task_boundaries.append(cumulative)\n",
    "        cumulative = len(list(history.values())[0])  # 取第一個任務的長度作為參考\n",
    "    \n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.ylim(0, 1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_forgetting(history, \"Sequential Training: Catastrophic Forgetting\")\n",
    "\n",
    "print(\"\\n觀察:\")\n",
    "print(\"- 學習新任務時，舊任務的準確率急劇下降\")\n",
    "print(\"- 這就是災難性遺忘現象\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: 遺忘的量化測量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forgetting_metrics(history: Dict[str, List[float]]) -> Dict:\n",
    "    \"\"\"\n",
    "    計算遺忘相關指標\n",
    "    \n",
    "    1. 平均遺忘 (Average Forgetting)\n",
    "    2. 最大遺忘 (Maximum Forgetting)\n",
    "    3. 後向轉移 (Backward Transfer)\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    forgetting_scores = []\n",
    "    \n",
    "    for task_name, accs in history.items():\n",
    "        if len(accs) > 1:\n",
    "            max_acc = max(accs)  # 該任務曾達到的最高準確率\n",
    "            final_acc = accs[-1]  # 最終準確率\n",
    "            forgetting = max_acc - final_acc\n",
    "            forgetting_scores.append(forgetting)\n",
    "    \n",
    "    if forgetting_scores:\n",
    "        metrics['average_forgetting'] = np.mean(forgetting_scores)\n",
    "        metrics['max_forgetting'] = max(forgetting_scores)\n",
    "        metrics['forgetting_per_task'] = {\n",
    "            name: max(accs) - accs[-1] if len(accs) > 1 else 0\n",
    "            for name, accs in history.items()\n",
    "        }\n",
    "    \n",
    "    # 計算平均準確率\n",
    "    final_accs = [accs[-1] for accs in history.values() if accs]\n",
    "    metrics['average_accuracy'] = np.mean(final_accs) if final_accs else 0\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# 計算並打印指標\n",
    "metrics = compute_forgetting_metrics(history)\n",
    "\n",
    "print(\"=== 遺忘指標 ===\")\n",
    "print(f\"平均遺忘: {metrics.get('average_forgetting', 0):.2%}\")\n",
    "print(f\"最大遺忘: {metrics.get('max_forgetting', 0):.2%}\")\n",
    "print(f\"最終平均準確率: {metrics.get('average_accuracy', 0):.2%}\")\n",
    "print(\"\\n各任務遺忘:\")\n",
    "for task, forgetting in metrics.get('forgetting_per_task', {}).items():\n",
    "    print(f\"  {task}: {forgetting:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: EWC (Elastic Weight Consolidation)\n",
    "\n",
    "### 4.1 EWC 原理\n",
    "\n",
    "EWC 的核心想法：保護對舊任務重要的參數。\n",
    "\n",
    "$$L_{EWC} = L_{new}(\\theta) + \\frac{\\lambda}{2} \\sum_i F_i (\\theta_i - \\theta^*_i)^2$$\n",
    "\n",
    "其中：\n",
    "- $L_{new}$: 新任務的損失\n",
    "- $F_i$: Fisher 資訊矩陣（衡量參數重要性）\n",
    "- $\\theta^*$: 舊任務訓練後的參數\n",
    "- $\\lambda$: 正則化強度\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                         EWC 示意圖                               │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│        參數空間                                                 │\n",
    "│           ▲                                                     │\n",
    "│           │      ★ Task A 最優解                                │\n",
    "│           │    ╱ ╲                                              │\n",
    "│           │   ╱   ╲  受 EWC 約束的更新路徑                      │\n",
    "│           │  ╱     ╲                                            │\n",
    "│           │ ●───────◆ Task B 最優解（在 A 附近）                │\n",
    "│           │                                                     │\n",
    "│           │ EWC 限制參數偏離 Task A 最優解的程度                │\n",
    "│           └──────────────────────────────→                     │\n",
    "│                                                                 │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EWC:\n",
    "    \"\"\"\n",
    "    Elastic Weight Consolidation (EWC)\n",
    "    \n",
    "    參考: \"Overcoming catastrophic forgetting in neural networks\" (Kirkpatrick et al., 2017)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, dataloader, perm, device, \n",
    "                 num_samples: int = 200):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: 已訓練好的模型\n",
    "            dataloader: 用於計算 Fisher 資訊的資料\n",
    "            perm: 該任務的像素排列\n",
    "            device: 計算設備\n",
    "            num_samples: 用於估計 Fisher 的樣本數\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        \n",
    "        # 儲存參數名稱和值\n",
    "        self.params = {n: p.clone().detach() for n, p in model.named_parameters() if p.requires_grad}\n",
    "        \n",
    "        # 計算 Fisher 資訊矩陣\n",
    "        self.fisher = self._compute_fisher(dataloader, perm, num_samples)\n",
    "    \n",
    "    def _compute_fisher(self, dataloader, perm, num_samples: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        計算 Fisher 資訊矩陣（對角近似）\n",
    "        \n",
    "        Fisher 資訊衡量參數對於輸出的重要性\n",
    "        \"\"\"\n",
    "        fisher = {n: torch.zeros_like(p) for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "        \n",
    "        self.model.eval()\n",
    "        count = 0\n",
    "        \n",
    "        for x, y in dataloader:\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "            \n",
    "            x = x.to(self.device)\n",
    "            x = apply_permutation(x, perm.to(self.device))\n",
    "            \n",
    "            self.model.zero_grad()\n",
    "            output = self.model(x)\n",
    "            \n",
    "            # 使用 log-likelihood 的梯度\n",
    "            # Fisher = E[grad log p(y|x) @ grad log p(y|x).T]\n",
    "            log_probs = F.log_softmax(output, dim=1)\n",
    "            \n",
    "            # 對每個樣本計算\n",
    "            for i in range(x.size(0)):\n",
    "                if count >= num_samples:\n",
    "                    break\n",
    "                    \n",
    "                # 取預測類別的 log prob\n",
    "                pred = output[i].argmax()\n",
    "                loss = -log_probs[i, pred]\n",
    "                \n",
    "                self.model.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "                \n",
    "                # 累加梯度平方\n",
    "                for n, p in self.model.named_parameters():\n",
    "                    if p.requires_grad and p.grad is not None:\n",
    "                        fisher[n] += p.grad.data.pow(2)\n",
    "                \n",
    "                count += 1\n",
    "        \n",
    "        # 取平均\n",
    "        for n in fisher:\n",
    "            fisher[n] /= num_samples\n",
    "        \n",
    "        return fisher\n",
    "    \n",
    "    def penalty(self, model: nn.Module) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        計算 EWC penalty\n",
    "        \n",
    "        penalty = sum_i F_i * (theta_i - theta*_i)^2\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        for n, p in model.named_parameters():\n",
    "            if n in self.fisher:\n",
    "                loss += (self.fisher[n] * (p - self.params[n]).pow(2)).sum()\n",
    "        return loss\n",
    "\n",
    "print(\"EWC 類別已定義\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_ewc(model, tasks, epochs_per_task=3, batch_size=128, \n",
    "                   lr=0.001, ewc_lambda=5000):\n",
    "    \"\"\"\n",
    "    使用 EWC 進行連續學習\n",
    "    \"\"\"\n",
    "    history = {task['name']: [] for task in tasks}\n",
    "    ewc_tasks = []  # 儲存每個任務的 EWC 物件\n",
    "    \n",
    "    for task_idx, task in enumerate(tasks):\n",
    "        print(f\"\\n訓練 {task['name']} (with EWC)...\")\n",
    "        \n",
    "        # 建立 DataLoader\n",
    "        subset_indices = list(range(min(5000, len(task['train_data']))))\n",
    "        train_subset = Subset(task['train_data'], subset_indices)\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        test_subset_indices = list(range(min(1000, len(task['test_data']))))\n",
    "        test_subsets = [Subset(t['test_data'], test_subset_indices) for t in tasks]\n",
    "        test_loaders = [DataLoader(ts, batch_size=batch_size) for ts in test_subsets]\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(epochs_per_task):\n",
    "            model.train()\n",
    "            \n",
    "            for x, y in train_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                x = apply_permutation(x, task['permutation'].to(device))\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                output = model(x)\n",
    "                \n",
    "                # 當前任務的損失\n",
    "                loss = F.cross_entropy(output, y)\n",
    "                \n",
    "                # 加上 EWC penalty（對所有之前的任務）\n",
    "                for ewc in ewc_tasks:\n",
    "                    loss += (ewc_lambda / 2) * ewc.penalty(model)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # 評估所有任務\n",
    "            for t_idx, t in enumerate(tasks[:task_idx+1]):\n",
    "                acc = evaluate(model, test_loaders[t_idx], t['permutation'], device)\n",
    "                history[t['name']].append(acc)\n",
    "        \n",
    "        # 訓練完當前任務後，計算並儲存 EWC\n",
    "        ewc = EWC(model, train_loader, task['permutation'], device)\n",
    "        ewc_tasks.append(ewc)\n",
    "        \n",
    "        # 打印當前各任務準確率\n",
    "        print(f\"  各任務準確率:\")\n",
    "        for t_idx, t in enumerate(tasks[:task_idx+1]):\n",
    "            print(f\"    {t['name']}: {history[t['name']][-1]:.2%}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# 使用 EWC 訓練\n",
    "print(\"=== 使用 EWC 減少遺忘 ===\")\n",
    "model_ewc = SimpleMLP().to(device)\n",
    "history_ewc = train_with_ewc(model_ewc, tasks, epochs_per_task=3, ewc_lambda=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較有無 EWC 的效果\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 無 EWC（之前的結果）\n",
    "colors = ['steelblue', 'coral', 'seagreen']\n",
    "for i, (task_name, accs) in enumerate(history.items()):\n",
    "    if accs:\n",
    "        ax1.plot(accs, 'o-', label=task_name, color=colors[i], markersize=4)\n",
    "ax1.set_xlabel('Training Steps')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Without EWC (Catastrophic Forgetting)')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "ax1.set_ylim(0, 1.05)\n",
    "\n",
    "# 有 EWC\n",
    "for i, (task_name, accs) in enumerate(history_ewc.items()):\n",
    "    if accs:\n",
    "        ax2.plot(accs, 'o-', label=task_name, color=colors[i], markersize=4)\n",
    "ax2.set_xlabel('Training Steps')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('With EWC (Reduced Forgetting)')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "ax2.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 比較指標\n",
    "metrics_no_ewc = compute_forgetting_metrics(history)\n",
    "metrics_ewc = compute_forgetting_metrics(history_ewc)\n",
    "\n",
    "print(\"\\n=== 指標比較 ===\")\n",
    "print(f\"平均遺忘: {metrics_no_ewc.get('average_forgetting', 0):.2%} → {metrics_ewc.get('average_forgetting', 0):.2%}\")\n",
    "print(f\"最終平均準確率: {metrics_no_ewc.get('average_accuracy', 0):.2%} → {metrics_ewc.get('average_accuracy', 0):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: LoRA 與遺忘的關係\n",
    "\n",
    "LoRA 微調有時可以減少遺忘，因為它只更新少量參數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 層（簡化版）\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, original_linear: nn.Linear, rank: int = 4, alpha: float = 8.0):\n",
    "        super().__init__()\n",
    "        self.original = original_linear\n",
    "        \n",
    "        # 凍結原始權重\n",
    "        for param in self.original.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # LoRA 參數\n",
    "        in_features = original_linear.in_features\n",
    "        out_features = original_linear.out_features\n",
    "        \n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=False)\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=False)\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight)\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.original(x) + self.lora_B(self.lora_A(x)) * self.scaling\n",
    "\n",
    "class SimpleMLP_LoRA(nn.Module):\n",
    "    \"\"\"帶 LoRA 的 MLP\"\"\"\n",
    "    def __init__(self, base_model: SimpleMLP, lora_rank: int = 4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 複製結構並添加 LoRA\n",
    "        self.fc1 = LoRALinear(base_model.fc1, rank=lora_rank)\n",
    "        self.fc2 = LoRALinear(base_model.fc2, rank=lora_rank)\n",
    "        self.fc3 = LoRALinear(base_model.fc3, rank=lora_rank)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "print(\"LoRA MLP 已定義\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較 LoRA 與 Full fine-tuning 的遺忘\n",
    "print(\"=== LoRA vs Full Fine-tuning 遺忘比較 ===\")\n",
    "\n",
    "# 首先訓練一個基礎模型\n",
    "base_model = SimpleMLP().to(device)\n",
    "task0 = tasks[0]\n",
    "\n",
    "# 在 Task 0 上訓練\n",
    "subset_indices = list(range(min(5000, len(task0['train_data']))))\n",
    "train_subset = Subset(task0['train_data'], subset_indices)\n",
    "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(base_model.parameters(), lr=0.001)\n",
    "for epoch in range(5):\n",
    "    train_epoch(base_model, train_loader, optimizer, task0['permutation'], device)\n",
    "\n",
    "# 評估初始效能\n",
    "test_subset_indices = list(range(min(1000, len(task0['test_data']))))\n",
    "test_subset = Subset(task0['test_data'], test_subset_indices)\n",
    "test_loader = DataLoader(test_subset, batch_size=128)\n",
    "\n",
    "initial_acc = evaluate(base_model, test_loader, task0['permutation'], device)\n",
    "print(f\"Task 0 初始準確率: {initial_acc:.2%}\")\n",
    "\n",
    "# 方法 1: Full fine-tuning on Task 1\n",
    "model_full = deepcopy(base_model)\n",
    "task1 = tasks[1]\n",
    "train_loader1 = DataLoader(Subset(task1['train_data'], subset_indices), batch_size=128, shuffle=True)\n",
    "optimizer = torch.optim.Adam(model_full.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_epoch(model_full, train_loader1, optimizer, task1['permutation'], device)\n",
    "\n",
    "# 方法 2: LoRA fine-tuning on Task 1\n",
    "model_lora = SimpleMLP_LoRA(deepcopy(base_model), lora_rank=8).to(device)\n",
    "# 只優化 LoRA 參數\n",
    "lora_params = [p for n, p in model_lora.named_parameters() if 'lora' in n]\n",
    "optimizer = torch.optim.Adam(lora_params, lr=0.001)\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_epoch(model_lora, train_loader1, optimizer, task1['permutation'], device)\n",
    "\n",
    "# 比較 Task 0 的遺忘\n",
    "acc_full_task0 = evaluate(model_full, test_loader, task0['permutation'], device)\n",
    "acc_lora_task0 = evaluate(model_lora, test_loader, task0['permutation'], device)\n",
    "\n",
    "# Task 1 準確率\n",
    "test_loader1 = DataLoader(Subset(task1['test_data'], test_subset_indices), batch_size=128)\n",
    "acc_full_task1 = evaluate(model_full, test_loader1, task1['permutation'], device)\n",
    "acc_lora_task1 = evaluate(model_lora, test_loader1, task1['permutation'], device)\n",
    "\n",
    "print(f\"\\n=== 結果比較 ===\")\n",
    "print(f\"方法              | Task 0 準確率 | Task 1 準確率 | Task 0 遺忘\")\n",
    "print(f\"-\" * 60)\n",
    "print(f\"Full Fine-tuning  | {acc_full_task0:.2%}        | {acc_full_task1:.2%}        | {initial_acc - acc_full_task0:.2%}\")\n",
    "print(f\"LoRA Fine-tuning  | {acc_lora_task0:.2%}        | {acc_lora_task1:.2%}        | {initial_acc - acc_lora_task0:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: 其他持續學習方法\n",
    "\n",
    "### 6.1 方法概覽\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│                    持續學習方法分類                             │\n",
    "├────────────────────────────────────────────────────────────────┤\n",
    "│                                                                │\n",
    "│  1. 正則化方法 (Regularization-based)                          │\n",
    "│     • EWC: 保護重要參數                                        │\n",
    "│     • SI (Synaptic Intelligence): 累積參數重要性               │\n",
    "│     • MAS (Memory Aware Synapses): 使用無標籤資料計算重要性    │\n",
    "│                                                                │\n",
    "│  2. 重放方法 (Replay-based)                                    │\n",
    "│     • Experience Replay: 儲存舊任務樣本                        │\n",
    "│     • Generative Replay: 用生成模型產生舊任務資料              │\n",
    "│     • Pseudo-rehearsal: 用隨機輸入記錄模型行為                  │\n",
    "│                                                                │\n",
    "│  3. 架構方法 (Architecture-based)                              │\n",
    "│     • Progressive Networks: 每個任務新增網路                   │\n",
    "│     • PackNet: 剪枝後的空間給新任務                            │\n",
    "│     • Adapter/LoRA: 每個任務一組小參數                         │\n",
    "│                                                                │\n",
    "│  比較:                                                         │\n",
    "│  ┌────────────────┬──────────────┬─────────────┬────────────┐ │\n",
    "│  │ 方法            │ 記憶體需求    │ 新任務效能   │ 舊任務保留 │ │\n",
    "│  ├────────────────┼──────────────┼─────────────┼────────────┤ │\n",
    "│  │ EWC            │ 低           │ 中          │ 中高       │ │\n",
    "│  │ Replay         │ 高（存資料）  │ 高          │ 高         │ │\n",
    "│  │ Progressive    │ 高（存模型）  │ 高          │ 最高       │ │\n",
    "│  │ LoRA per task  │ 中           │ 中高        │ 高         │ │\n",
    "│  └────────────────┴──────────────┴─────────────┴────────────┘ │\n",
    "│                                                                │\n",
    "└────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience Replay 簡單實作\n",
    "class ExperienceReplay:\n",
    "    \"\"\"經驗重放：儲存舊任務的樣本\"\"\"\n",
    "    \n",
    "    def __init__(self, buffer_size: int = 1000):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer_x = []\n",
    "        self.buffer_y = []\n",
    "        self.task_perms = []  # 儲存每個樣本對應的 permutation\n",
    "    \n",
    "    def add_task_samples(self, dataloader, perm, num_samples: int = 200):\n",
    "        \"\"\"從任務中添加樣本到 buffer\"\"\"\n",
    "        count = 0\n",
    "        for x, y in dataloader:\n",
    "            for i in range(x.size(0)):\n",
    "                if count >= num_samples:\n",
    "                    return\n",
    "                \n",
    "                self.buffer_x.append(x[i:i+1])\n",
    "                self.buffer_y.append(y[i:i+1])\n",
    "                self.task_perms.append(perm)\n",
    "                count += 1\n",
    "        \n",
    "        # 如果超過 buffer 大小，隨機移除\n",
    "        while len(self.buffer_x) > self.buffer_size:\n",
    "            idx = np.random.randint(0, len(self.buffer_x))\n",
    "            del self.buffer_x[idx]\n",
    "            del self.buffer_y[idx]\n",
    "            del self.task_perms[idx]\n",
    "    \n",
    "    def get_batch(self, batch_size: int, device):\n",
    "        \"\"\"取得一批重放樣本\"\"\"\n",
    "        if len(self.buffer_x) == 0:\n",
    "            return None, None, None\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer_x), \n",
    "                                   min(batch_size, len(self.buffer_x)), \n",
    "                                   replace=False)\n",
    "        \n",
    "        batch_x = torch.cat([self.buffer_x[i] for i in indices]).to(device)\n",
    "        batch_y = torch.cat([self.buffer_y[i] for i in indices]).to(device)\n",
    "        batch_perms = [self.task_perms[i] for i in indices]\n",
    "        \n",
    "        return batch_x, batch_y, batch_perms\n",
    "\n",
    "print(\"Experience Replay 已定義\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: 練習題\n",
    "\n",
    "### Exercise 1: 實作 Synaptic Intelligence (SI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynapticIntelligence:\n",
    "    \"\"\"\n",
    "    Synaptic Intelligence (SI)\n",
    "    \n",
    "    核心想法: 追蹤訓練過程中參數對損失下降的貢獻，\n",
    "    貢獻大的參數更重要，應該被保護。\n",
    "    \n",
    "    參考: \"Continual Learning Through Synaptic Intelligence\" (Zenke et al., 2017)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, c: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: 模型\n",
    "            c: 正則化強度\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.c = c\n",
    "        \n",
    "        # 累積的參數重要性\n",
    "        self.omega = {n: torch.zeros_like(p) for n, p in model.named_parameters() if p.requires_grad}\n",
    "        \n",
    "        # 訓練過程中追蹤的變數\n",
    "        self.prev_params = {n: p.clone().detach() for n, p in model.named_parameters() if p.requires_grad}\n",
    "        self.running_sum = {n: torch.zeros_like(p) for n, p in model.named_parameters() if p.requires_grad}\n",
    "    \n",
    "    def update_running_sum(self):\n",
    "        \"\"\"在每個訓練步驟後更新 running sum\"\"\"\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if n in self.running_sum and p.grad is not None:\n",
    "                # 累積: 梯度 * 參數變化\n",
    "                delta = p.detach() - self.prev_params[n]\n",
    "                self.running_sum[n] += -p.grad.detach() * delta\n",
    "                self.prev_params[n] = p.clone().detach()\n",
    "    \n",
    "    def update_omega(self, task_params_start: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        在任務結束時更新 omega\n",
    "        \n",
    "        omega = running_sum / (parameter_change^2 + epsilon)\n",
    "        \"\"\"\n",
    "        epsilon = 1e-7\n",
    "        \n",
    "        for n, p in self.model.named_parameters():\n",
    "            if n in self.omega:\n",
    "                delta = (p.detach() - task_params_start[n]).pow(2) + epsilon\n",
    "                self.omega[n] += self.running_sum[n] / delta\n",
    "        \n",
    "        # 重置 running sum\n",
    "        self.running_sum = {n: torch.zeros_like(p) for n, p in self.model.named_parameters() if p.requires_grad}\n",
    "    \n",
    "    def penalty(self, task_params: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        計算 SI penalty\n",
    "        \n",
    "        penalty = c * sum(omega * (theta - theta_task)^2)\n",
    "        \"\"\"\n",
    "        loss = 0\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if n in self.omega:\n",
    "                loss += (self.omega[n] * (p - task_params[n]).pow(2)).sum()\n",
    "        return self.c * loss\n",
    "\n",
    "print(\"Synaptic Intelligence (SI) 已實作\")\n",
    "print(\"\\nSI vs EWC 比較:\")\n",
    "print(\"- EWC: 使用 Fisher 資訊（在任務結束時計算）\")\n",
    "print(\"- SI: 追蹤訓練過程中參數對損失的貢獻（線上計算）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: 比較不同方法的遺忘程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_continual_learning_methods():\n",
    "    \"\"\"\n",
    "    比較不同持續學習方法的遺忘程度\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 重新創建任務\n",
    "    tasks = create_permuted_mnist_tasks(num_tasks=3)\n",
    "    \n",
    "    # 1. Naive (Sequential Training)\n",
    "    print(\"\\n1. Naive Sequential Training...\")\n",
    "    model_naive = SimpleMLP().to(device)\n",
    "    history_naive = sequential_training(model_naive, tasks, epochs_per_task=3)\n",
    "    results['Naive'] = compute_forgetting_metrics(history_naive)\n",
    "    \n",
    "    # 2. EWC\n",
    "    print(\"\\n2. EWC...\")\n",
    "    model_ewc = SimpleMLP().to(device)\n",
    "    history_ewc = train_with_ewc(model_ewc, tasks, epochs_per_task=3, ewc_lambda=5000)\n",
    "    results['EWC'] = compute_forgetting_metrics(history_ewc)\n",
    "    \n",
    "    # 打印比較結果\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"方法比較結果\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'方法':<20} {'平均遺忘':<15} {'最終平均準確率':<15}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for method, metrics in results.items():\n",
    "        avg_forget = metrics.get('average_forgetting', 0)\n",
    "        avg_acc = metrics.get('average_accuracy', 0)\n",
    "        print(f\"{method:<20} {avg_forget:<15.2%} {avg_acc:<15.2%}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "comparison_results = compare_continual_learning_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                災難性遺忘與持續學習總結                       │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  1. 災難性遺忘                                              │\n",
    "│     • 學習新任務時嚴重忘記舊任務                             │\n",
    "│     • 原因：參數共享、干擾、分布偏移                         │\n",
    "│                                                             │\n",
    "│  2. 量化指標                                                │\n",
    "│     • 平均遺忘、最大遺忘                                    │\n",
    "│     • 後向轉移                                              │\n",
    "│                                                             │\n",
    "│  3. 解決方法                                                │\n",
    "│     • 正則化: EWC, SI（保護重要參數）                        │\n",
    "│     • 重放: 儲存/生成舊任務樣本                              │\n",
    "│     • 架構: LoRA/Adapter per task                          │\n",
    "│                                                             │\n",
    "│  4. LLM 微調中的遺忘                                        │\n",
    "│     • Full fine-tuning 容易遺忘                             │\n",
    "│     • LoRA 可以減少遺忘                                     │\n",
    "│     • 結合 EWC/Replay 效果更好                              │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 下一步學習\n",
    "\n",
    "- **RLHF**: `reinforcement_learning/rlhf_alignment.ipynb`\n",
    "- **模型編輯**: `llm_advanced/model_editing.ipynb`\n",
    "- **模型合併**: `llm_advanced/model_merging.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 參考資源\n",
    "\n",
    "### 課程\n",
    "- [李宏毅 2025 Spring ML HW6](https://speech.ee.ntu.edu.tw/~hylee/ml/2025-spring.php)\n",
    "- [李宏毅 2025 Fall GenAI-ML HW8](https://speech.ee.ntu.edu.tw/~hylee/GenAI-ML/2025-fall.php)\n",
    "\n",
    "### 論文\n",
    "- [EWC: Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/1612.00796)\n",
    "- [SI: Continual Learning Through Synaptic Intelligence](https://arxiv.org/abs/1703.04200)\n",
    "- [Progressive Neural Networks](https://arxiv.org/abs/1606.04671)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
