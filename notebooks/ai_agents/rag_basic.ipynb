{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 基礎 (Retrieval-Augmented Generation)\n",
    "\n",
    "**對應課程**: 李宏毅 2025 Fall GenAI-ML HW2, 2025 Spring ML HW1\n",
    "\n",
    "RAG 是結合檢索與生成的技術，讓 LLM 能夠存取外部知識庫，解決知識截斷和幻覺問題。\n",
    "\n",
    "## 學習目標\n",
    "1. 理解 RAG 的核心架構與動機\n",
    "2. 實作文件載入與切分\n",
    "3. 使用向量嵌入（Embeddings）表示文本\n",
    "4. 建立向量資料庫（FAISS）\n",
    "5. 實作完整的 RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: 為什麼需要 RAG？\n",
    "\n",
    "### 1.1 LLM 的限制\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     LLM 的三大限制                           │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  1. 知識截斷 (Knowledge Cutoff)                              │\n",
    "│     • LLM 只知道訓練資料截止日期前的資訊                      │\n",
    "│     • 無法回答最新事件或更新的知識                           │\n",
    "│                                                             │\n",
    "│  2. 幻覺 (Hallucination)                                    │\n",
    "│     • 模型可能生成看似合理但實際錯誤的內容                    │\n",
    "│     • 特別是專業領域或細節資訊                               │\n",
    "│                                                             │\n",
    "│  3. 無法存取私有資料                                         │\n",
    "│     • 企業內部文件、個人筆記等                               │\n",
    "│     • 需要特定領域知識的應用                                 │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "                    RAG 的解決方案\n",
    "                         ↓\n",
    "    ┌─────────────────────────────────────────────┐\n",
    "    │  檢索相關文件 → 注入 Context → LLM 生成回答  │\n",
    "    └─────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 RAG 架構概覽\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                         RAG Pipeline                                │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│  ┌──────────────────── 離線索引階段 ────────────────────┐          │\n",
    "│  │                                                       │          │\n",
    "│  │   文件庫        切分           嵌入          向量 DB   │          │\n",
    "│  │  ┌─────┐      ┌─────┐      ┌─────┐      ┌─────────┐  │          │\n",
    "│  │  │Doc 1│  →   │Chunk│  →   │ Vec │  →   │ Vector  │  │          │\n",
    "│  │  │Doc 2│      │Chunk│      │ Vec │      │  Store  │  │          │\n",
    "│  │  │ ... │      │ ... │      │ ... │      │ (FAISS) │  │          │\n",
    "│  │  └─────┘      └─────┘      └─────┘      └─────────┘  │          │\n",
    "│  │                                              ↓       │          │\n",
    "│  └──────────────────────────────────────────────────────┘          │\n",
    "│                                                 │                   │\n",
    "│  ┌──────────────────── 線上查詢階段 ────────────┼───────┐          │\n",
    "│  │                                              │       │          │\n",
    "│  │   使用者問題       嵌入          相似度搜尋   │       │          │\n",
    "│  │  ┌─────────┐    ┌─────┐      ┌──────────┐   │       │          │\n",
    "│  │  │Question │ →  │ Vec │  →   │ Top-K    │←──┘       │          │\n",
    "│  │  └─────────┘    └─────┘      │ Retrieval│           │          │\n",
    "│  │                              └────┬─────┘           │          │\n",
    "│  │                                   ↓                 │          │\n",
    "│  │                           ┌──────────────┐          │          │\n",
    "│  │                           │ Retrieved    │          │          │\n",
    "│  │                           │ Documents    │          │          │\n",
    "│  │                           └──────┬───────┘          │          │\n",
    "│  │                                  ↓                  │          │\n",
    "│  │                    ┌────────────────────────┐       │          │\n",
    "│  │                    │   Prompt = Question    │       │          │\n",
    "│  │                    │         + Context      │       │          │\n",
    "│  │                    └───────────┬────────────┘       │          │\n",
    "│  │                                ↓                    │          │\n",
    "│  │                          ┌──────────┐               │          │\n",
    "│  │                          │   LLM    │               │          │\n",
    "│  │                          └────┬─────┘               │          │\n",
    "│  │                               ↓                     │          │\n",
    "│  │                          ┌──────────┐               │          │\n",
    "│  │                          │  Answer  │               │          │\n",
    "│  │                          └──────────┘               │          │\n",
    "│  └─────────────────────────────────────────────────────┘          │\n",
    "│                                                                     │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境設置\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import re\n",
    "import json\n",
    "\n",
    "# 檢查設備\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用設備: {device}\")\n",
    "\n",
    "# 安裝提示\n",
    "print(\"\\n建議安裝的套件:\")\n",
    "print(\"  pip install sentence-transformers faiss-cpu\")\n",
    "print(\"  # 或 GPU 版本: pip install faiss-gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: 文件載入與切分\n",
    "\n",
    "### 2.1 為什麼需要切分文件？\n",
    "\n",
    "- LLM 有 context window 限制\n",
    "- 長文件難以精確檢索\n",
    "- 切分成小塊可以提高相關性匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"文件資料結構\"\"\"\n",
    "    content: str\n",
    "    metadata: Dict = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "\n",
    "class TextSplitter:\n",
    "    \"\"\"文本切分器\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chunk_size: 每個 chunk 的最大字元數\n",
    "            chunk_overlap: 相鄰 chunk 的重疊字元數\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "    \n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"基礎切分：按字元數\"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + self.chunk_size\n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk.strip())\n",
    "            start = end - self.chunk_overlap\n",
    "        \n",
    "        return [c for c in chunks if c]  # 移除空 chunk\n",
    "    \n",
    "    def split_by_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"按句子切分，確保不會切斷句子\"\"\"\n",
    "        # 簡單的句子分割（可以用更複雜的 NLP 工具）\n",
    "        sentences = re.split(r'(?<=[.!?。！？])\\s+', text)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_length = len(sentence)\n",
    "            \n",
    "            if current_length + sentence_length > self.chunk_size and current_chunk:\n",
    "                # 儲存當前 chunk\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                \n",
    "                # 處理 overlap\n",
    "                overlap_text = ' '.join(current_chunk)\n",
    "                overlap_start = max(0, len(overlap_text) - self.chunk_overlap)\n",
    "                overlap_sentences = overlap_text[overlap_start:].strip()\n",
    "                \n",
    "                current_chunk = [overlap_sentences] if overlap_sentences else []\n",
    "                current_length = len(overlap_sentences)\n",
    "            \n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length + 1\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# 測試切分\n",
    "sample_text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience. \n",
    "It focuses on developing algorithms that can access data and use it to learn for themselves. \n",
    "The process begins with observations or data, such as examples, direct experience, or instruction. \n",
    "Deep learning is a type of machine learning that uses neural networks with many layers. \n",
    "These deep neural networks are capable of learning complex patterns in large amounts of data. \n",
    "Applications include image recognition, natural language processing, and autonomous vehicles.\n",
    "Transformers have revolutionized natural language processing since their introduction in 2017.\n",
    "They use self-attention mechanisms to process sequential data more efficiently than RNNs.\n",
    "\"\"\"\n",
    "\n",
    "splitter = TextSplitter(chunk_size=200, chunk_overlap=30)\n",
    "\n",
    "print(\"=== 基礎切分 ===\")\n",
    "chunks = splitter.split_text(sample_text)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(f\"  {chunk[:100]}...\" if len(chunk) > 100 else f\"  {chunk}\")\n",
    "\n",
    "print(\"\\n=== 按句子切分 ===\")\n",
    "chunks_sentences = splitter.split_by_sentences(sample_text)\n",
    "for i, chunk in enumerate(chunks_sentences):\n",
    "    print(f\"\\nChunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(f\"  {chunk[:100]}...\" if len(chunk) > 100 else f\"  {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 切分策略比較\n",
    "\n",
    "```\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                     切分策略比較                              │\n",
    "├────────────────┬─────────────────────────────────────────────┤\n",
    "│ 策略            │ 優缺點                                       │\n",
    "├────────────────┼─────────────────────────────────────────────┤\n",
    "│ 固定長度        │ + 實作簡單                                   │\n",
    "│                │ - 可能切斷語義                                │\n",
    "├────────────────┼─────────────────────────────────────────────┤\n",
    "│ 按句子          │ + 保持句子完整性                              │\n",
    "│                │ - chunk 大小不均勻                            │\n",
    "├────────────────┼─────────────────────────────────────────────┤\n",
    "│ 按段落          │ + 保持段落語義完整                            │\n",
    "│                │ - 段落可能太長                                │\n",
    "├────────────────┼─────────────────────────────────────────────┤\n",
    "│ 語義切分        │ + 最佳語義邊界                                │\n",
    "│ (Semantic)     │ - 計算成本高                                  │\n",
    "├────────────────┼─────────────────────────────────────────────┤\n",
    "│ 遞迴切分        │ + 適應性強                                    │\n",
    "│ (Recursive)    │ - 實作複雜                                    │\n",
    "└────────────────┴─────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecursiveTextSplitter:\n",
    "    \"\"\"遞迴文本切分器 - 類似 LangChain 的實作\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50,\n",
    "                 separators: List[str] = None):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.separators = separators or [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    \n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"遞迴切分文本\"\"\"\n",
    "        return self._split_text(text, self.separators)\n",
    "    \n",
    "    def _split_text(self, text: str, separators: List[str]) -> List[str]:\n",
    "        \"\"\"遞迴切分實作\"\"\"\n",
    "        final_chunks = []\n",
    "        \n",
    "        # 找到第一個可用的分隔符\n",
    "        separator = separators[-1]\n",
    "        for sep in separators:\n",
    "            if sep == \"\":\n",
    "                separator = sep\n",
    "                break\n",
    "            if sep in text:\n",
    "                separator = sep\n",
    "                break\n",
    "        \n",
    "        # 使用分隔符切分\n",
    "        if separator:\n",
    "            splits = text.split(separator)\n",
    "        else:\n",
    "            splits = list(text)\n",
    "        \n",
    "        # 合併小片段\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for split in splits:\n",
    "            split_length = len(split)\n",
    "            \n",
    "            if current_length + split_length > self.chunk_size:\n",
    "                if current_chunk:\n",
    "                    chunk_text = separator.join(current_chunk)\n",
    "                    \n",
    "                    # 如果 chunk 太大，遞迴切分\n",
    "                    if len(chunk_text) > self.chunk_size:\n",
    "                        if separators[:-1]:\n",
    "                            final_chunks.extend(self._split_text(chunk_text, separators[1:]))\n",
    "                        else:\n",
    "                            final_chunks.append(chunk_text[:self.chunk_size])\n",
    "                    else:\n",
    "                        final_chunks.append(chunk_text)\n",
    "                    \n",
    "                    current_chunk = []\n",
    "                    current_length = 0\n",
    "            \n",
    "            current_chunk.append(split)\n",
    "            current_length += split_length + len(separator)\n",
    "        \n",
    "        if current_chunk:\n",
    "            final_chunks.append(separator.join(current_chunk))\n",
    "        \n",
    "        return [c.strip() for c in final_chunks if c.strip()]\n",
    "\n",
    "# 測試遞迴切分\n",
    "recursive_splitter = RecursiveTextSplitter(chunk_size=200, chunk_overlap=30)\n",
    "chunks_recursive = recursive_splitter.split_text(sample_text)\n",
    "\n",
    "print(\"=== 遞迴切分 ===\")\n",
    "for i, chunk in enumerate(chunks_recursive):\n",
    "    print(f\"\\nChunk {i+1} ({len(chunk)} chars): {chunk[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: 向量嵌入（Embeddings）\n",
    "\n",
    "### 3.1 什麼是向量嵌入？\n",
    "\n",
    "將文本映射到高維向量空間，使得語義相似的文本在向量空間中距離較近。\n",
    "\n",
    "```\n",
    "文本                              向量表示\n",
    "\"機器學習\"    ────────→    [0.23, -0.41, 0.87, ...]\n",
    "\"深度學習\"    ────────→    [0.25, -0.38, 0.82, ...]  ← 相似！\n",
    "\"今天天氣\"    ────────→    [-0.52, 0.71, 0.12, ...] ← 不相似\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 sentence-transformers 進行嵌入\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    # 載入模型（多語言支援）\n",
    "    print(\"載入 embedding 模型...\")\n",
    "    embed_model = SentenceTransformer('all-MiniLM-L6-v2')  # 輕量級模型\n",
    "    # 或使用更強的多語言模型: 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    \n",
    "    print(f\"模型載入完成\")\n",
    "    print(f\"向量維度: {embed_model.get_sentence_embedding_dimension()}\")\n",
    "    \n",
    "    # 測試嵌入\n",
    "    test_sentences = [\n",
    "        \"Machine learning is a branch of artificial intelligence.\",\n",
    "        \"Deep learning uses neural networks with many layers.\",\n",
    "        \"The weather is sunny today.\",\n",
    "        \"AI and ML are transforming industries.\",\n",
    "        \"I like to eat pizza for dinner.\"\n",
    "    ]\n",
    "    \n",
    "    embeddings = embed_model.encode(test_sentences)\n",
    "    print(f\"\\n嵌入結果形狀: {embeddings.shape}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"請安裝 sentence-transformers: pip install sentence-transformers\")\n",
    "    embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算並視覺化相似度矩陣\n",
    "def compute_similarity_matrix(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"計算餘弦相似度矩陣\"\"\"\n",
    "    # 正規化\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    normalized = embeddings / norms\n",
    "    # 計算相似度\n",
    "    similarity = np.dot(normalized, normalized.T)\n",
    "    return similarity\n",
    "\n",
    "if embeddings is not None:\n",
    "    similarity_matrix = compute_similarity_matrix(embeddings)\n",
    "    \n",
    "    # 視覺化\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(similarity_matrix, cmap='RdYlBu_r', vmin=-1, vmax=1)\n",
    "    plt.colorbar(label='Cosine Similarity')\n",
    "    \n",
    "    # 標註\n",
    "    short_labels = ['ML/AI', 'Deep Learning', 'Weather', 'AI/ML transform', 'Pizza']\n",
    "    plt.xticks(range(len(short_labels)), short_labels, rotation=45, ha='right')\n",
    "    plt.yticks(range(len(short_labels)), short_labels)\n",
    "    \n",
    "    # 在格子中標註數值\n",
    "    for i in range(len(similarity_matrix)):\n",
    "        for j in range(len(similarity_matrix)):\n",
    "            plt.text(j, i, f'{similarity_matrix[i,j]:.2f}', \n",
    "                    ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    plt.title('文本向量相似度矩陣')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n觀察:\")\n",
    "    print(\"- ML/AI 與 Deep Learning 高度相似 (0.70+)\")\n",
    "    print(\"- Weather 和 Pizza 與技術主題不相似\")\n",
    "    print(\"- AI/ML transform 與前兩個也相似\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 自訂 Embedding 包裝器\n",
    "\n",
    "為了統一介面，我們建立一個 Embedding 類別："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel:\n",
    "    \"\"\"Embedding 模型包裝器\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            self.dimension = self.model.get_sentence_embedding_dimension()\n",
    "        except ImportError:\n",
    "            print(\"警告: 使用隨機嵌入作為替代\")\n",
    "            self.model = None\n",
    "            self.dimension = 384  # 預設維度\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"嵌入多個文件\"\"\"\n",
    "        if self.model:\n",
    "            return self.model.encode(texts, show_progress_bar=False)\n",
    "        else:\n",
    "            # 隨機嵌入（僅作示範）\n",
    "            np.random.seed(42)\n",
    "            return np.random.randn(len(texts), self.dimension).astype(np.float32)\n",
    "    \n",
    "    def embed_query(self, text: str) -> np.ndarray:\n",
    "        \"\"\"嵌入單一查詢\"\"\"\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "# 建立 embedding 模型\n",
    "embedding_model = EmbeddingModel()\n",
    "print(f\"Embedding 維度: {embedding_model.dimension}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: 向量資料庫（FAISS）\n",
    "\n",
    "### 4.1 FAISS 介紹\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) 是一個高效的相似度搜尋庫。\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     FAISS 索引類型                           │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  IndexFlatL2     - 暴力搜尋（最準確，最慢）                   │\n",
    "│  IndexFlatIP     - 內積搜尋                                  │\n",
    "│  IndexIVFFlat    - 倒排索引（更快，需要訓練）                 │\n",
    "│  IndexIVFPQ      - 量化壓縮（記憶體效率）                     │\n",
    "│  IndexHNSW       - 圖搜尋（快速，高記憶體）                   │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 向量儲存\n",
    "try:\n",
    "    import faiss\n",
    "    FAISS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    FAISS_AVAILABLE = False\n",
    "    print(\"FAISS 未安裝，使用簡化實作\")\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"向量儲存與檢索\"\"\"\n",
    "    \n",
    "    def __init__(self, dimension: int):\n",
    "        self.dimension = dimension\n",
    "        self.documents: List[Document] = []\n",
    "        \n",
    "        if FAISS_AVAILABLE:\n",
    "            # 使用 FAISS\n",
    "            self.index = faiss.IndexFlatIP(dimension)  # 內積（需要正規化向量）\n",
    "        else:\n",
    "            # 簡化實作\n",
    "            self.vectors = []\n",
    "    \n",
    "    def add_documents(self, documents: List[Document], embeddings: np.ndarray):\n",
    "        \"\"\"加入文件及其向量\"\"\"\n",
    "        self.documents.extend(documents)\n",
    "        \n",
    "        # 正規化向量（用於餘弦相似度）\n",
    "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        normalized = (embeddings / norms).astype(np.float32)\n",
    "        \n",
    "        if FAISS_AVAILABLE:\n",
    "            self.index.add(normalized)\n",
    "        else:\n",
    "            self.vectors.extend(normalized)\n",
    "    \n",
    "    def search(self, query_embedding: np.ndarray, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"搜尋最相似的文件\"\"\"\n",
    "        # 正規化查詢向量\n",
    "        query_norm = query_embedding / np.linalg.norm(query_embedding)\n",
    "        query_norm = query_norm.astype(np.float32).reshape(1, -1)\n",
    "        \n",
    "        if FAISS_AVAILABLE:\n",
    "            scores, indices = self.index.search(query_norm, k)\n",
    "            results = []\n",
    "            for score, idx in zip(scores[0], indices[0]):\n",
    "                if idx < len(self.documents):\n",
    "                    results.append((self.documents[idx], float(score)))\n",
    "        else:\n",
    "            # 簡化實作：計算所有相似度\n",
    "            vectors = np.array(self.vectors)\n",
    "            scores = np.dot(vectors, query_norm.T).flatten()\n",
    "            top_k_indices = np.argsort(scores)[::-1][:k]\n",
    "            results = [(self.documents[i], float(scores[i])) for i in top_k_indices]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "print(f\"向量儲存類別已定義\")\n",
    "print(f\"使用 FAISS: {FAISS_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: 完整 RAG Pipeline\n",
    "\n",
    "### 5.1 建立知識庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 準備示範知識庫\n",
    "knowledge_base = \"\"\"\n",
    "# PyTorch 深度學習框架\n",
    "\n",
    "PyTorch 是由 Facebook AI Research 開發的開源深度學習框架。它提供了兩個高級功能：張量計算（類似 NumPy）並具有強大的 GPU 加速能力，以及建立在基於磁帶的自動微分系統上的深度神經網路。\n",
    "\n",
    "PyTorch 的主要特點包括：\n",
    "1. 動態計算圖：PyTorch 使用動態計算圖，這意味著圖形是在運行時建立的，使得除錯更加直觀。\n",
    "2. Pythonic：PyTorch 的設計理念是盡可能像 Python，使得學習曲線較平緩。\n",
    "3. 強大的生態系統：包括 torchvision、torchaudio、torchtext 等擴展庫。\n",
    "\n",
    "# Transformer 架構\n",
    "\n",
    "Transformer 是 2017 年由 Google 在論文「Attention Is All You Need」中提出的架構。它完全基於自注意力機制，摒棄了傳統的循環結構。\n",
    "\n",
    "Transformer 的核心組件：\n",
    "1. Self-Attention：允許模型在處理序列時關注不同位置的資訊。\n",
    "2. Multi-Head Attention：並行執行多個注意力函數，捕捉不同的表示子空間。\n",
    "3. Position Encoding：由於沒有循環結構，需要額外加入位置資訊。\n",
    "4. Feed-Forward Network：每個位置獨立處理的全連接網路。\n",
    "\n",
    "# BERT 模型\n",
    "\n",
    "BERT（Bidirectional Encoder Representations from Transformers）是 Google 在 2018 年發表的預訓練語言模型。\n",
    "\n",
    "BERT 的創新之處：\n",
    "1. 雙向編碼：不同於 GPT 的單向，BERT 可以同時看到左右兩邊的上下文。\n",
    "2. Masked Language Model：訓練時隨機遮蔽一些 token，讓模型預測。\n",
    "3. Next Sentence Prediction：學習判斷兩個句子是否連續。\n",
    "\n",
    "BERT 可用於多種下游任務：文本分類、命名實體識別、問答系統等。\n",
    "\n",
    "# GPT 系列模型\n",
    "\n",
    "GPT（Generative Pre-trained Transformer）是 OpenAI 開發的自回歸語言模型系列。\n",
    "\n",
    "GPT 的特點：\n",
    "1. 自回歸生成：每次生成一個 token，基於之前所有的 token。\n",
    "2. 因果注意力：只能看到左側的 context，適合生成任務。\n",
    "3. 規模擴展：從 GPT-1 的 1.17 億參數到 GPT-3 的 1750 億參數。\n",
    "\n",
    "GPT-3 展示了大型語言模型的 few-shot 學習能力，無需微調即可完成多種任務。\n",
    "\n",
    "# 知識蒸餾\n",
    "\n",
    "知識蒸餾（Knowledge Distillation）是一種模型壓縮技術，將大型模型（教師）的知識轉移到小型模型（學生）。\n",
    "\n",
    "蒸餾的過程：\n",
    "1. 使用教師模型生成軟標籤（soft labels）。\n",
    "2. 學生模型同時學習硬標籤和軟標籤。\n",
    "3. 使用溫度參數控制軟化程度。\n",
    "\n",
    "蒸餾損失函數：L = α * L_hard + (1-α) * L_soft\n",
    "\n",
    "# RAG 系統\n",
    "\n",
    "RAG（Retrieval-Augmented Generation）結合了檢索和生成，讓語言模型能夠存取外部知識。\n",
    "\n",
    "RAG 的工作流程：\n",
    "1. 將文件切分成小塊並建立向量索引。\n",
    "2. 收到查詢時，檢索最相關的文件塊。\n",
    "3. 將檢索到的內容與問題一起輸入 LLM。\n",
    "4. LLM 基於提供的上下文生成回答。\n",
    "\n",
    "RAG 的優勢：減少幻覺、支援即時更新知識、可解釋性更強。\n",
    "\"\"\"\n",
    "\n",
    "print(f\"知識庫字數: {len(knowledge_base)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 RAG 系統\n",
    "class SimpleRAG:\n",
    "    \"\"\"簡單的 RAG 系統\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: EmbeddingModel, \n",
    "                 chunk_size: int = 300, chunk_overlap: int = 50):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.splitter = RecursiveTextSplitter(chunk_size, chunk_overlap)\n",
    "        self.vector_store = VectorStore(embedding_model.dimension)\n",
    "        \n",
    "    def index_documents(self, text: str, source: str = \"unknown\"):\n",
    "        \"\"\"索引文件\"\"\"\n",
    "        # 切分文件\n",
    "        chunks = self.splitter.split_text(text)\n",
    "        \n",
    "        # 建立 Document 物件\n",
    "        documents = [\n",
    "            Document(content=chunk, metadata={\"source\": source, \"chunk_id\": i})\n",
    "            for i, chunk in enumerate(chunks)\n",
    "        ]\n",
    "        \n",
    "        # 計算嵌入\n",
    "        embeddings = self.embedding_model.embed_documents([doc.content for doc in documents])\n",
    "        \n",
    "        # 加入向量儲存\n",
    "        self.vector_store.add_documents(documents, embeddings)\n",
    "        \n",
    "        print(f\"已索引 {len(documents)} 個文件塊\")\n",
    "        return documents\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"檢索相關文件\"\"\"\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        results = self.vector_store.search(query_embedding, k)\n",
    "        return results\n",
    "    \n",
    "    def format_context(self, retrieved_docs: List[Tuple[Document, float]]) -> str:\n",
    "        \"\"\"格式化檢索到的文件為上下文\"\"\"\n",
    "        context_parts = []\n",
    "        for i, (doc, score) in enumerate(retrieved_docs, 1):\n",
    "            context_parts.append(f\"[文件 {i}] (相關度: {score:.3f})\\n{doc.content}\")\n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    def create_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"建立 RAG prompt\"\"\"\n",
    "        return f\"\"\"基於以下參考資料回答問題。如果資料中沒有相關資訊，請說明。\n",
    "\n",
    "參考資料：\n",
    "{context}\n",
    "\n",
    "問題：{query}\n",
    "\n",
    "回答：\"\"\"\n",
    "\n",
    "# 建立 RAG 系統\n",
    "rag = SimpleRAG(embedding_model, chunk_size=400, chunk_overlap=50)\n",
    "\n",
    "# 索引知識庫\n",
    "indexed_docs = rag.index_documents(knowledge_base, source=\"deep_learning_notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試檢索\n",
    "test_queries = [\n",
    "    \"什麼是 Transformer？\",\n",
    "    \"BERT 和 GPT 有什麼區別？\",\n",
    "    \"如何進行知識蒸餾？\",\n",
    "    \"RAG 系統的工作流程是什麼？\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"查詢: {query}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    results = rag.retrieve(query, k=2)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\n[結果 {i}] 相關度: {score:.3f}\")\n",
    "        print(f\"內容: {doc.content[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整 RAG 流程（使用 GPT-2 作為示範生成器）\n",
    "try:\n",
    "    from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "    \n",
    "    # 載入生成模型\n",
    "    print(\"載入生成模型...\")\n",
    "    gen_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    gen_model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "    gen_model.eval()\n",
    "    \n",
    "    def generate_answer(prompt: str, max_length: int = 200) -> str:\n",
    "        \"\"\"使用 LLM 生成回答\"\"\"\n",
    "        inputs = gen_tokenizer.encode(prompt, return_tensors='pt', truncation=True, max_length=800)\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = gen_model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=gen_tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # 只返回生成的部分\n",
    "        return response[len(prompt):].strip()\n",
    "    \n",
    "    # 完整 RAG 問答\n",
    "    def rag_query(query: str, k: int = 3) -> str:\n",
    "        \"\"\"執行完整 RAG 流程\"\"\"\n",
    "        # 1. 檢索\n",
    "        retrieved = rag.retrieve(query, k)\n",
    "        \n",
    "        # 2. 格式化上下文\n",
    "        context = rag.format_context(retrieved)\n",
    "        \n",
    "        # 3. 建立 prompt\n",
    "        prompt = rag.create_prompt(query, context)\n",
    "        \n",
    "        # 4. 生成回答\n",
    "        answer = generate_answer(prompt, max_length=150)\n",
    "        \n",
    "        return answer, retrieved\n",
    "    \n",
    "    # 測試完整流程\n",
    "    query = \"什麼是 BERT 模型？它有什麼特點？\"\n",
    "    print(f\"問題: {query}\\n\")\n",
    "    \n",
    "    answer, sources = rag_query(query)\n",
    "    \n",
    "    print(\"檢索到的來源:\")\n",
    "    for i, (doc, score) in enumerate(sources, 1):\n",
    "        print(f\"  [{i}] (相關度 {score:.3f}): {doc.content[:100]}...\")\n",
    "    \n",
    "    print(f\"\\n生成的回答:\\n{answer}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"需要安裝 transformers 才能執行生成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: 評估 RAG 系統\n",
    "\n",
    "### 6.1 檢索品質評估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval(rag_system, test_cases: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    評估檢索品質\n",
    "    \n",
    "    Args:\n",
    "        rag_system: RAG 系統\n",
    "        test_cases: [{\"query\": ..., \"relevant_keywords\": [...]}]\n",
    "    \n",
    "    Returns:\n",
    "        評估指標\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"precision_at_k\": [],\n",
    "        \"recall_at_k\": [],\n",
    "        \"mrr\": []  # Mean Reciprocal Rank\n",
    "    }\n",
    "    \n",
    "    for case in test_cases:\n",
    "        query = case[\"query\"]\n",
    "        relevant_keywords = case[\"relevant_keywords\"]\n",
    "        \n",
    "        # 檢索\n",
    "        retrieved = rag_system.retrieve(query, k=5)\n",
    "        \n",
    "        # 計算 Precision@K\n",
    "        relevant_count = 0\n",
    "        first_relevant_rank = None\n",
    "        \n",
    "        for rank, (doc, score) in enumerate(retrieved, 1):\n",
    "            content_lower = doc.content.lower()\n",
    "            is_relevant = any(kw.lower() in content_lower for kw in relevant_keywords)\n",
    "            \n",
    "            if is_relevant:\n",
    "                relevant_count += 1\n",
    "                if first_relevant_rank is None:\n",
    "                    first_relevant_rank = rank\n",
    "        \n",
    "        k = len(retrieved)\n",
    "        precision = relevant_count / k if k > 0 else 0\n",
    "        recall = relevant_count / len(relevant_keywords) if relevant_keywords else 0\n",
    "        mrr = 1 / first_relevant_rank if first_relevant_rank else 0\n",
    "        \n",
    "        results[\"precision_at_k\"].append(precision)\n",
    "        results[\"recall_at_k\"].append(recall)\n",
    "        results[\"mrr\"].append(mrr)\n",
    "    \n",
    "    # 計算平均\n",
    "    return {\n",
    "        \"avg_precision\": np.mean(results[\"precision_at_k\"]),\n",
    "        \"avg_recall\": np.mean(results[\"recall_at_k\"]),\n",
    "        \"mrr\": np.mean(results[\"mrr\"]),\n",
    "        \"details\": results\n",
    "    }\n",
    "\n",
    "# 準備測試案例\n",
    "test_cases = [\n",
    "    {\n",
    "        \"query\": \"什麼是 Transformer 的核心組件？\",\n",
    "        \"relevant_keywords\": [\"transformer\", \"attention\", \"self-attention\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"BERT 如何進行預訓練？\",\n",
    "        \"relevant_keywords\": [\"bert\", \"masked\", \"預訓練\", \"mlm\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"知識蒸餾的損失函數是什麼？\",\n",
    "        \"relevant_keywords\": [\"蒸餾\", \"distillation\", \"損失\", \"教師\", \"學生\"]\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"RAG 有什麼優勢？\",\n",
    "        \"relevant_keywords\": [\"rag\", \"檢索\", \"幻覺\", \"retrieval\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# 評估\n",
    "eval_results = evaluate_retrieval(rag, test_cases)\n",
    "\n",
    "print(\"RAG 檢索評估結果:\")\n",
    "print(f\"  平均 Precision@K: {eval_results['avg_precision']:.3f}\")\n",
    "print(f\"  平均 Recall@K: {eval_results['avg_recall']:.3f}\")\n",
    "print(f\"  MRR (Mean Reciprocal Rank): {eval_results['mrr']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: 練習題\n",
    "\n",
    "### Exercise 1: 實作語義切分器\n",
    "\n",
    "使用 embedding 相似度來決定切分點。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticTextSplitter:\n",
    "    \"\"\"\n",
    "    基於語義相似度的文本切分器\n",
    "    \n",
    "    想法：當連續句子的語義相似度低於閾值時，在該處切分\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: EmbeddingModel, \n",
    "                 similarity_threshold: float = 0.5,\n",
    "                 min_chunk_size: int = 100):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "    \n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"基於語義的切分\"\"\"\n",
    "        # TODO: 實作語義切分\n",
    "        # 步驟：\n",
    "        # 1. 先按句子切分\n",
    "        # 2. 計算每個句子的嵌入\n",
    "        # 3. 計算相鄰句子的相似度\n",
    "        # 4. 在相似度低於閾值的地方切分\n",
    "        # 5. 合併太短的 chunk\n",
    "        \n",
    "        # 步驟 1: 按句子切分\n",
    "        sentences = re.split(r'(?<=[.!?。！？])\\s+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        if len(sentences) <= 1:\n",
    "            return [text]\n",
    "        \n",
    "        # 步驟 2: 計算嵌入\n",
    "        embeddings = self.embedding_model.embed_documents(sentences)\n",
    "        \n",
    "        # 步驟 3: 計算相鄰相似度\n",
    "        similarities = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            sim = np.dot(embeddings[i], embeddings[i+1]) / (\n",
    "                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i+1])\n",
    "            )\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        # 步驟 4: 找切分點\n",
    "        chunks = []\n",
    "        current_chunk = [sentences[0]]\n",
    "        current_length = len(sentences[0])\n",
    "        \n",
    "        for i, (sentence, sim) in enumerate(zip(sentences[1:], similarities)):\n",
    "            if sim < self.similarity_threshold and current_length >= self.min_chunk_size:\n",
    "                # 切分\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = [sentence]\n",
    "                current_length = len(sentence)\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += len(sentence)\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# 測試語義切分\n",
    "semantic_splitter = SemanticTextSplitter(embedding_model, similarity_threshold=0.4)\n",
    "semantic_chunks = semantic_splitter.split_text(knowledge_base)\n",
    "\n",
    "print(f\"語義切分產生 {len(semantic_chunks)} 個 chunks\")\n",
    "for i, chunk in enumerate(semantic_chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(f\"  {chunk[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: 實作 Hybrid Search（混合搜尋）\n",
    "\n",
    "結合關鍵字搜尋（BM25）和向量搜尋。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25:\n",
    "    \"\"\"簡化的 BM25 實作\"\"\"\n",
    "    \n",
    "    def __init__(self, k1: float = 1.5, b: float = 0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.doc_lengths = []\n",
    "        self.avg_doc_length = 0\n",
    "        self.doc_freqs = {}  # term -> doc count\n",
    "        self.term_freqs = []  # list of {term: freq} for each doc\n",
    "        self.N = 0\n",
    "    \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"簡單的分詞\"\"\"\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "    \n",
    "    def fit(self, documents: List[str]):\n",
    "        \"\"\"建立索引\"\"\"\n",
    "        self.N = len(documents)\n",
    "        self.term_freqs = []\n",
    "        self.doc_lengths = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            tokens = self._tokenize(doc)\n",
    "            self.doc_lengths.append(len(tokens))\n",
    "            \n",
    "            # 計算詞頻\n",
    "            tf = {}\n",
    "            for token in tokens:\n",
    "                tf[token] = tf.get(token, 0) + 1\n",
    "            self.term_freqs.append(tf)\n",
    "            \n",
    "            # 更新文件頻率\n",
    "            for token in set(tokens):\n",
    "                self.doc_freqs[token] = self.doc_freqs.get(token, 0) + 1\n",
    "        \n",
    "        self.avg_doc_length = sum(self.doc_lengths) / self.N\n",
    "    \n",
    "    def score(self, query: str, doc_idx: int) -> float:\n",
    "        \"\"\"計算 BM25 分數\"\"\"\n",
    "        query_tokens = self._tokenize(query)\n",
    "        score = 0\n",
    "        \n",
    "        doc_length = self.doc_lengths[doc_idx]\n",
    "        tf_doc = self.term_freqs[doc_idx]\n",
    "        \n",
    "        for term in query_tokens:\n",
    "            if term not in tf_doc:\n",
    "                continue\n",
    "            \n",
    "            tf = tf_doc[term]\n",
    "            df = self.doc_freqs.get(term, 0)\n",
    "            \n",
    "            # IDF\n",
    "            idf = np.log((self.N - df + 0.5) / (df + 0.5) + 1)\n",
    "            \n",
    "            # TF 正規化\n",
    "            tf_norm = (tf * (self.k1 + 1)) / (\n",
    "                tf + self.k1 * (1 - self.b + self.b * doc_length / self.avg_doc_length)\n",
    "            )\n",
    "            \n",
    "            score += idf * tf_norm\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"搜尋最相關的文件\"\"\"\n",
    "        scores = [(i, self.score(query, i)) for i in range(self.N)]\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scores[:k]\n",
    "\n",
    "class HybridRAG:\n",
    "    \"\"\"混合搜尋 RAG\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: EmbeddingModel, alpha: float = 0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: 向量搜尋的權重（0-1），1-alpha 為 BM25 權重\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        self.alpha = alpha\n",
    "        self.vector_store = VectorStore(embedding_model.dimension)\n",
    "        self.bm25 = BM25()\n",
    "        self.documents: List[Document] = []\n",
    "    \n",
    "    def add_documents(self, documents: List[Document]):\n",
    "        \"\"\"加入文件\"\"\"\n",
    "        self.documents.extend(documents)\n",
    "        \n",
    "        # 向量索引\n",
    "        embeddings = self.embedding_model.embed_documents([d.content for d in documents])\n",
    "        self.vector_store.add_documents(documents, embeddings)\n",
    "        \n",
    "        # BM25 索引\n",
    "        self.bm25.fit([d.content for d in self.documents])\n",
    "    \n",
    "    def search(self, query: str, k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"混合搜尋\"\"\"\n",
    "        # 向量搜尋\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        vector_results = self.vector_store.search(query_embedding, k=k*2)\n",
    "        \n",
    "        # BM25 搜尋\n",
    "        bm25_results = self.bm25.search(query, k=k*2)\n",
    "        \n",
    "        # 正規化分數\n",
    "        vector_scores = {id(r[0]): r[1] for r in vector_results}\n",
    "        max_vector = max(vector_scores.values()) if vector_scores else 1\n",
    "        vector_scores = {k: v/max_vector for k, v in vector_scores.items()}\n",
    "        \n",
    "        bm25_scores = {bm25_results[i][0]: bm25_results[i][1] for i in range(len(bm25_results))}\n",
    "        max_bm25 = max(bm25_scores.values()) if bm25_scores else 1\n",
    "        bm25_scores = {k: v/max_bm25 for k, v in bm25_scores.items()}\n",
    "        \n",
    "        # 合併分數\n",
    "        combined = {}\n",
    "        for doc, _ in vector_results:\n",
    "            doc_id = id(doc)\n",
    "            idx = self.documents.index(doc) if doc in self.documents else -1\n",
    "            v_score = vector_scores.get(doc_id, 0)\n",
    "            b_score = bm25_scores.get(idx, 0)\n",
    "            combined[doc_id] = (doc, self.alpha * v_score + (1-self.alpha) * b_score)\n",
    "        \n",
    "        # 排序並返回\n",
    "        results = sorted(combined.values(), key=lambda x: x[1], reverse=True)[:k]\n",
    "        return results\n",
    "\n",
    "# 測試混合搜尋\n",
    "hybrid_rag = HybridRAG(embedding_model, alpha=0.7)\n",
    "\n",
    "# 使用之前切分的文件\n",
    "docs = [Document(content=chunk) for chunk in chunks_sentences[:10]]\n",
    "hybrid_rag.add_documents(docs)\n",
    "\n",
    "# 搜尋\n",
    "query = \"machine learning neural networks\"\n",
    "results = hybrid_rag.search(query, k=3)\n",
    "\n",
    "print(f\"混合搜尋結果 (query: '{query}'):\")\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"\\n[{i}] Score: {score:.3f}\")\n",
    "    print(f\"    {doc.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    RAG 基礎總結                              │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  1. RAG 解決的問題                                          │\n",
    "│     • 知識截斷、幻覺、私有資料存取                           │\n",
    "│                                                             │\n",
    "│  2. 核心組件                                                │\n",
    "│     • 文件切分（固定/句子/遞迴/語義）                        │\n",
    "│     • 向量嵌入（Sentence Transformers）                     │\n",
    "│     • 向量資料庫（FAISS）                                   │\n",
    "│     • 檢索與生成整合                                        │\n",
    "│                                                             │\n",
    "│  3. 評估指標                                                │\n",
    "│     • Precision@K、Recall@K、MRR                           │\n",
    "│                                                             │\n",
    "│  4. 進階技術                                                │\n",
    "│     • 語義切分                                              │\n",
    "│     • 混合搜尋（向量 + BM25）                               │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 下一步學習\n",
    "\n",
    "- **RAG 進階**: `ai_agents/rag_fundamentals.ipynb` (Reranking, HyDE)\n",
    "- **AI Agent**: `ai_agents/agent_tools.ipynb`\n",
    "- **LLM 微調**: `language_models/llm_finetuning.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 參考資源\n",
    "\n",
    "### 課程\n",
    "- [李宏毅 2025 Fall GenAI-ML HW2](https://speech.ee.ntu.edu.tw/~hylee/GenAI-ML/2025-fall.php)\n",
    "- [李宏毅 2025 Spring ML HW1](https://speech.ee.ntu.edu.tw/~hylee/ml/2025-spring.php)\n",
    "\n",
    "### 論文\n",
    "- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)\n",
    "\n",
    "### 工具\n",
    "- [FAISS](https://github.com/facebookresearch/faiss)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [LangChain](https://python.langchain.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
