{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1：深度學習的數學基礎\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "本模組不是要教你「完整的數學」，而是教你「剛好夠用來理解深度學習」的數學。\n",
    "\n",
    "1. **線性代數**：向量、矩陣、線性變換的直覺\n",
    "2. **微積分**：偏導數、鏈鎖律（backprop 的數學基礎）\n",
    "3. **機率**：隨機變數、期望值、常態分佈、最大似然估計\n",
    "\n",
    "每個概念都會搭配 PyTorch 程式碼，讓你「看得到」數學在做什麼。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 讓圖片更好看\n",
    "plt.rcParams['figure.figsize'] = (10, 4)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1：線性代數\n",
    "\n",
    "### 1.1 向量（Vector）\n",
    "\n",
    "**直覺：** 向量就是「一組有序的數字」，可以想像成空間中的一個點或一個箭頭。\n",
    "\n",
    "在深度學習中，向量無處不在：\n",
    "- 一個樣本的特徵 = 一個向量\n",
    "- 神經網路某一層的輸出 = 一個向量\n",
    "- Word embedding = 一個向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量基本操作\n",
    "v1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "v2 = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "print(\"向量 v1:\", v1)\n",
    "print(\"向量 v2:\", v2)\n",
    "\n",
    "# 向量加法（逐元素相加）\n",
    "print(\"\\n向量加法 v1 + v2:\", v1 + v2)\n",
    "\n",
    "# 純量乘法（每個元素乘以同一個數）\n",
    "print(\"純量乘法 3 * v1:\", 3 * v1)\n",
    "\n",
    "# 逐元素乘法（Hadamard product）\n",
    "print(\"逐元素乘法 v1 * v2:\", v1 * v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 內積（Dot Product）\n",
    "\n",
    "**公式：** $\\mathbf{a} \\cdot \\mathbf{b} = \\sum_i a_i b_i = a_1 b_1 + a_2 b_2 + ...$\n",
    "\n",
    "**直覺：** 內積衡量「兩個向量有多相似」或「在同一個方向上走了多遠」。\n",
    "\n",
    "- 內積 > 0：兩向量方向相近\n",
    "- 內積 = 0：兩向量垂直（正交）\n",
    "- 內積 < 0：兩向量方向相反\n",
    "\n",
    "**在 DL 中的應用：**\n",
    "- 神經網路的每一層基本上都是「輸入向量」和「權重向量」做內積\n",
    "- Attention 機制中，Query 和 Key 做內積來計算相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 內積\n",
    "v1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "v2 = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "# 方法 1：torch.dot\n",
    "dot_product = torch.dot(v1, v2)\n",
    "print(f\"v1 · v2 = {dot_product}\")\n",
    "\n",
    "# 方法 2：手動計算\n",
    "manual_dot = (v1 * v2).sum()\n",
    "print(f\"手動計算: {manual_dot}\")\n",
    "\n",
    "# 驗證：1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32\n",
    "print(f\"驗算: 1*4 + 2*5 + 3*6 = {1*4 + 2*5 + 3*6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 內積的幾何意義：衡量相似度\n",
    "# 先將向量正規化（變成單位向量），內積就變成 cosine similarity\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"計算兩向量的 cosine 相似度\"\"\"\n",
    "    return torch.dot(a, b) / (torch.norm(a) * torch.norm(b))\n",
    "\n",
    "# 相同方向\n",
    "a = torch.tensor([1.0, 0.0])\n",
    "b = torch.tensor([2.0, 0.0])\n",
    "print(f\"相同方向: cos_sim = {cosine_similarity(a, b):.2f}\")\n",
    "\n",
    "# 垂直\n",
    "a = torch.tensor([1.0, 0.0])\n",
    "b = torch.tensor([0.0, 1.0])\n",
    "print(f\"垂直: cos_sim = {cosine_similarity(a, b):.2f}\")\n",
    "\n",
    "# 相反方向\n",
    "a = torch.tensor([1.0, 0.0])\n",
    "b = torch.tensor([-1.0, 0.0])\n",
    "print(f\"相反方向: cos_sim = {cosine_similarity(a, b):.2f}\")\n",
    "\n",
    "# 45 度角\n",
    "a = torch.tensor([1.0, 0.0])\n",
    "b = torch.tensor([1.0, 1.0])\n",
    "print(f\"45度角: cos_sim = {cosine_similarity(a, b):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 矩陣（Matrix）\n",
    "\n",
    "**直覺：** 矩陣就是「把多個向量排在一起」，形成一個 2D 的數字表格。\n",
    "\n",
    "在深度學習中：\n",
    "- 一批資料 = 一個矩陣（每行是一個樣本）\n",
    "- 神經網路的權重 = 矩陣\n",
    "- 圖片 = 矩陣（灰階）或多個矩陣（RGB）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 矩陣基本操作\n",
    "A = torch.tensor([[1., 2.], \n",
    "                  [3., 4.], \n",
    "                  [5., 6.]])  # 3x2 矩陣\n",
    "\n",
    "B = torch.tensor([[7., 8.], \n",
    "                  [9., 10.], \n",
    "                  [11., 12.]])  # 3x2 矩陣\n",
    "\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"A:\\n{A}\")\n",
    "\n",
    "# 矩陣加法（shape 必須相同）\n",
    "print(f\"\\nA + B:\\n{A + B}\")\n",
    "\n",
    "# 轉置（transpose）：行列互換\n",
    "print(f\"\\nA 的轉置 A.T shape: {A.T.shape}\")\n",
    "print(f\"A.T:\\n{A.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 矩陣乘法（Matrix Multiplication）\n",
    "\n",
    "**規則：** 如果 A 是 (m, n) 矩陣，B 是 (n, p) 矩陣，則 A @ B 是 (m, p) 矩陣。\n",
    "\n",
    "**直覺：** 矩陣乘法可以想成「A 的每一行」和「B 的每一列」做內積。\n",
    "\n",
    "**在 DL 中的應用：** 神經網路的核心運算就是矩陣乘法！\n",
    "- 輸入 X: (batch_size, input_dim)\n",
    "- 權重 W: (input_dim, output_dim)\n",
    "- 輸出 Y = X @ W: (batch_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 矩陣乘法\n",
    "A = torch.tensor([[1., 2., 3.],   # 2x3\n",
    "                  [4., 5., 6.]])\n",
    "\n",
    "B = torch.tensor([[7., 8.],       # 3x2\n",
    "                  [9., 10.],\n",
    "                  [11., 12.]])\n",
    "\n",
    "C = A @ B  # 或 torch.matmul(A, B)\n",
    "\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"B shape: {B.shape}\")\n",
    "print(f\"C = A @ B shape: {C.shape}\")\n",
    "print(f\"\\nC:\\n{C}\")\n",
    "\n",
    "# 驗算 C[0,0]：A 的第 0 行和 B 的第 0 列做內積\n",
    "# 1*7 + 2*9 + 3*11 = 7 + 18 + 33 = 58\n",
    "print(f\"\\n驗算 C[0,0] = 1*7 + 2*9 + 3*11 = {1*7 + 2*9 + 3*11}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神經網路實例：線性層就是矩陣乘法\n",
    "batch_size = 4\n",
    "input_dim = 3\n",
    "output_dim = 2\n",
    "\n",
    "# 輸入：4 個樣本，每個 3 維\n",
    "X = torch.randn(batch_size, input_dim)\n",
    "\n",
    "# 權重：3 -> 2\n",
    "W = torch.randn(input_dim, output_dim)\n",
    "b = torch.randn(output_dim)\n",
    "\n",
    "# 線性變換：Y = XW + b\n",
    "Y = X @ W + b\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"W shape: {W.shape}\")\n",
    "print(f\"b shape: {b.shape}\")\n",
    "print(f\"Y shape: {Y.shape}\")\n",
    "\n",
    "# 這就是 nn.Linear 做的事情！\n",
    "linear = nn.Linear(input_dim, output_dim)\n",
    "Y_linear = linear(X)\n",
    "print(f\"\\nnn.Linear output shape: {Y_linear.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 線性變換的直覺\n",
    "\n",
    "**核心概念：** 矩陣乘法 = 線性變換 = 空間的「旋轉、縮放、剪切」\n",
    "\n",
    "當你把一個向量 $\\mathbf{x}$ 乘上矩陣 $A$，得到 $A\\mathbf{x}$，就是把 $\\mathbf{x}$ 「變換」到另一個位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化線性變換\n",
    "\n",
    "def plot_transformation(A, title):\n",
    "    \"\"\"視覺化 2D 線性變換\"\"\"\n",
    "    # 原始的單位正方形四個頂點\n",
    "    square = torch.tensor([[0., 0.], [1., 0.], [1., 1.], [0., 1.], [0., 0.]]).T\n",
    "    \n",
    "    # 變換後的點\n",
    "    transformed = A @ square\n",
    "    \n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    # 原始正方形\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(square[0], square[1], 'b-', linewidth=2, label='Original')\n",
    "    plt.scatter([0], [0], c='red', s=100, zorder=5)  # 原點\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal')\n",
    "    plt.title('Original')\n",
    "    \n",
    "    # 變換後\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(transformed[0], transformed[1], 'r-', linewidth=2, label='Transformed')\n",
    "    plt.scatter([0], [0], c='red', s=100, zorder=5)\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.ylim(-3, 3)\n",
    "    plt.grid(True)\n",
    "    plt.axis('equal')\n",
    "    plt.title(f'After: {title}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 縮放矩陣\n",
    "scale_matrix = torch.tensor([[2., 0.], \n",
    "                              [0., 0.5]])\n",
    "plot_transformation(scale_matrix, 'Scale (2x, 0.5y)')\n",
    "\n",
    "# 旋轉矩陣（45度）\n",
    "theta = torch.tensor(np.pi / 4)  # 45 degrees\n",
    "rotation_matrix = torch.tensor([[torch.cos(theta), -torch.sin(theta)],\n",
    "                                 [torch.sin(theta), torch.cos(theta)]])\n",
    "plot_transformation(rotation_matrix, 'Rotate 45°')\n",
    "\n",
    "# 剪切矩陣\n",
    "shear_matrix = torch.tensor([[1., 0.5], \n",
    "                              [0., 1.]])\n",
    "plot_transformation(shear_matrix, 'Shear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Broadcasting（廣播機制）\n",
    "\n",
    "**問題：** 如果兩個 tensor 的 shape 不同，怎麼做運算？\n",
    "\n",
    "**答案：** PyTorch 會自動「廣播」較小的 tensor，讓它們 shape 相容。\n",
    "\n",
    "這在深度學習中超常用，例如：加 bias（向量加到矩陣每一行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting 範例\n",
    "\n",
    "# 矩陣 + 純量：純量會廣播到每個元素\n",
    "A = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(f\"A + 10:\\n{A + 10}\")\n",
    "\n",
    "# 矩陣 + 向量（行向量）：向量會廣播到每一行\n",
    "A = torch.tensor([[1., 2., 3.],\n",
    "                  [4., 5., 6.]])  # shape: (2, 3)\n",
    "b = torch.tensor([10., 20., 30.])  # shape: (3,)\n",
    "\n",
    "print(f\"\\nA shape: {A.shape}\")\n",
    "print(f\"b shape: {b.shape}\")\n",
    "print(f\"A + b:\\n{A + b}\")\n",
    "print(\"(b 被廣播到 A 的每一行)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這就是為什麼 Y = XW + b 可以運作\n",
    "# X @ W 的結果是 (batch, output_dim)\n",
    "# b 是 (output_dim,)\n",
    "# 相加時 b 會廣播到每個樣本\n",
    "\n",
    "batch_size = 4\n",
    "X = torch.randn(batch_size, 3)\n",
    "W = torch.randn(3, 2)\n",
    "b = torch.randn(2)\n",
    "\n",
    "Y = X @ W + b\n",
    "\n",
    "print(f\"X @ W shape: {(X @ W).shape}\")\n",
    "print(f\"b shape: {b.shape}\")\n",
    "print(f\"Y = X @ W + b shape: {Y.shape}\")\n",
    "print(\"\\nb 被廣播到 batch 中的每一個樣本！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2：微積分（Calculus for Backpropagation）\n",
    "\n",
    "### 2.1 為什麼需要微積分？\n",
    "\n",
    "**核心問題：** 我們想讓 loss 變小，但參數該往哪個方向調？\n",
    "\n",
    "**答案：** 用「梯度」（gradient）—— 它告訴你「如果參數稍微變大，loss 會變大還是變小」。\n",
    "\n",
    "- 梯度為正：增加參數會讓 loss 變大 → 應該減小參數\n",
    "- 梯度為負：增加參數會讓 loss 變小 → 應該增加參數\n",
    "- 梯度為零：已經在極值點（最小或最大）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 導數（Derivative）的直覺\n",
    "\n",
    "**定義：** $\\frac{df}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$\n",
    "\n",
    "**直覺：** 導數就是「斜率」—— 函數在某一點的變化率。\n",
    "\n",
    "常用導數公式：\n",
    "- $\\frac{d}{dx}(x^n) = n \\cdot x^{n-1}$\n",
    "- $\\frac{d}{dx}(e^x) = e^x$\n",
    "- $\\frac{d}{dx}(\\ln x) = \\frac{1}{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 PyTorch 自動計算導數\n",
    "\n",
    "# f(x) = x^2，導數應該是 2x\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "y = x ** 2  # y = 9\n",
    "y.backward()\n",
    "\n",
    "print(f\"f(x) = x^2\")\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"f(x) = {y.item()}\")\n",
    "print(f\"df/dx = {x.grad.item()} (應該是 2*3 = 6)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化導數 = 切線斜率\n",
    "\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "x_vals = torch.linspace(-3, 3, 100)\n",
    "y_vals = f(x_vals)\n",
    "\n",
    "# 在 x=1 點的切線\n",
    "x0 = 1.0\n",
    "y0 = f(torch.tensor(x0))\n",
    "slope = 2 * x0  # 導數 = 2x\n",
    "\n",
    "# 切線方程：y - y0 = slope * (x - x0)\n",
    "tangent_x = torch.linspace(-1, 3, 50)\n",
    "tangent_y = y0 + slope * (tangent_x - x0)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_vals, y_vals, 'b-', linewidth=2, label='$f(x) = x^2$')\n",
    "plt.plot(tangent_x, tangent_y, 'r--', linewidth=2, label=f'Tangent at x={x0} (slope={slope})')\n",
    "plt.scatter([x0], [y0], c='red', s=100, zorder=5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Derivative = Slope of Tangent Line')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlim(-3, 3)\n",
    "plt.ylim(-1, 9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 偏導數（Partial Derivative）\n",
    "\n",
    "**問題：** 如果函數有多個變數怎麼辦？例如 $f(x, y) = x^2 + y^2$\n",
    "\n",
    "**答案：** 對每個變數分別求導，其他變數當作常數。\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = 2x$（把 y 當常數）\n",
    "\n",
    "$\\frac{\\partial f}{\\partial y} = 2y$（把 x 當常數）\n",
    "\n",
    "**梯度（Gradient）** = 所有偏導數組成的向量 = $\\nabla f = [\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 偏導數範例\n",
    "# f(x, y) = x^2 + y^2\n",
    "\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "y = torch.tensor([4.0], requires_grad=True)\n",
    "\n",
    "f = x**2 + y**2  # f = 9 + 16 = 25\n",
    "f.backward()\n",
    "\n",
    "print(f\"f(x, y) = x^2 + y^2\")\n",
    "print(f\"x = {x.item()}, y = {y.item()}\")\n",
    "print(f\"f(x, y) = {f.item()}\")\n",
    "print(f\"\\n∂f/∂x = {x.grad.item()} (應該是 2*3 = 6)\")\n",
    "print(f\"∂f/∂y = {y.grad.item()} (應該是 2*4 = 8)\")\n",
    "print(f\"\\n梯度 ∇f = [{x.grad.item()}, {y.grad.item()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 鏈鎖律（Chain Rule）—— Backpropagation 的數學基礎\n",
    "\n",
    "**問題：** 如果 $y = f(g(x))$（複合函數），怎麼求 $\\frac{dy}{dx}$？\n",
    "\n",
    "**鏈鎖律：** $\\frac{dy}{dx} = \\frac{dy}{dg} \\cdot \\frac{dg}{dx}$\n",
    "\n",
    "**直覺：** 把微分「串起來」。如果 x 變一點點，g 會變多少？g 變了之後，y 又會變多少？\n",
    "\n",
    "**在神經網路中：**\n",
    "```\n",
    "input → layer1 → layer2 → layer3 → loss\n",
    "              ↓\n",
    "        d(loss)/d(input) = d(loss)/d(layer3) × d(layer3)/d(layer2) × d(layer2)/d(layer1) × d(layer1)/d(input)\n",
    "```\n",
    "\n",
    "這就是 **Backpropagation**：從 loss 開始，沿著網路「反向」乘回去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 鏈鎖律範例\n",
    "# y = (2x + 1)^3\n",
    "# 令 u = 2x + 1，則 y = u^3\n",
    "# dy/dx = dy/du * du/dx = 3u^2 * 2 = 6(2x+1)^2\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "u = 2*x + 1  # u = 5\n",
    "y = u ** 3   # y = 125\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(f\"y = (2x + 1)^3\")\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"y = {y.item()}\")\n",
    "print(f\"\\ndy/dx = {x.grad.item()}\")\n",
    "print(f\"手算驗證: 6 * (2*2+1)^2 = 6 * 25 = {6 * 25}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神經網路中的鏈鎖律\n",
    "# 簡化例子：x -> linear -> relu -> linear -> loss\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 資料\n",
    "x = torch.tensor([[1.0, 2.0]], requires_grad=False)\n",
    "target = torch.tensor([[1.0]])\n",
    "\n",
    "# 兩層網路\n",
    "w1 = torch.randn(2, 4, requires_grad=True)\n",
    "w2 = torch.randn(4, 1, requires_grad=True)\n",
    "\n",
    "# Forward pass（記錄計算圖）\n",
    "h = x @ w1              # 第一層線性\n",
    "h_relu = torch.relu(h)  # ReLU 激活\n",
    "y = h_relu @ w2         # 第二層線性\n",
    "loss = (y - target) ** 2  # MSE loss\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "print(f\"  x shape: {x.shape}\")\n",
    "print(f\"  h = x @ w1 shape: {h.shape}\")\n",
    "print(f\"  h_relu shape: {h_relu.shape}\")\n",
    "print(f\"  y = h_relu @ w2 shape: {y.shape}\")\n",
    "print(f\"  loss: {loss.item():.4f}\")\n",
    "\n",
    "# Backward pass（自動應用鏈鎖律）\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nBackward pass (gradients):\")\n",
    "print(f\"  w1.grad shape: {w1.grad.shape}\")\n",
    "print(f\"  w2.grad shape: {w2.grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 梯度下降（Gradient Descent）視覺化\n",
    "\n",
    "**核心思想：** 沿著梯度的反方向走，就能讓 loss 下降。\n",
    "\n",
    "$\\theta_{new} = \\theta_{old} - \\eta \\cdot \\nabla L$\n",
    "\n",
    "其中 $\\eta$ 是學習率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化梯度下降\n",
    "# 目標：找 f(x) = x^2 - 4x + 5 的最小值（應該在 x=2）\n",
    "\n",
    "def f(x):\n",
    "    return x**2 - 4*x + 5\n",
    "\n",
    "def grad_f(x):\n",
    "    return 2*x - 4  # 導數\n",
    "\n",
    "# 梯度下降\n",
    "x = 0.0  # 起始點\n",
    "lr = 0.1  # 學習率\n",
    "history = [(x, f(x))]\n",
    "\n",
    "for i in range(20):\n",
    "    grad = grad_f(x)\n",
    "    x = x - lr * grad  # 梯度下降更新\n",
    "    history.append((x, f(x)))\n",
    "\n",
    "# 視覺化\n",
    "x_vals = np.linspace(-1, 5, 100)\n",
    "y_vals = x_vals**2 - 4*x_vals + 5\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_vals, y_vals, 'b-', linewidth=2)\n",
    "history_x = [h[0] for h in history]\n",
    "history_y = [h[1] for h in history]\n",
    "plt.plot(history_x, history_y, 'ro-', markersize=8, alpha=0.7)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Gradient Descent Path')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_y, 'g-o')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Loss Curve')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"最終 x = {history_x[-1]:.4f} (理論最小值在 x = 2)\")\n",
    "print(f\"最終 f(x) = {history_y[-1]:.4f} (理論最小值 = 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3：機率與統計\n",
    "\n",
    "### 3.1 為什麼深度學習需要機率？\n",
    "\n",
    "深度學習中很多地方都有機率的影子：\n",
    "\n",
    "1. **不確定性**：模型輸出的是「機率」，不是絕對答案\n",
    "2. **損失函數**：Cross-entropy loss 來自機率論\n",
    "3. **正則化**：Dropout 是隨機的\n",
    "4. **生成模型**：VAE、Diffusion 都是機率模型\n",
    "5. **訓練資料**：SGD 的「S」= Stochastic（隨機）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 隨機變數、期望值、變異數\n",
    "\n",
    "**隨機變數（Random Variable）：** 一個可能取不同值的變數，每個值有對應的機率。\n",
    "\n",
    "**期望值（Expected Value）：** 「平均」會得到什麼值\n",
    "$$E[X] = \\sum_x x \\cdot P(X=x)$$\n",
    "\n",
    "**變異數（Variance）：** 值有多「分散」\n",
    "$$Var(X) = E[(X - E[X])^2] = E[X^2] - E[X]^2$$\n",
    "\n",
    "**標準差（Standard Deviation）：** $\\sigma = \\sqrt{Var(X)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 期望值和變異數\n",
    "\n",
    "# 擲骰子：X = {1, 2, 3, 4, 5, 6}，每個機率 = 1/6\n",
    "# E[X] = (1+2+3+4+5+6)/6 = 3.5\n",
    "\n",
    "# 用 PyTorch 模擬\n",
    "torch.manual_seed(42)\n",
    "n_samples = 100000\n",
    "\n",
    "# 模擬擲骰子\n",
    "dice_rolls = torch.randint(1, 7, (n_samples,)).float()\n",
    "\n",
    "mean = dice_rolls.mean()\n",
    "var = dice_rolls.var()\n",
    "std = dice_rolls.std()\n",
    "\n",
    "print(f\"擲 {n_samples} 次骰子：\")\n",
    "print(f\"  期望值 E[X] = {mean:.4f} (理論值 = 3.5)\")\n",
    "print(f\"  變異數 Var[X] = {var:.4f} (理論值 ≈ 2.917)\")\n",
    "print(f\"  標準差 σ = {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 常態分佈（Gaussian / Normal Distribution）\n",
    "\n",
    "**為什麼重要？** 深度學習中到處都是常態分佈：\n",
    "- 權重初始化：通常用常態分佈\n",
    "- Batch Normalization：把資料正規化成近似常態\n",
    "- VAE 的 latent space：假設是常態分佈\n",
    "\n",
    "**公式：** $p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "- $\\mu$：平均值（分佈的中心）\n",
    "- $\\sigma$：標準差（分佈的寬度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常態分佈視覺化\n",
    "\n",
    "# 標準常態分佈 N(0, 1)\n",
    "samples_standard = torch.randn(10000)\n",
    "\n",
    "# N(5, 2) - 平均值 5，標準差 2\n",
    "samples_shifted = torch.randn(10000) * 2 + 5\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(samples_standard.numpy(), bins=50, density=True, alpha=0.7)\n",
    "plt.title('Standard Normal N(0, 1)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(samples_shifted.numpy(), bins=50, density=True, alpha=0.7, color='orange')\n",
    "plt.title('Normal N(5, 2)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"N(0,1): mean={samples_standard.mean():.3f}, std={samples_standard.std():.3f}\")\n",
    "print(f\"N(5,2): mean={samples_shifted.mean():.3f}, std={samples_shifted.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Softmax 與機率\n",
    "\n",
    "**問題：** 分類模型最後一層輸出的是「任意數值」（logits），怎麼變成「機率」？\n",
    "\n",
    "**Softmax：** 把任意數值轉成機率分佈（所有值在 0-1 之間，加總等於 1）\n",
    "\n",
    "$$softmax(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax 範例\n",
    "\n",
    "# 模型輸出的 logits（原始分數）\n",
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "\n",
    "# 手動計算 softmax\n",
    "exp_logits = torch.exp(logits)\n",
    "softmax_manual = exp_logits / exp_logits.sum()\n",
    "\n",
    "# 用 PyTorch 的 softmax\n",
    "softmax_torch = torch.softmax(logits, dim=0)\n",
    "\n",
    "print(f\"Logits: {logits}\")\n",
    "print(f\"\\nSoftmax (手動): {softmax_manual}\")\n",
    "print(f\"Softmax (PyTorch): {softmax_torch}\")\n",
    "print(f\"\\n機率總和: {softmax_torch.sum():.4f}\")\n",
    "\n",
    "# 視覺化\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(['Class 0', 'Class 1', 'Class 2'], logits.numpy())\n",
    "plt.title('Logits (Raw Scores)')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(['Class 0', 'Class 1', 'Class 2'], softmax_torch.numpy())\n",
    "plt.title('After Softmax (Probabilities)')\n",
    "plt.ylabel('Probability')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Cross-Entropy Loss（交叉熵損失）\n",
    "\n",
    "**直覺：** 衡量「預測的機率分佈」和「真實的機率分佈」有多不同。\n",
    "\n",
    "**公式（分類任務）：**\n",
    "$$L = -\\sum_i y_i \\log(p_i)$$\n",
    "\n",
    "其中 $y_i$ 是真實標籤（one-hot），$p_i$ 是預測機率。\n",
    "\n",
    "如果只有一個正確類別 $c$，簡化成：$L = -\\log(p_c)$\n",
    "\n",
    "**直覺：**\n",
    "- 如果預測正確類別的機率接近 1，$-\\log(1) \\approx 0$，loss 很小\n",
    "- 如果預測正確類別的機率接近 0，$-\\log(0) \\to \\infty$，loss 很大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss 範例\n",
    "\n",
    "# 假設 3 個類別，真實標籤是 class 0\n",
    "target = torch.tensor([0])  # class index\n",
    "\n",
    "# 情況 1：模型很有信心，預測正確\n",
    "logits_good = torch.tensor([[5.0, 1.0, 0.1]])  # class 0 分數最高\n",
    "\n",
    "# 情況 2：模型不確定\n",
    "logits_uncertain = torch.tensor([[1.0, 0.9, 0.8]])  # 分數差不多\n",
    "\n",
    "# 情況 3：模型很有信心，但預測錯誤\n",
    "logits_bad = torch.tensor([[0.1, 5.0, 1.0]])  # class 1 分數最高\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_good = criterion(logits_good, target)\n",
    "loss_uncertain = criterion(logits_uncertain, target)\n",
    "loss_bad = criterion(logits_bad, target)\n",
    "\n",
    "print(\"真實標籤: Class 0\\n\")\n",
    "print(f\"情況 1 (預測正確，高信心): logits={logits_good[0].tolist()}\")\n",
    "print(f\"  probs={torch.softmax(logits_good, dim=1)[0].tolist()}\")\n",
    "print(f\"  loss={loss_good.item():.4f}\\n\")\n",
    "\n",
    "print(f\"情況 2 (不確定): logits={logits_uncertain[0].tolist()}\")\n",
    "print(f\"  probs={torch.softmax(logits_uncertain, dim=1)[0].tolist()}\")\n",
    "print(f\"  loss={loss_uncertain.item():.4f}\\n\")\n",
    "\n",
    "print(f\"情況 3 (預測錯誤，高信心): logits={logits_bad[0].tolist()}\")\n",
    "print(f\"  probs={torch.softmax(logits_bad, dim=1)[0].tolist()}\")\n",
    "print(f\"  loss={loss_bad.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 最大似然估計（Maximum Likelihood Estimation, MLE）\n",
    "\n",
    "**核心思想：** 找到讓「觀察到的資料最有可能發生」的參數。\n",
    "\n",
    "**直覺：**\n",
    "1. 你有一些觀察到的資料\n",
    "2. 假設資料來自某個機率分佈（有未知參數）\n",
    "3. 找到讓這些資料「最有可能」被生成的參數\n",
    "\n",
    "**和 Loss 的關係：**\n",
    "- 最大化 likelihood = 最小化 negative log-likelihood\n",
    "- Cross-entropy loss 其實就是 negative log-likelihood！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE 範例：估計常態分佈的參數\n",
    "# 假設資料來自 N(μ, σ)，我們要找出 μ 和 σ\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 生成資料（真實 μ=5, σ=2）\n",
    "true_mu = 5.0\n",
    "true_sigma = 2.0\n",
    "data = torch.randn(1000) * true_sigma + true_mu\n",
    "\n",
    "# MLE 估計（對常態分佈，MLE 就是樣本均值和樣本標準差）\n",
    "estimated_mu = data.mean()\n",
    "estimated_sigma = data.std()\n",
    "\n",
    "print(f\"真實參數: μ={true_mu}, σ={true_sigma}\")\n",
    "print(f\"MLE 估計: μ={estimated_mu:.4f}, σ={estimated_sigma:.4f}\")\n",
    "\n",
    "# 視覺化\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(data.numpy(), bins=50, density=True, alpha=0.7, label='Data')\n",
    "\n",
    "x = torch.linspace(-5, 15, 200)\n",
    "# 標準常態分佈的 PDF\n",
    "pdf = torch.exp(-0.5 * ((x - estimated_mu) / estimated_sigma) ** 2) / (estimated_sigma * np.sqrt(2 * np.pi))\n",
    "plt.plot(x, pdf, 'r-', linewidth=2, label=f'Fitted N({estimated_mu:.2f}, {estimated_sigma:.2f})')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.title('MLE: Fitting a Normal Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 練習題（已完成，請閱讀理解）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 1：向量運算練習\n",
    "\n",
    "**目標：** 熟悉向量內積和 cosine similarity\n",
    "\n",
    "**Hint：**\n",
    "- 內積：`torch.dot(a, b)` 或 `(a * b).sum()`\n",
    "- 向量長度：`torch.norm(a)`\n",
    "- Cosine similarity = 內積 / (長度乘積)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1：計算詞向量相似度\n",
    "# 假設這是三個詞的 embedding\n",
    "\n",
    "# 模擬詞向量（實際應用中會用預訓練的 word2vec 或其他）\n",
    "word_embeddings = {\n",
    "    'king': torch.tensor([0.5, 0.8, 0.1, 0.3]),\n",
    "    'queen': torch.tensor([0.6, 0.9, 0.2, 0.4]),\n",
    "    'apple': torch.tensor([0.1, 0.2, 0.9, 0.8]),\n",
    "}\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"計算 cosine similarity\"\"\"\n",
    "    return torch.dot(a, b) / (torch.norm(a) * torch.norm(b))\n",
    "\n",
    "# 計算所有詞對的相似度\n",
    "words = list(word_embeddings.keys())\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(f\"{'':>8}\", end='')\n",
    "for w in words:\n",
    "    print(f\"{w:>8}\", end='')\n",
    "print()\n",
    "\n",
    "for w1 in words:\n",
    "    print(f\"{w1:>8}\", end='')\n",
    "    for w2 in words:\n",
    "        sim = cosine_similarity(word_embeddings[w1], word_embeddings[w2])\n",
    "        print(f\"{sim.item():>8.3f}\", end='')\n",
    "    print()\n",
    "\n",
    "print(\"\\n觀察：'king' 和 'queen' 很相似，'apple' 和它們都不太像\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：手動實現梯度下降\n",
    "\n",
    "**目標：** 不用 PyTorch autograd，手動計算梯度並更新參數\n",
    "\n",
    "**Hint：**\n",
    "- 對於 $f(x) = x^2$，導數是 $2x$\n",
    "- 梯度下降更新：$x_{new} = x_{old} - lr \\times gradient$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2：手動梯度下降找 f(x) = (x-3)^2 的最小值\n",
    "# 最小值應該在 x = 3\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"目標函數\"\"\"\n",
    "    return (x - 3) ** 2\n",
    "\n",
    "def grad_f(x):\n",
    "    \"\"\"手動計算的梯度：d/dx (x-3)^2 = 2(x-3)\"\"\"\n",
    "    return 2 * (x - 3)\n",
    "\n",
    "# 梯度下降\n",
    "x = 0.0  # 起始點\n",
    "lr = 0.1  # 學習率\n",
    "history = []\n",
    "\n",
    "for i in range(30):\n",
    "    loss = f(x)\n",
    "    grad = grad_f(x)\n",
    "    history.append({'step': i, 'x': x, 'f(x)': loss, 'grad': grad})\n",
    "    \n",
    "    # 更新 x\n",
    "    x = x - lr * grad\n",
    "    \n",
    "    if i < 5 or i >= 25:\n",
    "        print(f\"Step {i:2d}: x = {history[-1]['x']:.4f}, f(x) = {history[-1]['f(x)']:.4f}, grad = {history[-1]['grad']:.4f}\")\n",
    "    elif i == 5:\n",
    "        print(\"...\")\n",
    "\n",
    "print(f\"\\n最終 x = {x:.6f} (目標: 3.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 對比：用 PyTorch autograd\n",
    "x = torch.tensor([0.0], requires_grad=True)\n",
    "lr = 0.1\n",
    "\n",
    "for i in range(30):\n",
    "    # Forward\n",
    "    loss = (x - 3) ** 2\n",
    "    \n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update (不要追蹤這個操作的梯度)\n",
    "    with torch.no_grad():\n",
    "        x -= lr * x.grad\n",
    "    \n",
    "    # 清零梯度\n",
    "    x.grad.zero_()\n",
    "\n",
    "print(f\"PyTorch autograd 結果: x = {x.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3：理解 Softmax 的溫度參數\n",
    "\n",
    "**目標：** 了解 temperature 如何影響 softmax 輸出\n",
    "\n",
    "**Hint：**\n",
    "- Softmax with temperature: $softmax(x_i / T)$\n",
    "- T 越大，分佈越「平坦」（更不確定）\n",
    "- T 越小，分佈越「尖銳」（更確定）\n",
    "- T → 0 時，變成 argmax（只有最大的是 1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 3：Softmax 溫度\n",
    "\n",
    "logits = torch.tensor([2.0, 1.0, 0.5, 0.1])\n",
    "\n",
    "temperatures = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, T in enumerate(temperatures):\n",
    "    probs = torch.softmax(logits / T, dim=0)\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.bar(range(4), probs.numpy())\n",
    "    plt.title(f'T = {T}')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(range(4))\n",
    "    if i == 0:\n",
    "        plt.ylabel('Probability')\n",
    "\n",
    "plt.suptitle('Softmax with Different Temperatures', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"觀察：\")\n",
    "print(\"- T=0.1 (很小): 幾乎是 one-hot，只有最大的有機率\")\n",
    "print(\"- T=1.0 (標準): 正常的 softmax\")\n",
    "print(\"- T=5.0 (很大): 接近均勻分佈\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 4：用矩陣乘法實現一個簡單的神經網路層\n",
    "\n",
    "**目標：** 手動實現 forward 和 backward pass\n",
    "\n",
    "**Hint：**\n",
    "- Forward: $Y = XW + b$\n",
    "- Backward: $\\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 4：手動實現線性層\n",
    "\n",
    "class ManualLinear:\n",
    "    \"\"\"手動實現的線性層（不用 autograd）\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        # 初始化權重（用小的隨機數）\n",
    "        self.W = torch.randn(in_features, out_features) * 0.01\n",
    "        self.b = torch.zeros(out_features)\n",
    "        \n",
    "        # 儲存梯度\n",
    "        self.W_grad = None\n",
    "        self.b_grad = None\n",
    "        \n",
    "        # 儲存 forward 的輸入（backward 時需要）\n",
    "        self.X = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass: Y = XW + b\"\"\"\n",
    "        self.X = X  # 儲存輸入\n",
    "        return X @ self.W + self.b\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Backward pass: 計算參數的梯度\"\"\"\n",
    "        # grad_output 是 dL/dY（從後面傳來的梯度）\n",
    "        \n",
    "        # dL/dW = X^T @ (dL/dY)\n",
    "        self.W_grad = self.X.T @ grad_output\n",
    "        \n",
    "        # dL/db = sum(dL/dY, dim=0)\n",
    "        self.b_grad = grad_output.sum(dim=0)\n",
    "        \n",
    "        # dL/dX = (dL/dY) @ W^T（傳給前一層）\n",
    "        grad_input = grad_output @ self.W.T\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "    def update(self, lr):\n",
    "        \"\"\"梯度下降更新參數\"\"\"\n",
    "        self.W -= lr * self.W_grad\n",
    "        self.b -= lr * self.b_grad\n",
    "\n",
    "# 測試\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 資料\n",
    "X = torch.randn(4, 3)  # 4 個樣本，3 維特徵\n",
    "y_true = torch.randn(4, 2)  # 目標\n",
    "\n",
    "# 手動實現的層\n",
    "layer = ManualLinear(3, 2)\n",
    "\n",
    "# Forward\n",
    "y_pred = layer.forward(X)\n",
    "\n",
    "# Loss (MSE)\n",
    "loss = ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "# dL/dY for MSE: 2 * (y_pred - y_true) / n\n",
    "grad_output = 2 * (y_pred - y_true) / y_pred.numel()\n",
    "\n",
    "# Backward\n",
    "layer.backward(grad_output)\n",
    "\n",
    "print(f\"Forward output shape: {y_pred.shape}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(f\"\\nW_grad shape: {layer.W_grad.shape}\")\n",
    "print(f\"b_grad shape: {layer.b_grad.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 驗證：和 PyTorch autograd 比較\n",
    "\n",
    "# 用相同的初始權重\n",
    "layer_torch = nn.Linear(3, 2)\n",
    "with torch.no_grad():\n",
    "    layer_torch.weight.copy_(layer.W.T)  # nn.Linear 的 weight 是轉置的\n",
    "    layer_torch.bias.copy_(layer.b)\n",
    "\n",
    "# Forward\n",
    "y_pred_torch = layer_torch(X)\n",
    "\n",
    "# Loss\n",
    "loss_torch = nn.MSELoss()(y_pred_torch, y_true)\n",
    "\n",
    "# Backward\n",
    "loss_torch.backward()\n",
    "\n",
    "print(\"PyTorch vs 手動實現的梯度比較：\")\n",
    "print(f\"\\nW_grad (PyTorch):\")\n",
    "print(layer_torch.weight.grad.T)  # 轉置回來比較\n",
    "print(f\"\\nW_grad (手動):\")\n",
    "print(layer.W_grad)\n",
    "print(f\"\\n差異: {(layer_torch.weight.grad.T - layer.W_grad).abs().max().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Module 1 總結\n\n### 線性代數\n- **向量**：一組有序數字，是神經網路的基本資料單位\n- **內積**：衡量相似度，神經網路每一層的核心運算\n- **矩陣乘法**：線性變換，`Y = XW + b` 是神經網路的基礎\n- **Broadcasting**：自動擴展維度，讓不同 shape 的 tensor 可以運算\n\n### 微積分\n- **導數**：函數的變化率（斜率）\n- **偏導數**：多變數函數對單一變數的導數\n- **梯度**：所有偏導數組成的向量，指向函數增長最快的方向\n- **鏈鎖律**：複合函數求導，是 backpropagation 的數學基礎\n\n### 機率\n- **期望值 & 變異數**：描述分佈的中心和散佈程度\n- **常態分佈**：深度學習中最常用的分佈\n- **Softmax**：把任意數值轉成機率分佈\n- **Cross-Entropy**：衡量預測分佈和真實分佈的差異\n- **MLE**：找到最能解釋觀察資料的參數"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 4：進階數學概念（深入理解）\n\n### 4.1 特徵值與特徵向量（Eigenvalues & Eigenvectors）\n\n**為什麼重要？**\n- PCA（主成分分析）的核心\n- 理解矩陣的「本質行為」\n- 神經網路優化的收斂分析\n\n**定義：** 對於矩陣 $A$，如果 $Av = \\lambda v$，則 $v$ 是特徵向量，$\\lambda$ 是特徵值。\n\n**直覺：** 特徵向量是矩陣作用下「只會被縮放而不會改變方向」的向量。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 特徵值分解範例\n\n# 建立一個對稱矩陣（協方差矩陣通常是對稱的）\nA = torch.tensor([[4., 2.],\n                  [2., 3.]], dtype=torch.float32)\n\n# 計算特徵值和特徵向量\neigenvalues, eigenvectors = torch.linalg.eigh(A)  # eigh 用於對稱矩陣\n\nprint(f\"矩陣 A:\\n{A}\")\nprint(f\"\\n特徵值: {eigenvalues}\")\nprint(f\"\\n特徵向量:\\n{eigenvectors}\")\n\n# 驗證：A @ v = λ * v\nfor i in range(2):\n    v = eigenvectors[:, i]\n    lam = eigenvalues[i]\n    Av = A @ v\n    lambda_v = lam * v\n    print(f\"\\n驗證特徵向量 {i+1}:\")\n    print(f\"  A @ v = {Av.tolist()}\")\n    print(f\"  λ * v = {lambda_v.tolist()}\")\n    print(f\"  差異: {(Av - lambda_v).abs().max().item():.6f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# PCA（主成分分析）實例：使用特徵值分解\n\ntorch.manual_seed(42)\n\n# 生成有相關性的 2D 資料\nn_samples = 500\nmean = torch.tensor([0., 0.])\n# 資料主要沿著 (1, 0.5) 方向分佈\nx1 = torch.randn(n_samples)\nx2 = 0.5 * x1 + 0.3 * torch.randn(n_samples)  # x2 和 x1 有相關性\ndata = torch.stack([x1, x2], dim=1)\n\n# 中心化\ndata_centered = data - data.mean(dim=0)\n\n# 計算協方差矩陣\ncov_matrix = (data_centered.T @ data_centered) / (n_samples - 1)\n\n# 特徵值分解\neigenvalues, eigenvectors = torch.linalg.eigh(cov_matrix)\n\n# 按特徵值大小排序（大的在前）\nidx = eigenvalues.argsort(descending=True)\neigenvalues = eigenvalues[idx]\neigenvectors = eigenvectors[:, idx]\n\nprint(f\"協方差矩陣:\\n{cov_matrix}\")\nprint(f\"\\n特徵值（解釋的變異數）: {eigenvalues}\")\nprint(f\"變異數解釋比例: {eigenvalues / eigenvalues.sum()}\")\nprint(f\"\\n主成分（特徵向量）:\\n{eigenvectors}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 視覺化 PCA\nplt.figure(figsize=(12, 5))\n\n# 原始資料\nplt.subplot(1, 2, 1)\nplt.scatter(data[:, 0], data[:, 1], alpha=0.5, s=10)\n\n# 畫主成分方向\norigin = data.mean(dim=0)\nfor i, (eigval, color, label) in enumerate(zip(eigenvalues, ['red', 'blue'], ['PC1', 'PC2'])):\n    vec = eigenvectors[:, i] * np.sqrt(eigval) * 2  # 縮放以便視覺化\n    plt.arrow(origin[0], origin[1], vec[0], vec[1], \n              head_width=0.1, head_length=0.05, fc=color, ec=color, linewidth=2)\n    plt.text(origin[0] + vec[0], origin[1] + vec[1], f' {label}', fontsize=12, color=color)\n\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('原始資料 + 主成分方向')\nplt.axis('equal')\nplt.grid(True)\n\n# 投影到主成分空間\nplt.subplot(1, 2, 2)\ndata_pca = data_centered @ eigenvectors  # 投影\nplt.scatter(data_pca[:, 0], data_pca[:, 1], alpha=0.5, s=10)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('投影到主成分空間後')\nplt.axis('equal')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n觀察：\")\nprint(\"- PC1（紅色）沿著資料變異最大的方向\")\nprint(\"- PC2（藍色）與 PC1 垂直\")\nprint(\"- 投影後的資料，PC1 和 PC2 不相關了\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.2 數值梯度檢查（Gradient Checking）\n\n**為什麼重要？** 當你自己實現 backward 時，很容易出錯。數值梯度檢查可以驗證你的梯度計算是否正確。\n\n**方法：** 用差分近似導數\n$$\\frac{\\partial f}{\\partial \\theta} \\approx \\frac{f(\\theta + \\epsilon) - f(\\theta - \\epsilon)}{2\\epsilon}$$\n\n如果數值梯度和解析梯度相差很小，說明實現是正確的。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def numerical_gradient(f, x, epsilon=1e-5):\n    \"\"\"\n    用差分法計算數值梯度\n    f: 函數，輸入 tensor，輸出純量\n    x: 要計算梯度的 tensor\n    \"\"\"\n    grad = torch.zeros_like(x)\n    x_flat = x.flatten()\n    grad_flat = grad.flatten()\n    \n    for i in range(len(x_flat)):\n        # 保存原值\n        old_val = x_flat[i].item()\n        \n        # f(x + epsilon)\n        x_flat[i] = old_val + epsilon\n        f_plus = f(x.view_as(grad))\n        \n        # f(x - epsilon)\n        x_flat[i] = old_val - epsilon\n        f_minus = f(x.view_as(grad))\n        \n        # 差分\n        grad_flat[i] = (f_plus - f_minus) / (2 * epsilon)\n        \n        # 恢復原值\n        x_flat[i] = old_val\n    \n    return grad\n\n# 測試：簡單函數 f(x) = x^T A x（二次型）\nA = torch.tensor([[2., 1.],\n                  [1., 3.]], dtype=torch.float32)\n\ndef quadratic_form(x):\n    return (x @ A @ x).sum()\n\n# 解析梯度：d/dx (x^T A x) = (A + A^T) x = 2Ax（當 A 對稱時）\nx = torch.tensor([1., 2.], requires_grad=True)\n\n# PyTorch autograd\ny = quadratic_form(x)\ny.backward()\nanalytic_grad = x.grad.clone()\n\n# 數值梯度\nx_no_grad = x.detach().clone()\nnumerical_grad = numerical_gradient(quadratic_form, x_no_grad)\n\nprint(f\"x = {x.detach().tolist()}\")\nprint(f\"f(x) = x^T A x = {y.item()}\")\nprint(f\"\\n解析梯度 (PyTorch): {analytic_grad.tolist()}\")\nprint(f\"數值梯度: {numerical_grad.tolist()}\")\nprint(f\"\\n相對誤差: {((analytic_grad - numerical_grad).norm() / (analytic_grad.norm() + 1e-8)).item():.2e}\")\nprint(\"\\n（相對誤差 < 1e-5 表示梯度正確）\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.3 資訊理論基礎（Information Theory）\n\n資訊理論在深度學習中非常重要，尤其是：\n- Cross-entropy loss 的理論基礎\n- VAE 的 ELBO\n- Diffusion models 的推導\n\n**熵（Entropy）：** 衡量不確定性\n$$H(p) = -\\sum_x p(x) \\log p(x)$$\n\n**KL 散度（Kullback-Leibler Divergence）：** 衡量兩個分佈的差異\n$$D_{KL}(p \\| q) = \\sum_x p(x) \\log \\frac{p(x)}{q(x)}$$\n\n**交叉熵（Cross-Entropy）：**\n$$H(p, q) = -\\sum_x p(x) \\log q(x) = H(p) + D_{KL}(p \\| q)$$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 熵的範例\n\ndef entropy(p):\n    \"\"\"計算離散分佈的熵（使用自然對數）\"\"\"\n    # 避免 log(0)\n    p = p[p > 0]\n    return -(p * torch.log(p)).sum()\n\n# 均勻分佈（最大熵）\nuniform_dist = torch.tensor([0.25, 0.25, 0.25, 0.25])\n\n# 偏斜分佈（較低熵）\nskewed_dist = torch.tensor([0.7, 0.1, 0.1, 0.1])\n\n# 確定分佈（最低熵 = 0）\ncertain_dist = torch.tensor([1.0, 0.0, 0.0, 0.0])\n\nprint(\"熵（Entropy）範例：\")\nprint(\"-\" * 40)\nprint(f\"均勻分佈 {uniform_dist.tolist()}\")\nprint(f\"  熵 = {entropy(uniform_dist):.4f}\")\nprint(f\"  理論最大熵 = ln(4) = {np.log(4):.4f}\")\n\nprint(f\"\\n偏斜分佈 {skewed_dist.tolist()}\")\nprint(f\"  熵 = {entropy(skewed_dist):.4f}\")\n\nprint(f\"\\n確定分佈 {certain_dist.tolist()}\")\nprint(f\"  熵 = {entropy(certain_dist):.4f}\")\n\nprint(\"\\n直覺：熵越大，不確定性越高；確定的事件熵為 0\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# KL 散度範例\n\ndef kl_divergence(p, q):\n    \"\"\"計算 KL(p || q)\"\"\"\n    # 只計算 p > 0 的地方\n    mask = p > 0\n    return (p[mask] * torch.log(p[mask] / q[mask])).sum()\n\n# 真實分佈\np = torch.tensor([0.4, 0.3, 0.2, 0.1])\n\n# 不同的近似分佈\nq1 = torch.tensor([0.4, 0.3, 0.2, 0.1])  # 完全相同\nq2 = torch.tensor([0.35, 0.30, 0.25, 0.10])  # 稍微不同\nq3 = torch.tensor([0.25, 0.25, 0.25, 0.25])  # 均勻分佈\nq4 = torch.tensor([0.1, 0.1, 0.1, 0.7])  # 差很多\n\nprint(\"KL 散度（KL Divergence）範例：\")\nprint(f\"真實分佈 p = {p.tolist()}\")\nprint(\"-\" * 50)\nprint(f\"q1（相同）= {q1.tolist()}\")\nprint(f\"  KL(p||q1) = {kl_divergence(p, q1):.6f}\")\n\nprint(f\"\\nq2（稍有不同）= {q2.tolist()}\")\nprint(f\"  KL(p||q2) = {kl_divergence(p, q2):.6f}\")\n\nprint(f\"\\nq3（均勻）= {q3.tolist()}\")\nprint(f\"  KL(p||q3) = {kl_divergence(p, q3):.6f}\")\n\nprint(f\"\\nq4（差很多）= {q4.tolist()}\")\nprint(f\"  KL(p||q4) = {kl_divergence(p, q4):.6f}\")\n\nprint(\"\\n注意：\")\nprint(\"- KL 散度 >= 0，當且僅當 p = q 時等於 0\")\nprint(\"- KL 散度不對稱：KL(p||q) != KL(q||p)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 高斯分佈的 KL 散度（VAE 的關鍵）\n# KL(N(μ₁, σ₁²) || N(μ₂, σ₂²)) 有解析解\n\ndef gaussian_kl(mu1, sigma1, mu2, sigma2):\n    \"\"\"\n    計算兩個高斯分佈的 KL 散度\n    KL(N(μ₁, σ₁²) || N(μ₂, σ₂²))\n    \"\"\"\n    var1 = sigma1 ** 2\n    var2 = sigma2 ** 2\n    \n    kl = torch.log(sigma2 / sigma1) + (var1 + (mu1 - mu2)**2) / (2 * var2) - 0.5\n    return kl\n\n# VAE 中常用的情況：KL(N(μ, σ²) || N(0, 1))\ndef gaussian_kl_to_standard_normal(mu, log_var):\n    \"\"\"\n    計算 KL(N(μ, exp(log_var)) || N(0, 1))\n    這是 VAE 的正則化項\n    \"\"\"\n    # 解析公式：-0.5 * (1 + log_var - μ² - exp(log_var))\n    return -0.5 * (1 + log_var - mu**2 - torch.exp(log_var))\n\nprint(\"高斯 KL 散度（用於 VAE）：\")\nprint(\"-\" * 50)\n\n# 例子：不同分佈到標準常態的 KL\ntest_cases = [\n    (0.0, 0.0, \"N(0, 1) - 標準常態\"),\n    (1.0, 0.0, \"N(1, 1) - 平均值偏移\"),\n    (0.0, 1.0, \"N(0, e) - 變異數較大\"),\n    (0.0, -1.0, \"N(0, 1/e) - 變異數較小\"),\n    (2.0, 1.0, \"N(2, e) - 兩者都偏\"),\n]\n\nfor mu, log_var, desc in test_cases:\n    mu_t = torch.tensor([mu])\n    log_var_t = torch.tensor([log_var])\n    kl = gaussian_kl_to_standard_normal(mu_t, log_var_t)\n    print(f\"{desc}\")\n    print(f\"  KL = {kl.item():.4f}\")\n\nprint(\"\\n在 VAE 中，這個 KL 散度作為正則化項，\")\nprint(\"鼓勵 latent space 接近標準常態分佈 N(0, 1)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.4 Batch Normalization 的數學\n\nBatch Normalization 是深度學習中最重要的技巧之一。讓我們理解它的數學原理。\n\n**前向傳播：**\n1. 計算 mini-batch 的均值和變異數\n2. 標準化：$\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$\n3. 縮放和平移：$y = \\gamma \\hat{x} + \\beta$\n\n**為什麼有效？**\n- 減少 internal covariate shift\n- 允許使用更大的學習率\n- 有輕微的正則化效果",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 手動實現 Batch Normalization\n\nclass ManualBatchNorm1d:\n    \"\"\"手動實現的 Batch Normalization（用於理解原理）\"\"\"\n    \n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        \n        # 可學習參數\n        self.gamma = torch.ones(num_features)   # 縮放\n        self.beta = torch.zeros(num_features)   # 平移\n        \n        # 運行統計量（用於推理時）\n        self.running_mean = torch.zeros(num_features)\n        self.running_var = torch.ones(num_features)\n        \n        self.training = True\n    \n    def forward(self, x):\n        \"\"\"\n        x: shape (batch_size, num_features)\n        \"\"\"\n        if self.training:\n            # 訓練時：用 batch 統計量\n            batch_mean = x.mean(dim=0)\n            batch_var = x.var(dim=0, unbiased=False)\n            \n            # 更新運行統計量\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n            \n            mean, var = batch_mean, batch_var\n        else:\n            # 推理時：用運行統計量\n            mean, var = self.running_mean, self.running_var\n        \n        # 標準化\n        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n        \n        # 縮放和平移\n        out = self.gamma * x_norm + self.beta\n        \n        return out\n\n# 測試\ntorch.manual_seed(42)\nx = torch.randn(32, 10)  # 32 個樣本，10 個特徵\n\nbn_manual = ManualBatchNorm1d(10)\nbn_torch = nn.BatchNorm1d(10)\n\n# 前向傳播\ny_manual = bn_manual.forward(x)\ny_torch = bn_torch(x)\n\nprint(\"手動 BatchNorm vs PyTorch BatchNorm：\")\nprint(f\"輸入 x: mean={x.mean():.4f}, std={x.std():.4f}\")\nprint(f\"\\n手動 BN 輸出: mean={y_manual.mean():.4f}, std={y_manual.std():.4f}\")\nprint(f\"PyTorch BN 輸出: mean={y_torch.mean():.4f}, std={y_torch.std():.4f}\")\n\n# 檢查每個特徵是否被正確標準化\nprint(f\"\\n每個特徵的統計量（應該接近 mean=0, std=1）:\")\nprint(f\"手動 BN - 特徵均值: {y_manual.mean(dim=0)[:5].tolist()}\")\nprint(f\"手動 BN - 特徵標準差: {y_manual.std(dim=0)[:5].tolist()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 完整總結與實戰 Checklist\n\n### 本 Module 涵蓋的內容：\n\n| 主題 | 重點 | 應用場景 |\n|------|------|----------|\n| **向量與內積** | 相似度計算 | Attention、Embedding |\n| **矩陣乘法** | 線性變換 | 全連接層、所有神經網路層 |\n| **Broadcasting** | 維度擴展 | Bias 加法、標準化 |\n| **導數與偏導** | 變化率 | 理解梯度含義 |\n| **鏈鎖律** | 複合函數求導 | Backpropagation |\n| **梯度下降** | 優化方法 | 模型訓練 |\n| **機率分佈** | 不確定性建模 | 分類輸出、生成模型 |\n| **Softmax** | 轉換為機率 | 分類任務 |\n| **Cross-Entropy** | 分佈差異 | 分類 Loss |\n| **特徵值分解** | 矩陣本質 | PCA、理解協方差 |\n| **數值梯度** | 梯度驗證 | Debug 自定義層 |\n| **KL 散度** | 分佈距離 | VAE、正則化 |\n| **Batch Norm** | 標準化 | 加速訓練、穩定性 |\n\n### 實戰 Checklist：\n\n- [ ] 能用 PyTorch 計算向量內積和 cosine similarity\n- [ ] 理解矩陣乘法的維度規則：`(m, n) @ (n, p) -> (m, p)`\n- [ ] 能解釋 backpropagation 的鏈鎖律原理\n- [ ] 知道 softmax 的作用和溫度參數的影響\n- [ ] 理解 cross-entropy loss 的含義\n- [ ] 能用數值梯度檢查自己的梯度實現\n- [ ] 理解 KL 散度在 VAE 中的作用\n- [ ] 能手動實現 batch normalization\n\n### 下一步：Module 2 - 多層感知機 (MLP) 與訓練技巧",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}