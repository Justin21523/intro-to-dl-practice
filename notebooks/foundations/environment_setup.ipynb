{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 0：環境與基本工具\n",
    "\n",
    "## 學習目標\n",
    "1. 確認 PyTorch + CUDA 環境正常運作\n",
    "2. 掌握 Tensor 基本操作\n",
    "3. 理解 Autograd（自動微分）機制\n",
    "4. 完成第一個 GPU 上的線性回歸實作\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1：環境驗證\n",
    "\n",
    "首先確認你的 PyTorch 和 CUDA 環境設定正確。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available, will use CPU only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 device - 這是之後所有程式都會用到的標準寫法\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2：Tensor 基礎\n",
    "\n",
    "### 2.1 什麼是 Tensor？\n",
    "\n",
    "**直覺理解：** Tensor 就是「多維陣列」，是深度學習中所有資料的載體。\n",
    "\n",
    "| 維度 | 名稱 | 例子 |\n",
    "|------|------|------|\n",
    "| 0D | 純量 (Scalar) | 單一數值，如 `3.14` |\n",
    "| 1D | 向量 (Vector) | `[1, 2, 3]` |\n",
    "| 2D | 矩陣 (Matrix) | 灰階圖片 |\n",
    "| 3D | 3階張量 | RGB 彩色圖 `[C, H, W]` |\n",
    "| 4D | 4階張量 | 一批 RGB 圖 `[B, C, H, W]` |\n",
    "\n",
    "**為什麼不直接用 NumPy？**\n",
    "- PyTorch Tensor 可以在 GPU 上運算（快 10-100 倍）\n",
    "- PyTorch Tensor 支援自動微分（autograd），這是訓練神經網路的關鍵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 建立 Tensor 的各種方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法 1：從 Python list 建立\n",
    "t1 = torch.tensor([1, 2, 3, 4])\n",
    "print(f\"從 list 建立: {t1}\")\n",
    "print(f\"  shape: {t1.shape}, dtype: {t1.dtype}\")\n",
    "\n",
    "# 方法 2：從 NumPy 建立（共享記憶體，修改一方會影響另一方）\n",
    "np_arr = np.array([1.0, 2.0, 3.0])\n",
    "t2 = torch.from_numpy(np_arr)\n",
    "print(f\"\\n從 NumPy 建立: {t2}\")\n",
    "\n",
    "# 方法 3：指定 shape 建立特殊 tensor\n",
    "t_zeros = torch.zeros(2, 3)      # 全零\n",
    "t_ones = torch.ones(2, 3)        # 全一\n",
    "t_rand = torch.rand(2, 3)        # 均勻分佈 [0, 1)\n",
    "t_randn = torch.randn(2, 3)      # 標準常態分佈 N(0, 1)\n",
    "t_eye = torch.eye(3)             # 單位矩陣\n",
    "\n",
    "print(f\"\\nzeros(2,3):\\n{t_zeros}\")\n",
    "print(f\"\\nrand(2,3):\\n{t_rand}\")\n",
    "print(f\"\\neye(3):\\n{t_eye}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法 4：用 _like 建立相同 shape 的 tensor\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "x_zeros = torch.zeros_like(x)    # 相同 shape 的全零\n",
    "x_rand = torch.rand_like(x, dtype=torch.float32)  # 相同 shape 的隨機值\n",
    "\n",
    "print(f\"原始 x:\\n{x}\")\n",
    "print(f\"\\nzeros_like(x):\\n{x_zeros}\")\n",
    "print(f\"\\nrand_like(x):\\n{x_rand}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tensor 屬性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(3, 4, 5)\n",
    "\n",
    "print(f\"Tensor shape: {t.shape}\")           # 或 t.size()\n",
    "print(f\"Tensor ndim: {t.ndim}\")             # 維度數量\n",
    "print(f\"Tensor dtype: {t.dtype}\")           # 資料型別\n",
    "print(f\"Tensor device: {t.device}\")         # 在 CPU 還是 GPU\n",
    "print(f\"Total elements: {t.numel()}\")       # 元素總數 = 3*4*5 = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Tensor 運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1., 2.], [3., 4.]])\n",
    "b = torch.tensor([[5., 6.], [7., 8.]])\n",
    "\n",
    "# 逐元素運算 (element-wise)\n",
    "print(\"逐元素加法 a + b:\")\n",
    "print(a + b)\n",
    "\n",
    "print(\"\\n逐元素乘法 a * b:\")\n",
    "print(a * b)\n",
    "\n",
    "# 矩陣乘法 (matrix multiplication)\n",
    "print(\"\\n矩陣乘法 a @ b:\")\n",
    "print(a @ b)  # 等同於 torch.matmul(a, b)\n",
    "\n",
    "# 常用數學函數\n",
    "print(f\"\\na.sum() = {a.sum()}\")\n",
    "print(f\"a.mean() = {a.mean()}\")\n",
    "print(f\"a.max() = {a.max()}\")\n",
    "print(f\"a.argmax() = {a.argmax()}\")  # 最大值的 index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 沿特定維度運算\n",
    "x = torch.tensor([[1., 2., 3.],\n",
    "                  [4., 5., 6.]])\n",
    "\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"x:\\n{x}\")\n",
    "\n",
    "# dim=0 表示沿著第 0 維（row 方向）壓縮，結果 shape = (3,)\n",
    "print(f\"\\nsum(dim=0): {x.sum(dim=0)}  # 每個 column 加總\")\n",
    "\n",
    "# dim=1 表示沿著第 1 維（column 方向）壓縮，結果 shape = (2,)\n",
    "print(f\"sum(dim=1): {x.sum(dim=1)}  # 每個 row 加總\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Reshape 與 View\n",
    "\n",
    "**重點：** `view` 和 `reshape` 都可以改變 tensor 形狀，但元素總數必須相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12)  # [0, 1, 2, ..., 11]\n",
    "print(f\"原始 x: {x}, shape: {x.shape}\")\n",
    "\n",
    "# reshape 成 3x4 矩陣\n",
    "x_3x4 = x.view(3, 4)\n",
    "print(f\"\\nview(3, 4):\\n{x_3x4}\")\n",
    "\n",
    "# 用 -1 讓 PyTorch 自動計算該維度\n",
    "x_2x6 = x.view(2, -1)  # 2 x ? = 12, 所以 ? = 6\n",
    "print(f\"\\nview(2, -1):\\n{x_2x6}\")\n",
    "\n",
    "# 攤平成 1D\n",
    "x_flat = x_3x4.flatten()\n",
    "print(f\"\\nflatten(): {x_flat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 CPU / GPU 之間移動資料\n",
    "\n",
    "**重點：** 運算時，所有 tensor 必須在同一個 device 上！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立在 CPU 上\n",
    "x_cpu = torch.randn(3, 3)\n",
    "print(f\"x_cpu device: {x_cpu.device}\")\n",
    "\n",
    "# 移動到 GPU\n",
    "if torch.cuda.is_available():\n",
    "    x_gpu = x_cpu.to(device)  # 或 x_cpu.cuda()\n",
    "    print(f\"x_gpu device: {x_gpu.device}\")\n",
    "    \n",
    "    # 直接在 GPU 上建立\n",
    "    y_gpu = torch.randn(3, 3, device=device)\n",
    "    print(f\"y_gpu device: {y_gpu.device}\")\n",
    "    \n",
    "    # GPU tensor 運算\n",
    "    z_gpu = x_gpu @ y_gpu\n",
    "    print(f\"\\nGPU 矩陣乘法結果:\\n{z_gpu}\")\n",
    "    \n",
    "    # 移回 CPU（例如要轉成 NumPy 時必須先移回 CPU）\n",
    "    z_cpu = z_gpu.cpu()\n",
    "    z_numpy = z_cpu.numpy()\n",
    "    print(f\"\\n轉成 NumPy:\\n{z_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3：Autograd（自動微分）\n",
    "\n",
    "### 3.1 為什麼需要自動微分？\n",
    "\n",
    "**直覺：** 訓練神經網路的核心是「調整參數讓 loss 變小」。怎麼知道參數該往哪個方向調？答案是**梯度（gradient）**。\n",
    "\n",
    "- 梯度告訴你：「如果我把這個參數稍微增加一點，loss 會增加還是減少？」\n",
    "- **反向傳播（Backpropagation）** 就是計算所有參數梯度的演算法\n",
    "- PyTorch 的 `autograd` 會自動幫你做這件事！\n",
    "\n",
    "### 3.2 requires_grad 與 backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一個需要追蹤梯度的 tensor\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "print(f\"x = {x}\")\n",
    "print(f\"x.requires_grad = {x.requires_grad}\")\n",
    "\n",
    "# 進行一些運算\n",
    "y = x ** 2          # y = [4, 9]\n",
    "z = y.sum()         # z = 13\n",
    "print(f\"\\ny = x^2 = {y}\")\n",
    "print(f\"z = sum(y) = {z}\")\n",
    "\n",
    "# 反向傳播：計算 dz/dx\n",
    "z.backward()\n",
    "\n",
    "# 梯度存在 x.grad 裡\n",
    "# 數學上：z = x0^2 + x1^2，所以 dz/dx0 = 2*x0 = 4，dz/dx1 = 2*x1 = 6\n",
    "print(f\"\\nx.grad = dz/dx = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Computational Graph（計算圖）\n",
    "\n",
    "**直覺：** PyTorch 在你做運算時，會偷偷記錄「這個值是怎麼算出來的」，形成一張計算圖。\n",
    "\n",
    "當你呼叫 `backward()` 時，PyTorch 就沿著這張圖「反向」走，用 **chain rule（鏈鎖律）** 計算每個節點的梯度。\n",
    "\n",
    "```\n",
    "x ──> y = x^2 ──> z = sum(y)\n",
    "      │              │\n",
    "      │   backward   │\n",
    "      <──────────────┘\n",
    "      dz/dx = dz/dy * dy/dx\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更複雜的例子：線性函數\n",
    "# 模擬 y = w*x + b 的梯度計算\n",
    "\n",
    "w = torch.tensor([2.0], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "x = torch.tensor([3.0])  # 輸入，不需要梯度\n",
    "\n",
    "# Forward pass\n",
    "y = w * x + b  # y = 2*3 + 1 = 7\n",
    "\n",
    "# 假設真實值是 10，計算 MSE loss\n",
    "target = torch.tensor([10.0])\n",
    "loss = (y - target) ** 2  # loss = (7-10)^2 = 9\n",
    "\n",
    "print(f\"y = {y.item():.2f}\")\n",
    "print(f\"loss = {loss.item():.2f}\")\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\nw.grad = d(loss)/dw = {w.grad.item():.2f}\")\n",
    "print(f\"b.grad = d(loss)/db = {b.grad.item():.2f}\")\n",
    "\n",
    "# 手算驗證：\n",
    "# loss = (wx + b - target)^2\n",
    "# d(loss)/dw = 2*(wx + b - target) * x = 2*(7-10)*3 = -18\n",
    "# d(loss)/db = 2*(wx + b - target) * 1 = 2*(7-10)*1 = -6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 重要注意事項"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意 1：梯度會累加！每次 backward 前要清零\n",
    "w = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "for i in range(3):\n",
    "    y = (w * 3).sum()\n",
    "    y.backward()\n",
    "    print(f\"Iteration {i+1}: w.grad = {w.grad}\")\n",
    "\n",
    "print(\"\\n梯度累加了！應該每次都是 3，但變成 3, 6, 9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正確做法：每次 backward 前清零\n",
    "w = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "for i in range(3):\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()  # 清零！底線結尾表示 in-place 操作\n",
    "    \n",
    "    y = (w * 3).sum()\n",
    "    y.backward()\n",
    "    print(f\"Iteration {i+1}: w.grad = {w.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意 2：推理時不需要梯度，用 torch.no_grad() 節省記憶體\n",
    "w = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# 訓練時\n",
    "y_train = w * 3\n",
    "print(f\"訓練時 y.requires_grad = {y_train.requires_grad}\")\n",
    "\n",
    "# 推理時\n",
    "with torch.no_grad():\n",
    "    y_infer = w * 3\n",
    "    print(f\"推理時 y.requires_grad = {y_infer.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4：nn.Module, Optimizer, Loss\n",
    "\n",
    "### 4.1 用 nn.Module 定義模型\n",
    "\n",
    "**直覺：** `nn.Module` 是所有神經網路的基礎類別。你的模型就是一個 Module，裡面可以包含其他 Module（像積木一樣堆疊）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 最簡單的線性模型：y = wx + b\n",
    "class SimpleLinear(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # nn.Linear 包含 weight 和 bias，會自動設定 requires_grad=True\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 定義前向傳播：輸入 x 經過什麼運算得到輸出\n",
    "        return self.linear(x)\n",
    "\n",
    "# 建立模型\n",
    "model = SimpleLinear(input_dim=3, output_dim=1)\n",
    "print(model)\n",
    "\n",
    "# 查看模型參數\n",
    "print(\"\\n模型參數：\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: shape={param.shape}, requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試前向傳播\n",
    "x = torch.randn(5, 3)  # 5 個樣本，每個 3 維\n",
    "y = model(x)           # 等同於 model.forward(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"\\nOutput:\\n{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Loss Function（損失函數）\n",
    "\n",
    "**直覺：** Loss 衡量「模型預測值」和「真實值」之間的差距。訓練的目標就是讓 loss 越小越好。\n",
    "\n",
    "常見 loss：\n",
    "- `nn.MSELoss()`：回歸任務（預測連續值）\n",
    "- `nn.CrossEntropyLoss()`：分類任務（預測類別）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE Loss 範例\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "pred = torch.tensor([1.0, 2.0, 3.0])\n",
    "target = torch.tensor([1.5, 2.5, 3.5])\n",
    "\n",
    "loss = mse_loss(pred, target)\n",
    "print(f\"Predictions: {pred}\")\n",
    "print(f\"Targets: {target}\")\n",
    "print(f\"MSE Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 手算驗證：MSE = mean((1-1.5)^2 + (2-2.5)^2 + (3-3.5)^2) = mean(0.25+0.25+0.25) = 0.25\n",
    "print(f\"手算驗證: {((pred - target) ** 2).mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Optimizer（優化器）\n",
    "\n",
    "**直覺：** Optimizer 負責「根據梯度更新參數」。最基本的是 SGD（隨機梯度下降）：\n",
    "\n",
    "```\n",
    "new_param = old_param - learning_rate * gradient\n",
    "```\n",
    "\n",
    "常見 optimizer：\n",
    "- `torch.optim.SGD`：最基本\n",
    "- `torch.optim.Adam`：最常用，自動調整學習率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 建立模型和優化器\n",
    "model = SimpleLinear(input_dim=3, output_dim=1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # lr = learning rate\n",
    "\n",
    "print(\"優化器會更新這些參數：\")\n",
    "for param_group in optimizer.param_groups:\n",
    "    print(f\"  learning rate: {param_group['lr']}\")\n",
    "    print(f\"  參數數量: {len(param_group['params'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 訓練循環的標準模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準訓練流程（偽代碼）\n",
    "\"\"\"\n",
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        # 1. 清零梯度\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. 前向傳播\n",
    "        pred = model(x_batch)\n",
    "        \n",
    "        # 3. 計算 loss\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        \n",
    "        # 4. 反向傳播（計算梯度）\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. 更新參數\n",
    "        optimizer.step()\n",
    "\"\"\"\n",
    "print(\"標準訓練流程的五個步驟：\")\n",
    "print(\"1. optimizer.zero_grad()  # 清零梯度\")\n",
    "print(\"2. pred = model(x)        # 前向傳播\")\n",
    "print(\"3. loss = loss_fn(pred, y) # 計算損失\")\n",
    "print(\"4. loss.backward()        # 反向傳播\")\n",
    "print(\"5. optimizer.step()       # 更新參數\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5：完整範例 - GPU 上的線性回歸\n",
    "\n",
    "現在把所有東西組合起來，在 GPU 上訓練一個簡單的線性回歸模型。\n",
    "\n",
    "**任務：** 學習 `y = 2*x1 + 3*x2 - 1` 這個關係"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 設定 device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 設定隨機種子（確保結果可重現）\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成合成資料\n",
    "# 真實關係：y = 2*x1 + 3*x2 - 1 + noise\n",
    "\n",
    "n_samples = 1000\n",
    "X = torch.randn(n_samples, 2)  # 1000 個樣本，每個 2 維\n",
    "true_w = torch.tensor([[2.0], [3.0]])\n",
    "true_b = torch.tensor([-1.0])\n",
    "noise = torch.randn(n_samples, 1) * 0.1  # 加一點噪音\n",
    "\n",
    "y = X @ true_w + true_b + noise\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\n真實參數：w = {true_w.squeeze().tolist()}, b = {true_b.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把資料移到 GPU\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "\n",
    "print(f\"X device: {X.device}\")\n",
    "print(f\"y device: {y.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義模型\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 1)  # 2 個輸入，1 個輸出\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 建立模型並移到 GPU\n",
    "model = LinearRegression().to(device)\n",
    "print(model)\n",
    "print(f\"\\n模型在 device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 loss 和 optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# 訓練前的參數\n",
    "print(\"訓練前的參數：\")\n",
    "print(f\"  weight: {model.linear.weight.data}\")\n",
    "print(f\"  bias: {model.linear.bias.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練！\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 1. 清零梯度\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 2. 前向傳播\n",
    "    pred = model(X)\n",
    "    \n",
    "    # 3. 計算 loss\n",
    "    loss = criterion(pred, y)\n",
    "    \n",
    "    # 4. 反向傳播\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. 更新參數\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 記錄 loss\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # 每 20 個 epoch 印一次\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練後的參數\n",
    "print(\"訓練後的參數：\")\n",
    "print(f\"  weight: {model.linear.weight.data}\")\n",
    "print(f\"  bias: {model.linear.bias.data}\")\n",
    "print(f\"\\n真實參數：\")\n",
    "print(f\"  weight: {true_w.squeeze().tolist()}\")\n",
    "print(f\"  bias: {true_b.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 畫 loss 曲線\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 練習題（已完成，請閱讀理解）\n",
    "\n",
    "以下練習題已經完成，包含詳細的 hints 說明。請仔細閱讀程式碼和註解，理解每個步驟的意義。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 1：改變學習率 (Learning Rate)\n",
    "\n",
    "**目標：** 觀察不同學習率對訓練的影響\n",
    "\n",
    "**Hint：**\n",
    "- 學習率太小：收斂很慢，需要更多 epoch\n",
    "- 學習率太大：可能震盪甚至發散（loss 變大或 NaN）\n",
    "- 通常從 0.01 或 0.001 開始嘗試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1：比較不同學習率\n",
    "\n",
    "def train_with_lr(lr, num_epochs=100):\n",
    "    \"\"\"用指定學習率訓練，返回 loss 歷史\"\"\"\n",
    "    # 重新生成資料（確保公平比較）\n",
    "    torch.manual_seed(42)\n",
    "    X = torch.randn(1000, 2, device=device)\n",
    "    y = X @ torch.tensor([[2.0], [3.0]], device=device) + (-1.0) + torch.randn(1000, 1, device=device) * 0.1\n",
    "    \n",
    "    # 重新建立模型\n",
    "    model = LinearRegression().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# 測試不同學習率\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    results[lr] = train_with_lr(lr)\n",
    "    print(f\"LR={lr}: Final loss = {results[lr][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化比較\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for lr, losses in results.items():\n",
    "    plt.plot(losses, label=f'LR={lr}')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Learning Rate Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.yscale('log')  # 用 log scale 更容易看出差異\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n觀察：\")\n",
    "print(\"- LR=0.001：收斂最慢，100 epoch 後還沒完全收斂\")\n",
    "print(\"- LR=0.01：穩定收斂\")\n",
    "print(\"- LR=0.1：收斂很快\")\n",
    "print(\"- LR=0.5：可能會震盪（取決於資料）\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：使用 Adam 優化器\n",
    "\n",
    "**目標：** 比較 SGD 和 Adam 的差異\n",
    "\n",
    "**Hint：**\n",
    "- Adam 會自動調整每個參數的學習率\n",
    "- Adam 通常更容易調參，對初始學習率沒那麼敏感\n",
    "- Adam 的預設 lr=0.001 通常就夠用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2：SGD vs Adam\n",
    "\n",
    "def train_with_optimizer(optimizer_class, lr, num_epochs=100):\n",
    "    \"\"\"用指定優化器訓練\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    X = torch.randn(1000, 2, device=device)\n",
    "    y = X @ torch.tensor([[2.0], [3.0]], device=device) + (-1.0) + torch.randn(1000, 1, device=device) * 0.1\n",
    "    \n",
    "    model = LinearRegression().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# 比較 SGD 和 Adam（都用 lr=0.01）\n",
    "sgd_losses = train_with_optimizer(optim.SGD, lr=0.01)\n",
    "adam_losses = train_with_optimizer(optim.Adam, lr=0.01)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(sgd_losses, label='SGD (lr=0.01)')\n",
    "plt.plot(adam_losses, label='Adam (lr=0.01)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('SGD vs Adam')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"SGD final loss: {sgd_losses[-1]:.6f}\")\n",
    "print(f\"Adam final loss: {adam_losses[-1]:.6f}\")\n",
    "print(\"\\n觀察：Adam 通常在初期收斂更快，對於複雜模型優勢更明顯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3：增加模型複雜度（多層網路）\n",
    "\n",
    "**目標：** 建立一個多層的神經網路\n",
    "\n",
    "**Hint：**\n",
    "- 多層網路需要在層之間加入非線性激活函數（如 ReLU）\n",
    "- 如果沒有激活函數，多層線性層等於一層線性層（因為線性組合的線性組合還是線性）\n",
    "- 對於簡單的線性回歸任務，多層網路可能 overkill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 3：多層網路（MLP）\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"簡單的多層感知機\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # 定義層\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()  # 激活函數\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)   # 線性變換\n",
    "        x = self.relu(x)     # 非線性激活\n",
    "        x = self.layer2(x)   # 線性變換\n",
    "        return x\n",
    "\n",
    "# 建立模型\n",
    "mlp_model = MLP(input_dim=2, hidden_dim=16, output_dim=1).to(device)\n",
    "print(mlp_model)\n",
    "\n",
    "# 計算參數數量\n",
    "total_params = sum(p.numel() for p in mlp_model.parameters())\n",
    "print(f\"\\n總參數數量: {total_params}\")\n",
    "print(\"  - layer1: 2*16 + 16 = 48 (weight + bias)\")\n",
    "print(\"  - layer2: 16*1 + 1 = 17 (weight + bias)\")\n",
    "print(\"  - 總共: 48 + 17 = 65\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練 MLP\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(1000, 2, device=device)\n",
    "y = X @ torch.tensor([[2.0], [3.0]], device=device) + (-1.0) + torch.randn(1000, 1, device=device) * 0.1\n",
    "\n",
    "mlp_model = MLP(input_dim=2, hidden_dim=16, output_dim=1).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.01)\n",
    "\n",
    "mlp_losses = []\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    pred = mlp_model(X)\n",
    "    loss = criterion(pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    mlp_losses.append(loss.item())\n",
    "\n",
    "# 比較單層 vs 多層\n",
    "linear_losses = train_with_optimizer(optim.Adam, lr=0.01)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(linear_losses, label='Linear (2 params)')\n",
    "plt.plot(mlp_losses, label='MLP (65 params)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Linear vs MLP for Linear Regression Task')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Linear final loss: {linear_losses[-1]:.6f}\")\n",
    "print(f\"MLP final loss: {mlp_losses[-1]:.6f}\")\n",
    "print(\"\\n觀察：對於線性關係的資料，簡單的線性模型就夠用了\")\n",
    "print(\"MLP 在這種情況下可能會更慢收斂，因為參數更多\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 4：使用 DataLoader 做 mini-batch 訓練\n",
    "\n",
    "**目標：** 學會使用 DataLoader，這是處理大量資料的標準方式\n",
    "\n",
    "**Hint：**\n",
    "- DataLoader 會自動把資料分成小批次（mini-batch）\n",
    "- Mini-batch 訓練比 full-batch 更常見，因為：\n",
    "  - 節省記憶體（不用一次載入所有資料）\n",
    "  - 有正則化效果（每個 batch 的梯度有噪音）\n",
    "  - 可以利用 GPU 並行計算\n",
    "- 常見 batch size：32, 64, 128, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 4：DataLoader 使用方式\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 準備資料\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(1000, 2)\n",
    "y = X @ torch.tensor([[2.0], [3.0]]) + (-1.0) + torch.randn(1000, 1) * 0.1\n",
    "\n",
    "# 建立 Dataset 和 DataLoader\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=32,      # 每個 batch 32 個樣本\n",
    "    shuffle=True,       # 每個 epoch 打亂順序\n",
    "    num_workers=0,      # 在 notebook 裡用 0，實際訓練可以用 4-8\n",
    "    pin_memory=True     # 加速 CPU->GPU 資料傳輸\n",
    ")\n",
    "\n",
    "print(f\"Dataset 大小: {len(dataset)}\")\n",
    "print(f\"Batch 大小: {dataloader.batch_size}\")\n",
    "print(f\"每個 epoch 的 batch 數: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看看 DataLoader 怎麼產生 batch\n",
    "for i, (x_batch, y_batch) in enumerate(dataloader):\n",
    "    print(f\"Batch {i}: x shape = {x_batch.shape}, y shape = {y_batch.shape}\")\n",
    "    if i >= 2:  # 只看前 3 個 batch\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 DataLoader 訓練\n",
    "model = LinearRegression().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 20\n",
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = []\n",
    "    \n",
    "    for x_batch, y_batch in dataloader:\n",
    "        # 把資料移到 GPU\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        # 標準訓練流程\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    # 記錄這個 epoch 的平均 loss\n",
    "    avg_loss = sum(batch_losses) / len(batch_losses)\n",
    "    epoch_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(f\"\\n訓練後的參數：\")\n",
    "print(f\"  weight: {model.linear.weight.data}\")\n",
    "print(f\"  bias: {model.linear.bias.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Module 0 總結\n\n### 你學到了：\n\n1. **環境驗證**：確認 PyTorch + CUDA 正常運作\n\n2. **Tensor 基礎**：\n   - 建立方式：`torch.tensor()`, `torch.zeros()`, `torch.randn()` 等\n   - 屬性：`shape`, `dtype`, `device`\n   - 運算：逐元素運算 (`+`, `*`)、矩陣乘法 (`@`)\n   - CPU/GPU 移動：`.to(device)`, `.cuda()`, `.cpu()`\n\n3. **Autograd**：\n   - `requires_grad=True` 追蹤梯度\n   - `backward()` 計算梯度\n   - 梯度存在 `.grad` 屬性\n   - 記得清零梯度！\n\n4. **訓練三元素**：\n   - `nn.Module`：定義模型結構\n   - `Loss function`：衡量預測與真實值的差距\n   - `Optimizer`：根據梯度更新參數\n\n5. **訓練流程**：\n   ```python\n   optimizer.zero_grad()  # 清零\n   pred = model(x)        # 前向\n   loss = criterion(pred, y)  # 損失\n   loss.backward()        # 反向\n   optimizer.step()       # 更新\n   ```"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 6：進階實戰技巧\n\n### 6.1 GPU vs CPU 效能比較\n\n在深度學習中，GPU 的平行運算能力是關鍵優勢。讓我們實際測量一下差異。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import time\n\ndef benchmark_matmul(size, device, num_iterations=100):\n    \"\"\"測量矩陣乘法的效能\"\"\"\n    # 建立隨機矩陣\n    A = torch.randn(size, size, device=device)\n    B = torch.randn(size, size, device=device)\n    \n    # 預熱（讓 GPU 準備好）\n    for _ in range(10):\n        C = A @ B\n    \n    if device.type == 'cuda':\n        torch.cuda.synchronize()  # 確保 GPU 運算完成\n    \n    # 計時\n    start = time.time()\n    for _ in range(num_iterations):\n        C = A @ B\n    \n    if device.type == 'cuda':\n        torch.cuda.synchronize()\n    \n    elapsed = time.time() - start\n    return elapsed / num_iterations * 1000  # 返回毫秒\n\n# 測試不同大小的矩陣\nsizes = [256, 512, 1024, 2048, 4096]\ncpu_times = []\ngpu_times = []\n\nprint(\"矩陣乘法效能比較 (毫秒/次):\")\nprint(\"-\" * 50)\n\nfor size in sizes:\n    cpu_time = benchmark_matmul(size, torch.device('cpu'), num_iterations=10)\n    cpu_times.append(cpu_time)\n    \n    if torch.cuda.is_available():\n        gpu_time = benchmark_matmul(size, torch.device('cuda'), num_iterations=100)\n        gpu_times.append(gpu_time)\n        speedup = cpu_time / gpu_time\n        print(f\"Size {size}x{size}: CPU={cpu_time:.2f}ms, GPU={gpu_time:.3f}ms, Speedup={speedup:.1f}x\")\n    else:\n        print(f\"Size {size}x{size}: CPU={cpu_time:.2f}ms, GPU=N/A\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 視覺化效能比較\nif torch.cuda.is_available() and len(gpu_times) > 0:\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    # 絕對時間比較\n    x = np.arange(len(sizes))\n    width = 0.35\n    \n    axes[0].bar(x - width/2, cpu_times, width, label='CPU', color='steelblue')\n    axes[0].bar(x + width/2, gpu_times, width, label='GPU', color='coral')\n    axes[0].set_xlabel('Matrix Size')\n    axes[0].set_ylabel('Time (ms)')\n    axes[0].set_title('Matrix Multiplication Time')\n    axes[0].set_xticks(x)\n    axes[0].set_xticklabels([f'{s}x{s}' for s in sizes])\n    axes[0].legend()\n    axes[0].set_yscale('log')\n    \n    # 加速比\n    speedups = [c/g for c, g in zip(cpu_times, gpu_times)]\n    axes[1].bar(x, speedups, color='green', alpha=0.7)\n    axes[1].set_xlabel('Matrix Size')\n    axes[1].set_ylabel('Speedup (x)')\n    axes[1].set_title('GPU Speedup over CPU')\n    axes[1].set_xticks(x)\n    axes[1].set_xticklabels([f'{s}x{s}' for s in sizes])\n    axes[1].axhline(y=1, color='r', linestyle='--', label='Baseline (1x)')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\n觀察：\")\n    print(\"- 矩陣越大，GPU 的優勢越明顯\")\n    print(\"- 對於小矩陣，CPU-GPU 資料傳輸的開銷可能抵消 GPU 的速度優勢\")\n    print(\"- 這就是為什麼我們用 batch 訓練，而不是一次處理一個樣本\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 6.2 Broadcasting（廣播機制）\n\nBroadcasting 是 PyTorch 自動擴展 tensor 維度以進行運算的機制。掌握這個可以讓你的代碼更簡潔高效。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Broadcasting 範例 1：純量與向量\na = torch.tensor([1, 2, 3])\nb = 10  # 純量\n\nprint(\"純量與向量相加:\")\nprint(f\"a = {a}\")\nprint(f\"a + 10 = {a + b}  # 10 被廣播成 [10, 10, 10]\")\n\n# Broadcasting 範例 2：向量與矩陣\nA = torch.tensor([[1, 2, 3],\n                  [4, 5, 6]])  # shape: (2, 3)\nv = torch.tensor([10, 20, 30])  # shape: (3,)\n\nprint(\"\\n向量與矩陣相加:\")\nprint(f\"A (2x3):\\n{A}\")\nprint(f\"v (3,): {v}\")\nprint(f\"A + v =\\n{A + v}  # v 被廣播成 [[10,20,30],[10,20,30]]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Broadcasting 規則詳解\n# 規則：從最後一個維度開始比較，兩個維度相容當且僅當：\n# 1. 它們相等，或\n# 2. 其中一個是 1\n\n# 實用範例：標準化（Z-score）\ndata = torch.randn(100, 5)  # 100 個樣本，5 個特徵\n\n# 計算每個特徵的均值和標準差\nmean = data.mean(dim=0)  # shape: (5,)\nstd = data.std(dim=0)    # shape: (5,)\n\n# 標準化：(data - mean) / std\n# data: (100, 5)\n# mean: (5,) -> 廣播成 (100, 5)\nnormalized = (data - mean) / std\n\nprint(f\"原始資料 shape: {data.shape}\")\nprint(f\"均值 shape: {mean.shape}\")\nprint(f\"標準化後 shape: {normalized.shape}\")\nprint(f\"\\n標準化後每個特徵的均值（應接近 0）: {normalized.mean(dim=0)}\")\nprint(f\"標準化後每個特徵的標準差（應接近 1）: {normalized.std(dim=0)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 進階 Broadcasting：外積計算（Outer Product）\n# 這在 attention 機制中很常見\n\na = torch.tensor([1, 2, 3])  # shape: (3,)\nb = torch.tensor([4, 5])     # shape: (2,)\n\n# 外積：每個 a 元素與每個 b 元素相乘\n# a.unsqueeze(1): (3,) -> (3, 1)\n# b.unsqueeze(0): (2,) -> (1, 2)\n# 相乘後: (3, 1) * (1, 2) -> (3, 2)\nouter = a.unsqueeze(1) * b.unsqueeze(0)\n\nprint(f\"a: {a}\")\nprint(f\"b: {b}\")\nprint(f\"外積 (outer product):\\n{outer}\")\nprint(f\"\\n這等同於:\\n{torch.outer(a, b)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 6.3 模型保存與載入\n\n訓練好的模型需要保存下來，以便日後推理或繼續訓練。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\n\n# 建立一個簡單模型\nmodel = LinearRegression().to(device)\n\n# 假裝訓練了一下\nwith torch.no_grad():\n    model.linear.weight.fill_(2.5)\n    model.linear.bias.fill_(-1.0)\n\nprint(\"原始模型參數:\")\nprint(f\"  weight: {model.linear.weight.data}\")\nprint(f\"  bias: {model.linear.bias.data}\")\n\n# ========== 方法 1：只保存 state_dict（推薦！） ==========\n# state_dict 只包含模型的參數，不包含模型結構\nsave_path = \"model_weights.pth\"\ntorch.save(model.state_dict(), save_path)\nprint(f\"\\n✓ 模型參數已保存到 {save_path}\")\n\n# 載入：需要先建立模型結構，再載入參數\nmodel_loaded = LinearRegression().to(device)  # 先建立相同結構\nmodel_loaded.load_state_dict(torch.load(save_path, weights_only=True))\nmodel_loaded.eval()  # 切換到評估模式\n\nprint(\"\\n載入後的模型參數:\")\nprint(f\"  weight: {model_loaded.linear.weight.data}\")\nprint(f\"  bias: {model_loaded.linear.bias.data}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ========== 方法 2：保存完整 checkpoint（包含 optimizer 狀態） ==========\n# 當你想繼續訓練時，需要保存 optimizer 的狀態\n\nmodel = LinearRegression().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# 模擬訓練幾步\nfor _ in range(10):\n    x = torch.randn(32, 2, device=device)\n    y = x @ torch.tensor([[2.0], [3.0]], device=device) + (-1.0)\n    \n    optimizer.zero_grad()\n    pred = model(x)\n    loss = nn.MSELoss()(pred, y)\n    loss.backward()\n    optimizer.step()\n\n# 保存完整 checkpoint\ncheckpoint = {\n    'epoch': 10,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'loss': loss.item(),\n}\ntorch.save(checkpoint, 'checkpoint.pth')\nprint(f\"✓ Checkpoint 已保存\")\nprint(f\"  包含: epoch, model_state_dict, optimizer_state_dict, loss\")\n\n# 載入 checkpoint 繼續訓練\ncheckpoint = torch.load('checkpoint.pth', weights_only=False)\nmodel_resume = LinearRegression().to(device)\noptimizer_resume = optim.Adam(model_resume.parameters(), lr=0.01)\n\nmodel_resume.load_state_dict(checkpoint['model_state_dict'])\noptimizer_resume.load_state_dict(checkpoint['optimizer_state_dict'])\nstart_epoch = checkpoint['epoch']\n\nprint(f\"\\n✓ 從 epoch {start_epoch} 繼續訓練\")\nprint(f\"  上次的 loss: {checkpoint['loss']:.6f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 6.4 常見錯誤與除錯技巧\n\n深度學習程式碼的除錯可能很棘手。以下是一些常見錯誤和解決方法。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 錯誤 1：Device 不匹配\nprint(\"=\" * 50)\nprint(\"錯誤 1：Device 不匹配\")\nprint(\"=\" * 50)\n\ntry:\n    x_cpu = torch.randn(3, 3)\n    x_gpu = torch.randn(3, 3, device='cuda') if torch.cuda.is_available() else torch.randn(3, 3)\n    \n    if torch.cuda.is_available():\n        result = x_cpu + x_gpu  # 這會報錯！\nexcept RuntimeError as e:\n    print(f\"錯誤訊息: {str(e)[:100]}...\")\n    print(\"\\n解決方法: 確保所有 tensor 在同一個 device\")\n    print(\"  x_cpu = x_cpu.to(device)  # 或 x_gpu.cpu()\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 錯誤 2：Shape 不匹配\nprint(\"=\" * 50)\nprint(\"錯誤 2：Shape 不匹配\")\nprint(\"=\" * 50)\n\ntry:\n    a = torch.randn(3, 4)  # shape: (3, 4)\n    b = torch.randn(5, 4)  # shape: (5, 4)\n    result = a + b  # 這會報錯！\nexcept RuntimeError as e:\n    print(f\"錯誤訊息: {e}\")\n    print(\"\\n解決方法: 檢查 tensor 的 shape，可能需要 reshape 或 unsqueeze\")\n    print(\"  print(a.shape, b.shape)  # 先檢查 shape\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 錯誤 3：NaN 或 Inf 出現（常見於訓練不穩定）\nprint(\"=\" * 50)\nprint(\"錯誤 3：NaN / Inf 出現\")\nprint(\"=\" * 50)\n\n# 模擬 NaN 產生的情況\nx = torch.tensor([0.0])\ny = torch.log(x)  # log(0) = -inf\nz = 0.0 / 0.0     # 0/0 = nan\n\nprint(f\"log(0) = {y}\")\nprint(f\"0/0 = {z}\")\n\n# 檢測 NaN 和 Inf 的方法\ntensor_with_nan = torch.tensor([1.0, float('nan'), 3.0])\ntensor_with_inf = torch.tensor([1.0, float('inf'), 3.0])\n\nprint(f\"\\n檢測 NaN: torch.isnan() -> {torch.isnan(tensor_with_nan)}\")\nprint(f\"檢測 Inf: torch.isinf() -> {torch.isinf(tensor_with_inf)}\")\nprint(f\"有任何 NaN? {torch.isnan(tensor_with_nan).any()}\")\n\nprint(\"\\n常見原因和解決方法:\")\nprint(\"  1. 學習率太大 -> 降低學習率\")\nprint(\"  2. 數值溢出 -> 使用梯度裁剪 (gradient clipping)\")\nprint(\"  3. log(0) 或 除以 0 -> 加入 epsilon (如 log(x + 1e-8))\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 除錯技巧：使用 hooks 監控中間層\nprint(\"=\" * 50)\nprint(\"除錯技巧：使用 hooks 監控中間層輸出\")\nprint(\"=\" * 50)\n\n# 定義一個簡單的網路\nclass DebugNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(10, 20)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(20, 5)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n# 建立模型\ndebug_model = DebugNet()\n\n# 用來儲存中間輸出的字典\nactivations = {}\n\n# 定義 hook 函數\ndef get_activation(name):\n    def hook(model, input, output):\n        activations[name] = output.detach()\n    return hook\n\n# 註冊 hooks\ndebug_model.fc1.register_forward_hook(get_activation('fc1'))\ndebug_model.relu.register_forward_hook(get_activation('relu'))\n\n# 前向傳播\nx = torch.randn(2, 10)\noutput = debug_model(x)\n\n# 查看中間層輸出\nprint(f\"輸入 shape: {x.shape}\")\nprint(f\"fc1 輸出 shape: {activations['fc1'].shape}\")\nprint(f\"relu 輸出 shape: {activations['relu'].shape}\")\nprint(f\"最終輸出 shape: {output.shape}\")\nprint(f\"\\nfc1 輸出統計: mean={activations['fc1'].mean():.3f}, std={activations['fc1'].std():.3f}\")\nprint(f\"relu 輸出統計: mean={activations['relu'].mean():.3f}, std={activations['relu'].std():.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 6.5 GPU 記憶體管理\n\n當你在訓練大型模型時，GPU 記憶體管理變得至關重要。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "if torch.cuda.is_available():\n    # 查看 GPU 記憶體使用情況\n    print(\"GPU 記憶體管理工具:\")\n    print(\"-\" * 50)\n    \n    # 清空 cache 前\n    allocated_before = torch.cuda.memory_allocated() / 1e6\n    reserved_before = torch.cuda.memory_reserved() / 1e6\n    \n    # 建立一些 tensors\n    tensors = [torch.randn(1000, 1000, device='cuda') for _ in range(10)]\n    \n    allocated_during = torch.cuda.memory_allocated() / 1e6\n    reserved_during = torch.cuda.memory_reserved() / 1e6\n    \n    print(f\"建立 10 個 1000x1000 tensor 後:\")\n    print(f\"  已分配記憶體: {allocated_during:.1f} MB\")\n    print(f\"  已保留記憶體: {reserved_during:.1f} MB\")\n    \n    # 刪除 tensors\n    del tensors\n    torch.cuda.empty_cache()  # 釋放未使用的 cache\n    \n    allocated_after = torch.cuda.memory_allocated() / 1e6\n    reserved_after = torch.cuda.memory_reserved() / 1e6\n    \n    print(f\"\\n刪除 tensors 並清空 cache 後:\")\n    print(f\"  已分配記憶體: {allocated_after:.1f} MB\")\n    print(f\"  已保留記憶體: {reserved_after:.1f} MB\")\n    \n    # 記憶體摘要\n    print(f\"\\n完整記憶體摘要:\")\n    print(torch.cuda.memory_summary(abbreviated=True))\nelse:\n    print(\"CUDA 不可用，跳過 GPU 記憶體管理示範\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 6.6 混合精度訓練預覽 (Mixed Precision Training)\n\n混合精度訓練使用 FP16 和 FP32 的組合，可以：\n- 減少約一半的 GPU 記憶體使用\n- 加速訓練（在 Tensor Core GPU 上）\n- 通常不會損失模型精度",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 混合精度訓練範例\nfrom torch.amp import autocast, GradScaler\n\n# 建立模型和資料\nmodel = MLP(input_dim=100, hidden_dim=256, output_dim=10).to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# 建立 GradScaler（只在 CUDA 上使用）\nscaler = GradScaler('cuda') if torch.cuda.is_available() else None\n\n# 模擬資料\nx = torch.randn(64, 100, device=device)\ny = torch.randint(0, 10, (64,), device=device)\n\nprint(\"混合精度訓練示範:\")\nprint(\"-\" * 50)\n\n# 標準訓練（FP32）\noptimizer.zero_grad()\nwith torch.no_grad():\n    start_mem = torch.cuda.memory_allocated() / 1e6 if torch.cuda.is_available() else 0\n\npred_fp32 = model(x)\nloss_fp32 = criterion(pred_fp32, y)\nloss_fp32.backward()\n\nwith torch.no_grad():\n    fp32_mem = torch.cuda.memory_allocated() / 1e6 if torch.cuda.is_available() else 0\n\nprint(f\"FP32 訓練:\")\nprint(f\"  Loss: {loss_fp32.item():.4f}\")\nif torch.cuda.is_available():\n    print(f\"  記憶體使用: {fp32_mem:.1f} MB\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 混合精度訓練（AMP）\nif torch.cuda.is_available():\n    model_amp = MLP(input_dim=100, hidden_dim=256, output_dim=10).to(device)\n    optimizer_amp = optim.Adam(model_amp.parameters(), lr=0.001)\n    scaler = GradScaler('cuda')\n    \n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    \n    optimizer_amp.zero_grad()\n    \n    # 使用 autocast 自動轉換精度\n    with autocast('cuda'):\n        pred_amp = model_amp(x)\n        loss_amp = criterion(pred_amp, y)\n    \n    # 使用 scaler 來縮放 loss，避免 FP16 下溢\n    scaler.scale(loss_amp).backward()\n    scaler.step(optimizer_amp)\n    scaler.update()\n    \n    amp_mem = torch.cuda.max_memory_allocated() / 1e6\n    \n    print(f\"\\n混合精度 (AMP) 訓練:\")\n    print(f\"  Loss: {loss_amp.item():.4f}\")\n    print(f\"  峰值記憶體使用: {amp_mem:.1f} MB\")\n    \n    print(\"\\n混合精度訓練的完整模板:\")\n    print(\"\"\"\n    scaler = GradScaler('cuda')\n    \n    for epoch in range(num_epochs):\n        for x_batch, y_batch in dataloader:\n            optimizer.zero_grad()\n            \n            with autocast('cuda'):\n                pred = model(x_batch)\n                loss = criterion(pred, y_batch)\n            \n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n    \"\"\")\nelse:\n    print(\"CUDA 不可用，跳過混合精度示範\")\n    print(\"注意：混合精度訓練主要在 GPU 上使用\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 清理臨時檔案\nimport os\nfor f in ['model_weights.pth', 'checkpoint.pth']:\n    if os.path.exists(f):\n        os.remove(f)\n        print(f\"已刪除臨時檔案: {f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 完整總結\n\n### 本 Module 涵蓋的內容：\n\n| 主題 | 重點 |\n|------|------|\n| **環境設定** | PyTorch + CUDA 驗證、device 設定 |\n| **Tensor 基礎** | 建立、屬性、運算、reshape、device 移動 |\n| **Autograd** | requires_grad、backward、計算圖、梯度清零 |\n| **訓練元件** | nn.Module、Loss、Optimizer |\n| **訓練流程** | zero_grad → forward → loss → backward → step |\n| **GPU 效能** | CPU vs GPU 比較、記憶體管理 |\n| **Broadcasting** | 維度擴展、標準化、外積 |\n| **模型保存** | state_dict、checkpoint |\n| **除錯技巧** | device 不匹配、shape 錯誤、NaN/Inf、hooks |\n| **混合精度** | autocast、GradScaler |\n\n### 實戰 Checklist：\n\n- [ ] 確認 GPU 可用：`torch.cuda.is_available()`\n- [ ] 建立標準 device：`device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")`\n- [ ] 模型和資料都移到同一個 device\n- [ ] 訓練前清零梯度：`optimizer.zero_grad()`\n- [ ] 推理時停止追蹤梯度：`with torch.no_grad()`\n- [ ] 大型模型考慮使用混合精度訓練\n- [ ] 定期保存 checkpoint\n\n### 下一步：Module 1 - 深度學習需要的數學基礎\n\n我們將學習：\n- 線性代數複習（矩陣運算、特徵值）\n- 微積分基礎（偏導數、鏈鎖法則）\n- 機率與統計（分佈、最大似然估計）\n- 資訊理論（熵、KL散度、交叉熵）",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}