{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch åŸºç¤Žå¯¦æˆ°\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "- æŽŒæ¡ PyTorch Tensor æ“ä½œ\n",
    "- ç†è§£è‡ªå‹•å¾®åˆ† (Autograd) æ©Ÿåˆ¶\n",
    "- å­¸æœƒå»ºç«‹è‡ªå®šç¾© Dataset å’Œ DataLoader\n",
    "- ç†Ÿæ‚‰ nn.Module æ¨¡åž‹å»ºæ§‹\n",
    "\n",
    "## åƒè€ƒè³‡æº\n",
    "- [æŽå®æ¯… ML 2021](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php)\n",
    "- [PyTorch å®˜æ–¹æ•™ç¨‹](https://pytorch.org/tutorials/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# æª¢æŸ¥ PyTorch ç‰ˆæœ¬å’Œ CUDA\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Tensor åŸºç¤Žæ“ä½œ\n",
    "\n",
    "Tensor æ˜¯ PyTorch çš„æ ¸å¿ƒè³‡æ–™çµæ§‹ï¼Œé¡žä¼¼ NumPy çš„ ndarrayï¼Œä½†å¯ä»¥åœ¨ GPU ä¸Šé‹ç®—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Tensor å»ºç«‹ ==========\n",
    "\n",
    "# å¾ž Python list å»ºç«‹\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "print(f\"å¾ž list å»ºç«‹: {x}\")\n",
    "\n",
    "# å¾ž NumPy å»ºç«‹\n",
    "np_array = np.array([[1, 2], [3, 4]])\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(f\"å¾ž NumPy å»ºç«‹:\\n{x_np}\")\n",
    "\n",
    "# ç‰¹æ®Š Tensor\n",
    "zeros = torch.zeros(2, 3)\n",
    "ones = torch.ones(2, 3)\n",
    "rand = torch.rand(2, 3)  # [0, 1) å‡å‹»åˆ†å¸ƒ\n",
    "randn = torch.randn(2, 3)  # æ¨™æº–æ­£æ…‹åˆ†å¸ƒ\n",
    "\n",
    "print(f\"\\néš¨æ©Ÿ Tensor:\\n{randn}\")\n",
    "\n",
    "# æŒ‡å®šè³‡æ–™é¡žåž‹å’Œè¨­å‚™\n",
    "x_float = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "x_gpu = torch.rand(2, 3).to(device)\n",
    "print(f\"\\nGPU Tensor è¨­å‚™: {x_gpu.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Tensor å±¬æ€§ ==========\n",
    "\n",
    "x = torch.randn(3, 4, 5)\n",
    "\n",
    "print(f\"å½¢ç‹€ (shape): {x.shape}\")\n",
    "print(f\"ç¶­åº¦æ•¸ (dim): {x.dim()}\")\n",
    "print(f\"è³‡æ–™é¡žåž‹ (dtype): {x.dtype}\")\n",
    "print(f\"è¨­å‚™ (device): {x.device}\")\n",
    "print(f\"å…ƒç´ æ•¸é‡: {x.numel()}\")\n",
    "print(f\"æ˜¯å¦éœ€è¦æ¢¯åº¦: {x.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Tensor é‹ç®— ==========\n",
    "\n",
    "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "# å…ƒç´ é‹ç®—\n",
    "print(f\"åŠ æ³•: \\n{a + b}\")\n",
    "print(f\"ä¹˜æ³• (å…ƒç´ ): \\n{a * b}\")\n",
    "print(f\"çŸ©é™£ä¹˜æ³•: \\n{a @ b}\")\n",
    "print(f\"çŸ©é™£ä¹˜æ³• (ç­‰åƒ¹): \\n{torch.matmul(a, b)}\")\n",
    "\n",
    "# èšåˆé‹ç®—\n",
    "print(f\"\\nç¸½å’Œ: {a.sum()}\")\n",
    "print(f\"å¹³å‡: {a.mean()}\")\n",
    "print(f\"æœ€å¤§å€¼: {a.max()}\")\n",
    "print(f\"æ²¿è»¸æ±‚å’Œ: {a.sum(dim=0)}\")\n",
    "print(f\"æ²¿è»¸æœ€å¤§: {a.max(dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Tensor å½¢ç‹€æ“ä½œ ==========\n",
    "\n",
    "x = torch.arange(12)  # [0, 1, 2, ..., 11]\n",
    "print(f\"åŽŸå§‹: {x.shape}\")\n",
    "\n",
    "# reshape / view\n",
    "x_reshaped = x.view(3, 4)\n",
    "print(f\"reshape ç‚º (3,4):\\n{x_reshaped}\")\n",
    "\n",
    "# è‡ªå‹•æŽ¨æ–·ç¶­åº¦\n",
    "x_auto = x.view(2, -1)  # -1 è‡ªå‹•è¨ˆç®—\n",
    "print(f\"reshape ç‚º (2,-1): {x_auto.shape}\")\n",
    "\n",
    "# è½‰ç½®\n",
    "x_t = x_reshaped.T\n",
    "print(f\"è½‰ç½®: {x_t.shape}\")\n",
    "\n",
    "# å¢žåŠ /ç§»é™¤ç¶­åº¦\n",
    "x_unsq = x.unsqueeze(0)  # [12] -> [1, 12]\n",
    "x_sq = x_unsq.squeeze(0)  # [1, 12] -> [12]\n",
    "print(f\"unsqueeze: {x_unsq.shape}\")\n",
    "print(f\"squeeze: {x_sq.shape}\")\n",
    "\n",
    "# å±•å¹³\n",
    "x_flat = x_reshaped.flatten()\n",
    "print(f\"flatten: {x_flat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ç´¢å¼•èˆ‡åˆ‡ç‰‡ ==========\n",
    "\n",
    "x = torch.arange(20).view(4, 5)\n",
    "print(f\"åŽŸå§‹:\\n{x}\")\n",
    "\n",
    "# åŸºæœ¬ç´¢å¼•\n",
    "print(f\"x[0]: {x[0]}\")\n",
    "print(f\"x[0, 0]: {x[0, 0]}\")\n",
    "print(f\"x[:, 0]: {x[:, 0]}\")\n",
    "print(f\"x[1:3, 2:4]:\\n{x[1:3, 2:4]}\")\n",
    "\n",
    "# å¸ƒæž—ç´¢å¼•\n",
    "mask = x > 10\n",
    "print(f\"x > 10:\\n{mask}\")\n",
    "print(f\"x[x > 10]: {x[mask]}\")\n",
    "\n",
    "# é€²éšŽç´¢å¼•\n",
    "indices = torch.tensor([0, 2, 3])\n",
    "print(f\"x[indices]:\\n{x[indices]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: è‡ªå‹•å¾®åˆ† (Autograd)\n",
    "\n",
    "PyTorch çš„è‡ªå‹•å¾®åˆ†ç³»çµ±è®“æˆ‘å€‘å¯ä»¥è‡ªå‹•è¨ˆç®—æ¢¯åº¦ï¼Œé€™æ˜¯è¨“ç·´ç¥žç¶“ç¶²è·¯çš„æ ¸å¿ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== åŸºæœ¬ Autograd ==========\n",
    "\n",
    "# å»ºç«‹éœ€è¦æ¢¯åº¦çš„ tensor\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "print(f\"x: {x}\")\n",
    "print(f\"x.requires_grad: {x.requires_grad}\")\n",
    "\n",
    "# å‰å‘è¨ˆç®—\n",
    "y = x ** 2 + 3 * x + 1\n",
    "print(f\"y = xÂ² + 3x + 1: {y}\")\n",
    "\n",
    "# è¨ˆç®—ç¸½å’Œä»¥å¾—åˆ°ç´”é‡\n",
    "loss = y.sum()\n",
    "print(f\"loss = sum(y): {loss}\")\n",
    "\n",
    "# åå‘å‚³æ’­\n",
    "loss.backward()\n",
    "\n",
    "# dy/dx = 2x + 3\n",
    "print(f\"x.grad (âˆ‚loss/âˆ‚x): {x.grad}\")\n",
    "print(f\"é©—è­‰: 2*x + 3 = {2*x.detach() + 3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è¨ˆç®—åœ–èˆ‡æ¢¯åº¦ç´¯ç© ==========\n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# å¤šæ¬¡åå‘å‚³æ’­æœƒç´¯ç©æ¢¯åº¦\n",
    "for i in range(3):\n",
    "    y = x * 2\n",
    "    y.backward()\n",
    "    print(f\"ç¬¬ {i+1} æ¬¡: x.grad = {x.grad}\")\n",
    "\n",
    "print(\"\\næ¢¯åº¦æœƒç´¯ç©ï¼è¨“ç·´æ™‚è¨˜å¾—æ¸…é›¶ã€‚\")\n",
    "\n",
    "# æ­£ç¢ºåšæ³•ï¼šæ¸…é›¶æ¢¯åº¦\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "for i in range(3):\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()  # æ¸…é›¶æ¢¯åº¦\n",
    "    y = x * 2\n",
    "    y.backward()\n",
    "    print(f\"æ¸…é›¶å¾Œç¬¬ {i+1} æ¬¡: x.grad = {x.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== åœæ­¢æ¢¯åº¦è¿½è¹¤ ==========\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "# æ–¹æ³• 1: with torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "    print(f\"no_grad å…§: y.requires_grad = {y.requires_grad}\")\n",
    "\n",
    "# æ–¹æ³• 2: .detach()\n",
    "y = x.detach() * 2\n",
    "print(f\"detach: y.requires_grad = {y.requires_grad}\")\n",
    "\n",
    "# æ–¹æ³• 3: @torch.no_grad() è£é£¾å™¨\n",
    "@torch.no_grad()\n",
    "def inference(x):\n",
    "    return x * 2\n",
    "\n",
    "y = inference(x)\n",
    "print(f\"è£é£¾å™¨: y.requires_grad = {y.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Dataset èˆ‡ DataLoader\n",
    "\n",
    "PyTorch æä¾›äº† Dataset å’Œ DataLoader ä¾†è™•ç†è³‡æ–™è¼‰å…¥å’Œæ‰¹æ¬¡è™•ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è‡ªå®šç¾© Dataset ==========\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    è‡ªå®šç¾© Dataset éœ€è¦å¯¦ç¾ä¸‰å€‹æ–¹æ³•ï¼š\n",
    "    - __init__: åˆå§‹åŒ–\n",
    "    - __len__: è¿”å›žè³‡æ–™é›†å¤§å°\n",
    "    - __getitem__: æ ¹æ“šç´¢å¼•è¿”å›žæ¨£æœ¬\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# å»ºç«‹ç¯„ä¾‹è³‡æ–™\n",
    "X = torch.randn(100, 10)  # 100 å€‹æ¨£æœ¬ï¼Œæ¯å€‹ 10 ç¶­\n",
    "y = torch.randint(0, 2, (100,))  # äºŒåˆ†é¡žæ¨™ç±¤\n",
    "\n",
    "dataset = CustomDataset(X, y)\n",
    "print(f\"è³‡æ–™é›†å¤§å°: {len(dataset)}\")\n",
    "print(f\"ç¬¬ä¸€å€‹æ¨£æœ¬: {dataset[0][0].shape}, æ¨™ç±¤: {dataset[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DataLoader ==========\n",
    "\n",
    "# å»ºç«‹ DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,       # æ‰¹æ¬¡å¤§å°\n",
    "    shuffle=True,        # æ˜¯å¦æ‰“äº‚\n",
    "    num_workers=0,       # ä¸¦è¡Œè¼‰å…¥çš„é€²ç¨‹æ•¸\n",
    "    drop_last=True,      # ä¸Ÿæ£„æœ€å¾Œä¸å®Œæ•´çš„æ‰¹æ¬¡\n",
    ")\n",
    "\n",
    "print(f\"æ‰¹æ¬¡æ•¸: {len(dataloader)}\")\n",
    "\n",
    "# è¿­ä»£ DataLoader\n",
    "for batch_idx, (batch_x, batch_y) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx}: X shape = {batch_x.shape}, y shape = {batch_y.shape}\")\n",
    "    if batch_idx >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== å¯¦ç”¨æŠ€å·§ï¼šè¨“ç·´/é©—è­‰åˆ†å‰² ==========\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# åˆ†å‰²è³‡æ–™é›†\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"è¨“ç·´é›†: {len(train_dataset)}\")\n",
    "print(f\"é©—è­‰é›†: {len(val_dataset)}\")\n",
    "\n",
    "# åˆ†åˆ¥å»ºç«‹ DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: nn.Module æ¨¡åž‹å»ºæ§‹\n",
    "\n",
    "æ‰€æœ‰ PyTorch æ¨¡åž‹éƒ½ç¹¼æ‰¿è‡ª nn.Moduleã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== åŸºæœ¬æ¨¡åž‹å®šç¾© ==========\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    \"\"\"\n",
    "    ç°¡å–®çš„å‰é¥‹ç¥žç¶“ç¶²è·¯\n",
    "    \n",
    "    å¿…é ˆå¯¦ç¾:\n",
    "    - __init__: å®šç¾©å±¤\n",
    "    - forward: å®šç¾©å‰å‘å‚³æ’­\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()  # å¿…é ˆèª¿ç”¨çˆ¶é¡žåˆå§‹åŒ–\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# å»ºç«‹æ¨¡åž‹\n",
    "model = SimpleNN(input_dim=10, hidden_dim=64, output_dim=2)\n",
    "print(model)\n",
    "\n",
    "# æ¸¬è©¦å‰å‘å‚³æ’­\n",
    "x = torch.randn(4, 10)\n",
    "output = model(x)\n",
    "print(f\"\\nè¼¸å…¥: {x.shape}\")\n",
    "print(f\"è¼¸å‡º: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ä½¿ç”¨ nn.Sequential ==========\n",
    "\n",
    "# å¿«é€Ÿå»ºç«‹ç°¡å–®æ¨¡åž‹\n",
    "model_seq = nn.Sequential(\n",
    "    nn.Linear(10, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(64, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(64, 2),\n",
    ")\n",
    "\n",
    "print(\"Sequential æ¨¡åž‹:\")\n",
    "print(model_seq)\n",
    "\n",
    "# æ¸¬è©¦\n",
    "output = model_seq(x)\n",
    "print(f\"\\nè¼¸å‡º: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ¨¡åž‹åƒæ•¸ ==========\n",
    "\n",
    "# æŸ¥çœ‹æ‰€æœ‰åƒæ•¸\n",
    "print(\"æ¨¡åž‹åƒæ•¸:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}\")\n",
    "\n",
    "# è¨ˆç®—ç¸½åƒæ•¸é‡\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nç¸½åƒæ•¸: {total_params:,}\")\n",
    "print(f\"å¯è¨“ç·´åƒæ•¸: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ¨¡åž‹ç‹€æ…‹ ==========\n",
    "\n",
    "# è¨“ç·´æ¨¡å¼ vs è©•ä¼°æ¨¡å¼\n",
    "model.train()  # å•Ÿç”¨ Dropout, BatchNorm ä½¿ç”¨æ‰¹æ¬¡çµ±è¨ˆ\n",
    "print(f\"è¨“ç·´æ¨¡å¼: model.training = {model.training}\")\n",
    "\n",
    "model.eval()   # åœç”¨ Dropout, BatchNorm ä½¿ç”¨ç´¯ç©çµ±è¨ˆ\n",
    "print(f\"è©•ä¼°æ¨¡å¼: model.training = {model.training}\")\n",
    "\n",
    "# ç§»å‹•æ¨¡åž‹åˆ° GPU\n",
    "model = model.to(device)\n",
    "print(f\"\\næ¨¡åž‹è¨­å‚™: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: å®Œæ•´è¨“ç·´æµç¨‹\n",
    "\n",
    "æŠŠæ‰€æœ‰å…ƒç´ çµ„åˆæˆå®Œæ•´çš„è¨“ç·´æµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== å®Œæ•´è¨“ç·´ç¯„ä¾‹ ==========\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=10, lr=1e-3):\n",
    "    \"\"\"\n",
    "    æ¨™æº– PyTorch è¨“ç·´æµç¨‹\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # å„ªåŒ–å™¨\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # æå¤±å‡½æ•¸\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # å­¸ç¿’çŽ‡èª¿åº¦å™¨\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # ========== è¨“ç·´éšŽæ®µ ==========\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # 1. æ¸…é›¶æ¢¯åº¦\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 2. å‰å‘å‚³æ’­\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # 3. åå‘å‚³æ’­\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. æ›´æ–°åƒæ•¸\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # ========== é©—è­‰éšŽæ®µ ==========\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():  # é—œé–‰æ¢¯åº¦è¨ˆç®—\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += predicted.eq(batch_y).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = correct / total\n",
    "        \n",
    "        # æ›´æ–°å­¸ç¿’çŽ‡\n",
    "        scheduler.step()\n",
    "        \n",
    "        # è¨˜éŒ„æ­·å²\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        print(f\"  LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# è¨“ç·´\n",
    "model = SimpleNN(input_dim=10, hidden_dim=64, output_dim=2)\n",
    "history = train_model(model, train_loader, val_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ç¹ªè£½è¨“ç·´æ›²ç·š ==========\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='Train')\n",
    "axes[0].plot(history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Curve')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history['val_acc'])\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: æ¨¡åž‹ä¿å­˜èˆ‡è¼‰å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ä¿å­˜èˆ‡è¼‰å…¥ ==========\n",
    "\n",
    "# æ–¹æ³• 1: åªä¿å­˜æ¬Šé‡ï¼ˆæŽ¨è–¦ï¼‰\n",
    "torch.save(model.state_dict(), 'model_weights.pt')\n",
    "\n",
    "# è¼‰å…¥æ¬Šé‡\n",
    "new_model = SimpleNN(input_dim=10, hidden_dim=64, output_dim=2)\n",
    "new_model.load_state_dict(torch.load('model_weights.pt'))\n",
    "print(\"æ¬Šé‡è¼‰å…¥æˆåŠŸï¼\")\n",
    "\n",
    "# æ–¹æ³• 2: ä¿å­˜å®Œæ•´æª¢æŸ¥é»žï¼ˆåŒ…å«å„ªåŒ–å™¨ç‹€æ…‹ï¼‰\n",
    "checkpoint = {\n",
    "    'epoch': 10,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict() if 'optimizer' in dir() else None,\n",
    "    'loss': history['train_loss'][-1] if history['train_loss'] else None,\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint.pt')\n",
    "print(\"æª¢æŸ¥é»žä¿å­˜æˆåŠŸï¼\")\n",
    "\n",
    "# æ¸…ç†\n",
    "import os\n",
    "os.remove('model_weights.pt')\n",
    "os.remove('checkpoint.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ¯ ç¸½çµ\n",
    "\n",
    "### PyTorch æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "| æ¦‚å¿µ | èªªæ˜Ž |\n",
    "|------|------|\n",
    "| **Tensor** | å¤šç¶­æ•¸çµ„ï¼Œæ”¯æŒ GPU åŠ é€Ÿ |\n",
    "| **Autograd** | è‡ªå‹•å¾®åˆ†ï¼Œè¿½è¹¤è¨ˆç®—åœ– |\n",
    "| **Dataset** | å°è£è³‡æ–™ï¼Œæä¾›ç´¢å¼•è¨ªå• |\n",
    "| **DataLoader** | æ‰¹æ¬¡è¼‰å…¥ï¼Œæ”¯æŒå¤šé€²ç¨‹ |\n",
    "| **nn.Module** | æ¨¡åž‹åŸºé¡žï¼Œç®¡ç†åƒæ•¸å’Œå‰å‘å‚³æ’­ |\n",
    "\n",
    "### è¨“ç·´æµç¨‹ Checklist\n",
    "\n",
    "```\n",
    "1. æº–å‚™è³‡æ–™\n",
    "   â–¡ å»ºç«‹ Dataset\n",
    "   â–¡ å»ºç«‹ DataLoader\n",
    "   â–¡ åˆ†å‰²è¨“ç·´/é©—è­‰é›†\n",
    "\n",
    "2. å»ºç«‹æ¨¡åž‹\n",
    "   â–¡ å®šç¾©ç¶²è·¯æž¶æ§‹\n",
    "   â–¡ ç§»å‹•åˆ° GPU\n",
    "\n",
    "3. è¨­å®šè¨“ç·´\n",
    "   â–¡ é¸æ“‡å„ªåŒ–å™¨\n",
    "   â–¡ é¸æ“‡æå¤±å‡½æ•¸\n",
    "   â–¡ è¨­å®šå­¸ç¿’çŽ‡èª¿åº¦\n",
    "\n",
    "4. è¨“ç·´å¾ªç’°\n",
    "   â–¡ æ¸…é›¶æ¢¯åº¦ optimizer.zero_grad()\n",
    "   â–¡ å‰å‘å‚³æ’­ outputs = model(inputs)\n",
    "   â–¡ è¨ˆç®—æå¤± loss = criterion(outputs, labels)\n",
    "   â–¡ åå‘å‚³æ’­ loss.backward()\n",
    "   â–¡ æ›´æ–°åƒæ•¸ optimizer.step()\n",
    "\n",
    "5. é©—è­‰èˆ‡ä¿å­˜\n",
    "   â–¡ model.eval() + torch.no_grad()\n",
    "   â–¡ ä¿å­˜æœ€ä½³æ¨¡åž‹\n",
    "```\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å‰å¾€ `supervised_learning/regression.ipynb` å­¸ç¿’å¦‚ä½•ç”¨ PyTorch å¯¦ä½œè¿´æ­¸ä»»å‹™ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## ç·´ç¿’é¡Œ\n\n### ç·´ç¿’ 1: å¯¦ä½œä¸€å€‹ç°¡å–®çš„ç·šæ€§å›žæ­¸\n\n**ç›®æ¨™**: ä½¿ç”¨ PyTorch å¾žé›¶å¯¦ä½œç·šæ€§å›žæ­¸ y = 2x + 3",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ç·´ç¿’ 1: ç·šæ€§å›žæ­¸\n\n# ç”Ÿæˆè³‡æ–™: y = 2x + 3 + noise\ntorch.manual_seed(42)\nX = torch.linspace(-5, 5, 100).unsqueeze(1)  # [100, 1]\ny_true = 2 * X + 3 + torch.randn_like(X) * 0.5\n\n# å®šç¾©æ¨¡åž‹åƒæ•¸\nw = torch.randn(1, requires_grad=True)\nb = torch.zeros(1, requires_grad=True)\n\n# è¨“ç·´\nlr = 0.01\nepochs = 100\nlosses = []\n\nfor epoch in range(epochs):\n    # å‰å‘å‚³æ’­\n    y_pred = X * w + b\n    loss = ((y_pred - y_true) ** 2).mean()  # MSE\n    \n    # åå‘å‚³æ’­\n    loss.backward()\n    \n    # æ›´æ–°åƒæ•¸ï¼ˆæ‰‹å‹•æ¢¯åº¦ä¸‹é™ï¼‰\n    with torch.no_grad():\n        w -= lr * w.grad\n        b -= lr * b.grad\n        \n        # æ¸…é›¶æ¢¯åº¦\n        w.grad.zero_()\n        b.grad.zero_()\n    \n    losses.append(loss.item())\n    \n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}, w = {w.item():.4f}, b = {b.item():.4f}\")\n\nprint(f\"\\nå­¸åˆ°çš„åƒæ•¸: y = {w.item():.2f}x + {b.item():.2f}\")\nprint(f\"çœŸå¯¦åƒæ•¸: y = 2x + 3\")\n\n# è¦–è¦ºåŒ–\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].scatter(X.numpy(), y_true.numpy(), alpha=0.5, label='Data')\naxes[0].plot(X.numpy(), (X * w + b).detach().numpy(), 'r-', linewidth=2, label='Fitted')\naxes[0].set_xlabel('x')\naxes[0].set_ylabel('y')\naxes[0].set_title('Linear Regression')\naxes[0].legend()\n\naxes[1].plot(losses)\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Loss')\naxes[1].set_title('Training Loss')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### ç·´ç¿’ 2: å¯¦ä½œè‡ªå®šç¾© Dataset è™•ç†åœ–åƒè³‡æ–™\n\n**ç›®æ¨™**: å»ºç«‹ä¸€å€‹è™•ç†åˆæˆåœ–åƒçš„ Dataset",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ç·´ç¿’ 2: è‡ªå®šç¾©åœ–åƒ Dataset\n\nclass SyntheticImageDataset(Dataset):\n    \"\"\"\n    åˆæˆåœ–åƒè³‡æ–™é›†\n    - é¡žåˆ¥ 0: å¸¶æœ‰æ°´å¹³æ¢ç´‹çš„åœ–åƒ\n    - é¡žåˆ¥ 1: å¸¶æœ‰åž‚ç›´æ¢ç´‹çš„åœ–åƒ\n    \"\"\"\n    \n    def __init__(self, num_samples=1000, img_size=28, transform=None):\n        self.num_samples = num_samples\n        self.img_size = img_size\n        self.transform = transform\n        \n        # é ç”Ÿæˆè³‡æ–™\n        self.images = []\n        self.labels = []\n        \n        for i in range(num_samples):\n            if i % 2 == 0:\n                # æ°´å¹³æ¢ç´‹\n                img = self._create_horizontal_stripes()\n                label = 0\n            else:\n                # åž‚ç›´æ¢ç´‹\n                img = self._create_vertical_stripes()\n                label = 1\n            \n            self.images.append(img)\n            self.labels.append(label)\n    \n    def _create_horizontal_stripes(self):\n        img = torch.zeros(1, self.img_size, self.img_size)\n        stripe_width = np.random.randint(2, 5)\n        for i in range(0, self.img_size, stripe_width * 2):\n            img[:, i:i+stripe_width, :] = 1.0\n        img += torch.randn_like(img) * 0.1\n        return img.clamp(0, 1)\n    \n    def _create_vertical_stripes(self):\n        img = torch.zeros(1, self.img_size, self.img_size)\n        stripe_width = np.random.randint(2, 5)\n        for i in range(0, self.img_size, stripe_width * 2):\n            img[:, :, i:i+stripe_width] = 1.0\n        img += torch.randn_like(img) * 0.1\n        return img.clamp(0, 1)\n    \n    def __len__(self):\n        return self.num_samples\n    \n    def __getitem__(self, idx):\n        img = self.images[idx]\n        label = self.labels[idx]\n        \n        if self.transform:\n            img = self.transform(img)\n        \n        return img, label\n\n# å»ºç«‹è³‡æ–™é›†\ndataset = SyntheticImageDataset(num_samples=500)\nprint(f\"è³‡æ–™é›†å¤§å°: {len(dataset)}\")\n\n# è¦–è¦ºåŒ–æ¨£æœ¬\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfor i in range(5):\n    img, label = dataset[i * 2]\n    axes[0, i].imshow(img.squeeze(), cmap='gray')\n    axes[0, i].set_title(f'Class {label} (Horizontal)')\n    axes[0, i].axis('off')\n    \n    img, label = dataset[i * 2 + 1]\n    axes[1, i].imshow(img.squeeze(), cmap='gray')\n    axes[1, i].set_title(f'Class {label} (Vertical)')\n    axes[1, i].axis('off')\n\nplt.suptitle('Synthetic Image Dataset')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### ç·´ç¿’ 3: ä½¿ç”¨è‡ªå®šç¾© Dataset è¨“ç·´ CNN\n\n**ç›®æ¨™**: ç”¨ç°¡å–®çš„ CNN åˆ†é¡žä¸Šé¢çš„æ¢ç´‹åœ–åƒ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ç·´ç¿’ 3: è¨“ç·´ CNN åˆ†é¡žæ¢ç´‹\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc = nn.Linear(32 * 7 * 7, 2)\n        \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))  # 28 -> 14\n        x = self.pool(F.relu(self.conv2(x)))  # 14 -> 7\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\n# æº–å‚™è³‡æ–™\ntrain_dataset = SyntheticImageDataset(num_samples=800)\ntest_dataset = SyntheticImageDataset(num_samples=200)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n# è¨“ç·´\nmodel = SimpleCNN().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\nepochs = 10\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    for imgs, labels in train_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    # æ¸¬è©¦\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for imgs, labels in test_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    acc = 100 * correct / total\n    print(f\"Epoch {epoch+1}/{epochs}: Loss = {total_loss/len(train_loader):.4f}, Test Acc = {acc:.1f}%\")\n\nprint(f\"\\næœ€çµ‚æ¸¬è©¦æº–ç¢ºçŽ‡: {acc:.1f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}