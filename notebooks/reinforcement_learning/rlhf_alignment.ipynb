{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLHF 與 LLM 對齊 (RLHF and LLM Alignment)\n",
    "\n",
    "本 notebook 對應李宏毅老師 2025 Spring ML HW7，深入探討如何使用人類反饋強化學習（RLHF）來對齊大型語言模型。\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 理解 LLM 對齊問題與 RLHF 的動機\n",
    "2. 掌握 Reward Model 的訓練方法\n",
    "3. 了解 PPO 在 LLM 上的應用\n",
    "4. 學習 DPO（Direct Preference Optimization）\n",
    "5. 使用 TRL 庫進行實作\n",
    "\n",
    "## 參考資源\n",
    "\n",
    "- [InstructGPT Paper](https://arxiv.org/abs/2203.02155) - OpenAI RLHF 原始論文\n",
    "- [DPO Paper](https://arxiv.org/abs/2305.18290) - Direct Preference Optimization\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl) - Transformer Reinforcement Learning\n",
    "- [2025 Spring HW7](https://speech.ee.ntu.edu.tw/~hylee/ml/2025-spring.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 為什麼需要對齊？(Why Alignment?)\n",
    "\n",
    "### 1.1 預訓練 LLM 的問題\n",
    "\n",
    "預訓練的 LLM 透過預測下一個 token 學習語言，但這不等於：\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────┐\n",
    "│                    預訓練 vs 使用者期望                           │\n",
    "├─────────────────────────────────────────────────────────────────┤\n",
    "│                                                                 │\n",
    "│  預訓練目標：P(next_token | context)                             │\n",
    "│  ↓                                                              │\n",
    "│  模型學到：                                                      │\n",
    "│  - 統計模式（網路文本的分布）                                      │\n",
    "│  - 各種風格（包含有害內容）                                        │\n",
    "│  - 事實與虛構混合                                                 │\n",
    "│                                                                 │\n",
    "│  使用者期望：                                                     │\n",
    "│  - 有幫助 (Helpful)                                             │\n",
    "│  - 無害 (Harmless)                                              │\n",
    "│  - 誠實 (Honest)                                                │\n",
    "│                                                                 │\n",
    "│  → 存在 \"對齊差距\" (Alignment Gap)                               │\n",
    "└─────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 1.2 對齊的目標\n",
    "\n",
    "**HHH 原則**（Anthropic 提出）：\n",
    "- **Helpful（有幫助）**：回答問題、完成任務\n",
    "- **Harmless（無害）**：不產生有害、歧視、危險內容\n",
    "- **Honest（誠實）**：承認不確定性、不編造事實"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RLHF 流程概覽\n",
    "\n",
    "### 2.1 三階段訓練流程\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                      RLHF 三階段流程 (InstructGPT)                       │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  Stage 1: Supervised Fine-Tuning (SFT)                                 │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐       │\n",
    "│  │  Human Demonstrations                                        │       │\n",
    "│  │  (prompt, ideal_response) pairs                             │       │\n",
    "│  │           ↓                                                  │       │\n",
    "│  │  Pretrained LLM  ──────→  SFT Model                         │       │\n",
    "│  │  (GPT-3)                  (follows instructions)            │       │\n",
    "│  └─────────────────────────────────────────────────────────────┘       │\n",
    "│                              │                                          │\n",
    "│                              ▼                                          │\n",
    "│  Stage 2: Reward Model Training                                        │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐       │\n",
    "│  │  SFT Model generates multiple responses per prompt          │       │\n",
    "│  │           ↓                                                  │       │\n",
    "│  │  Human ranks responses: A > B > C > D                       │       │\n",
    "│  │           ↓                                                  │       │\n",
    "│  │  Train Reward Model: r(prompt, response) → scalar           │       │\n",
    "│  └─────────────────────────────────────────────────────────────┘       │\n",
    "│                              │                                          │\n",
    "│                              ▼                                          │\n",
    "│  Stage 3: RL Fine-Tuning (PPO)                                         │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐       │\n",
    "│  │  Environment: Prompts from dataset                          │       │\n",
    "│  │  Policy: SFT Model (being optimized)                        │       │\n",
    "│  │  Reward: Reward Model output                                │       │\n",
    "│  │           ↓                                                  │       │\n",
    "│  │  Optimize: max E[r(x,y)] - β·KL(π||π_ref)                   │       │\n",
    "│  │           ↓                                                  │       │\n",
    "│  │  Final RLHF Model                                           │       │\n",
    "│  └─────────────────────────────────────────────────────────────┘       │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "# 設定隨機種子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reward Model 訓練\n",
    "\n",
    "### 3.1 偏好資料格式\n",
    "\n",
    "Reward Model 學習人類偏好，訓練資料格式：\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                   偏好資料 (Preference Data)                 │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  Prompt: \"What is the capital of France?\"                  │\n",
    "│                                                             │\n",
    "│  Response A (Chosen/Winner):                               │\n",
    "│  \"The capital of France is Paris. It is located in        │\n",
    "│   northern France and is known for...\"                     │\n",
    "│                                                             │\n",
    "│  Response B (Rejected/Loser):                              │\n",
    "│  \"France capital Paris maybe.\"                             │\n",
    "│                                                             │\n",
    "│  Human Preference: A > B                                   │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 3.2 Bradley-Terry Model\n",
    "\n",
    "Reward Model 使用 Bradley-Terry 模型來建模偏好：\n",
    "\n",
    "$$P(y_w \\succ y_l | x) = \\sigma(r(x, y_w) - r(x, y_l))$$\n",
    "\n",
    "其中 $\\sigma$ 是 sigmoid 函數，$r(x, y)$ 是 reward model 對 (prompt, response) 的評分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PreferenceExample:\n",
    "    \"\"\"偏好資料範例\"\"\"\n",
    "    prompt: str\n",
    "    chosen: str      # 人類偏好的回應\n",
    "    rejected: str    # 被拒絕的回應\n",
    "\n",
    "\n",
    "# 範例偏好資料\n",
    "preference_data = [\n",
    "    PreferenceExample(\n",
    "        prompt=\"Explain what machine learning is.\",\n",
    "        chosen=\"Machine learning is a subset of artificial intelligence that enables computers to learn patterns from data without being explicitly programmed. It involves algorithms that improve through experience.\",\n",
    "        rejected=\"ML is computers doing stuff.\"\n",
    "    ),\n",
    "    PreferenceExample(\n",
    "        prompt=\"How do I make coffee?\",\n",
    "        chosen=\"To make coffee: 1) Boil water, 2) Add 2 tablespoons of ground coffee per cup to your filter, 3) Pour hot water over the grounds, 4) Let it brew for 3-4 minutes, 5) Enjoy!\",\n",
    "        rejected=\"Just put coffee in water.\"\n",
    "    ),\n",
    "    PreferenceExample(\n",
    "        prompt=\"Is the Earth flat?\",\n",
    "        chosen=\"No, the Earth is not flat. Scientific evidence from multiple sources including satellite imagery, physics, and direct observation confirms that Earth is an oblate spheroid (slightly flattened sphere).\",\n",
    "        rejected=\"Some people say yes, some say no. Who knows?\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"偏好資料範例：\")\n",
    "for i, ex in enumerate(preference_data):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Prompt: {ex.prompt}\")\n",
    "    print(f\"Chosen: {ex.chosen[:50]}...\")\n",
    "    print(f\"Rejected: {ex.rejected[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRewardModel(nn.Module):\n",
    "    \"\"\"\n",
    "    簡化版 Reward Model\n",
    "    \n",
    "    實際的 Reward Model 是在 LLM 基礎上加一個 scalar head，\n",
    "    這裡使用簡化版本來展示概念。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # 簡單的 Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=4,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        # Reward head: 將序列表示映射到 scalar\n",
    "        self.reward_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: [batch_size, seq_len]\n",
    "            attention_mask: [batch_size, seq_len]\n",
    "        Returns:\n",
    "            rewards: [batch_size, 1]\n",
    "        \"\"\"\n",
    "        # Embedding\n",
    "        x = self.embedding(input_ids)  # [batch, seq, embed]\n",
    "        \n",
    "        # Transformer encoding\n",
    "        if attention_mask is not None:\n",
    "            # Convert to transformer mask format\n",
    "            src_key_padding_mask = ~attention_mask.bool()\n",
    "        else:\n",
    "            src_key_padding_mask = None\n",
    "        \n",
    "        x = self.transformer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # 取最後一個 token 的表示（或平均）\n",
    "        if attention_mask is not None:\n",
    "            # Masked mean pooling\n",
    "            mask_expanded = attention_mask.unsqueeze(-1).float()\n",
    "            sum_embeddings = (x * mask_expanded).sum(dim=1)\n",
    "            pooled = sum_embeddings / mask_expanded.sum(dim=1).clamp(min=1e-9)\n",
    "        else:\n",
    "            pooled = x.mean(dim=1)\n",
    "        \n",
    "        # 計算 reward\n",
    "        reward = self.reward_head(pooled)\n",
    "        return reward\n",
    "\n",
    "\n",
    "# 測試 Reward Model\n",
    "vocab_size = 1000\n",
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "\n",
    "reward_model = SimpleRewardModel(vocab_size, embed_dim, hidden_dim).to(device)\n",
    "\n",
    "# 模擬輸入\n",
    "batch_size = 4\n",
    "seq_len = 32\n",
    "dummy_input = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
    "dummy_mask = torch.ones(batch_size, seq_len).to(device)\n",
    "\n",
    "rewards = reward_model(dummy_input, dummy_mask)\n",
    "print(f\"Reward shape: {rewards.shape}\")\n",
    "print(f\"Sample rewards: {rewards.squeeze().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_model_loss(reward_chosen: torch.Tensor, \n",
    "                      reward_rejected: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Reward Model 的損失函數（Bradley-Terry loss）\n",
    "    \n",
    "    L = -log(σ(r_chosen - r_rejected))\n",
    "    \n",
    "    希望 chosen response 的 reward 高於 rejected response\n",
    "    \"\"\"\n",
    "    # 計算 reward 差異\n",
    "    reward_diff = reward_chosen - reward_rejected\n",
    "    \n",
    "    # Binary cross entropy: -log(sigmoid(diff))\n",
    "    loss = -F.logsigmoid(reward_diff).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def compute_reward_accuracy(reward_chosen: torch.Tensor, \n",
    "                            reward_rejected: torch.Tensor) -> float:\n",
    "    \"\"\"\n",
    "    計算 Reward Model 的準確率\n",
    "    正確 = r(chosen) > r(rejected)\n",
    "    \"\"\"\n",
    "    correct = (reward_chosen > reward_rejected).float().mean()\n",
    "    return correct.item()\n",
    "\n",
    "\n",
    "# 示範損失計算\n",
    "r_chosen = torch.tensor([[2.5], [1.8], [3.2]])\n",
    "r_rejected = torch.tensor([[1.2], [2.1], [0.8]])\n",
    "\n",
    "loss = reward_model_loss(r_chosen, r_rejected)\n",
    "acc = compute_reward_accuracy(r_chosen, r_rejected)\n",
    "\n",
    "print(f\"Reward differences: {(r_chosen - r_rejected).squeeze().tolist()}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(f\"Accuracy: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PPO 用於 LLM 對齊\n",
    "\n",
    "### 4.1 PPO 回顧\n",
    "\n",
    "PPO (Proximal Policy Optimization) 是一種 policy gradient 方法，在 RLHF 中：\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                    PPO for LLM 對齊                                  │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│  State (s):    Prompt x + generated tokens so far                  │\n",
    "│  Action (a):   Next token to generate                              │\n",
    "│  Policy π(a|s): LLM probability distribution over vocabulary        │\n",
    "│  Reward (r):   Reward Model output (only at end of response)       │\n",
    "│                                                                     │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐   │\n",
    "│  │                                                             │   │\n",
    "│  │  Prompt    Policy (LLM)    Response    Reward Model         │   │\n",
    "│  │    x    →     π_θ      →     y     →      r(x,y)           │   │\n",
    "│  │                                              ↓              │   │\n",
    "│  │                                          Scalar             │   │\n",
    "│  │                                          Reward             │   │\n",
    "│  │                                                             │   │\n",
    "│  └─────────────────────────────────────────────────────────────┘   │\n",
    "│                                                                     │\n",
    "│  PPO Objective:                                                    │\n",
    "│  L_PPO = E[min(r_t(θ)·Â_t, clip(r_t(θ), 1-ε, 1+ε)·Â_t)]          │\n",
    "│                                                                     │\n",
    "│  where r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t)                   │\n",
    "│                                                                     │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 4.2 KL Penalty\n",
    "\n",
    "為了防止模型偏離太遠，加入 KL divergence 懲罰：\n",
    "\n",
    "$$\\text{Objective} = \\mathbb{E}[r(x, y)] - \\beta \\cdot KL(\\pi_\\theta \\| \\pi_{ref})$$\n",
    "\n",
    "其中 $\\pi_{ref}$ 是原始 SFT model。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePPOTrainer:\n",
    "    \"\"\"\n",
    "    簡化版 PPO Trainer for LLM\n",
    "    展示核心概念，實際使用請參考 TRL 庫\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy_model: nn.Module,\n",
    "        ref_model: nn.Module,\n",
    "        reward_model: nn.Module,\n",
    "        kl_coef: float = 0.1,\n",
    "        clip_range: float = 0.2,\n",
    "        value_clip_range: float = 0.2,\n",
    "    ):\n",
    "        self.policy = policy_model\n",
    "        self.ref_model = ref_model\n",
    "        self.reward_model = reward_model\n",
    "        self.kl_coef = kl_coef\n",
    "        self.clip_range = clip_range\n",
    "        self.value_clip_range = value_clip_range\n",
    "        \n",
    "        # Freeze reference model\n",
    "        for param in self.ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.reward_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def compute_kl_divergence(self, policy_logprobs: torch.Tensor, \n",
    "                               ref_logprobs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        計算 KL divergence: KL(π_θ || π_ref)\n",
    "        使用 log probabilities 的近似\n",
    "        \"\"\"\n",
    "        kl = policy_logprobs - ref_logprobs\n",
    "        return kl.mean()\n",
    "    \n",
    "    def compute_advantages(self, rewards: torch.Tensor, \n",
    "                          values: torch.Tensor,\n",
    "                          gamma: float = 1.0,\n",
    "                          lam: float = 0.95) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        計算 GAE (Generalized Advantage Estimation)\n",
    "        \n",
    "        對於 LLM，通常只在 response 結束時給一個 reward，\n",
    "        所以這裡簡化處理\n",
    "        \"\"\"\n",
    "        # 簡化版：advantage = reward - value (baseline)\n",
    "        advantages = rewards - values\n",
    "        returns = rewards\n",
    "        return advantages, returns\n",
    "    \n",
    "    def ppo_loss(self, \n",
    "                 old_logprobs: torch.Tensor,\n",
    "                 new_logprobs: torch.Tensor,\n",
    "                 advantages: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        PPO Clipped Objective\n",
    "        \"\"\"\n",
    "        # Probability ratio\n",
    "        ratio = torch.exp(new_logprobs - old_logprobs)\n",
    "        \n",
    "        # Clipped ratio\n",
    "        clipped_ratio = torch.clamp(ratio, \n",
    "                                     1 - self.clip_range, \n",
    "                                     1 + self.clip_range)\n",
    "        \n",
    "        # PPO loss: negative because we want to maximize\n",
    "        loss1 = ratio * advantages\n",
    "        loss2 = clipped_ratio * advantages\n",
    "        loss = -torch.min(loss1, loss2).mean()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "print(\"PPO Trainer 類別已定義\")\n",
    "print(\"\\n核心組件：\")\n",
    "print(\"1. Policy Model - 要優化的 LLM\")\n",
    "print(\"2. Reference Model - 原始 SFT Model（凍結）\")\n",
    "print(\"3. Reward Model - 評估回應品質\")\n",
    "print(\"4. KL Penalty - 防止偏離太遠\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 PPO Clipping\n",
    "def visualize_ppo_clipping(clip_range=0.2):\n",
    "    \"\"\"視覺化 PPO clipping 機制\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Ratio values\n",
    "    ratios = np.linspace(0.5, 1.5, 200)\n",
    "    \n",
    "    # Case 1: Positive advantage (want to increase probability)\n",
    "    advantage_pos = 1.0\n",
    "    \n",
    "    unclipped_pos = ratios * advantage_pos\n",
    "    clipped_ratios = np.clip(ratios, 1 - clip_range, 1 + clip_range)\n",
    "    clipped_pos = clipped_ratios * advantage_pos\n",
    "    ppo_objective_pos = np.minimum(unclipped_pos, clipped_pos)\n",
    "    \n",
    "    axes[0].plot(ratios, unclipped_pos, 'b--', label='Unclipped', alpha=0.7)\n",
    "    axes[0].plot(ratios, clipped_pos, 'r--', label='Clipped', alpha=0.7)\n",
    "    axes[0].plot(ratios, ppo_objective_pos, 'g-', label='PPO Objective', linewidth=2)\n",
    "    axes[0].axvline(x=1, color='gray', linestyle=':', alpha=0.5)\n",
    "    axes[0].axvline(x=1-clip_range, color='orange', linestyle='--', alpha=0.5, label=f'Clip bounds ({1-clip_range}, {1+clip_range})')\n",
    "    axes[0].axvline(x=1+clip_range, color='orange', linestyle='--', alpha=0.5)\n",
    "    axes[0].set_xlabel('Probability Ratio (π_new / π_old)')\n",
    "    axes[0].set_ylabel('Objective')\n",
    "    axes[0].set_title('Positive Advantage (A > 0)\\n\"Good action, want higher probability\"')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Case 2: Negative advantage (want to decrease probability)\n",
    "    advantage_neg = -1.0\n",
    "    \n",
    "    unclipped_neg = ratios * advantage_neg\n",
    "    clipped_neg = clipped_ratios * advantage_neg\n",
    "    ppo_objective_neg = np.minimum(unclipped_neg, clipped_neg)\n",
    "    \n",
    "    axes[1].plot(ratios, unclipped_neg, 'b--', label='Unclipped', alpha=0.7)\n",
    "    axes[1].plot(ratios, clipped_neg, 'r--', label='Clipped', alpha=0.7)\n",
    "    axes[1].plot(ratios, ppo_objective_neg, 'g-', label='PPO Objective', linewidth=2)\n",
    "    axes[1].axvline(x=1, color='gray', linestyle=':', alpha=0.5)\n",
    "    axes[1].axvline(x=1-clip_range, color='orange', linestyle='--', alpha=0.5)\n",
    "    axes[1].axvline(x=1+clip_range, color='orange', linestyle='--', alpha=0.5)\n",
    "    axes[1].set_xlabel('Probability Ratio (π_new / π_old)')\n",
    "    axes[1].set_ylabel('Objective')\n",
    "    axes[1].set_title('Negative Advantage (A < 0)\\n\"Bad action, want lower probability\"')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPPO Clipping 說明：\")\n",
    "    print(\"- 當 A > 0（好動作）：限制 ratio 增加的幅度，防止過度更新\")\n",
    "    print(\"- 當 A < 0（壞動作）：限制 ratio 減少的幅度，保持穩定\")\n",
    "    print(\"- 這確保每次更新都是 'proximal'（接近的）\")\n",
    "\n",
    "visualize_ppo_clipping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DPO: Direct Preference Optimization\n",
    "\n",
    "### 5.1 DPO 的動機\n",
    "\n",
    "RLHF 的問題：\n",
    "- 需要訓練獨立的 Reward Model\n",
    "- PPO 訓練複雜且不穩定\n",
    "- 需要同時運行多個模型（memory 消耗大）\n",
    "\n",
    "DPO 的核心洞察：**可以直接從偏好資料優化 policy，不需要顯式的 reward model！**\n",
    "\n",
    "### 5.2 DPO 推導\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                         DPO 推導過程                                     │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  Step 1: RLHF 最優解的形式                                               │\n",
    "│  ─────────────────────────────                                          │\n",
    "│  對於 RLHF objective: max E[r(x,y)] - β·KL(π||π_ref)                   │\n",
    "│                                                                         │\n",
    "│  最優 policy 有封閉解：                                                  │\n",
    "│  π*(y|x) = (1/Z(x)) · π_ref(y|x) · exp(r(x,y)/β)                       │\n",
    "│                                                                         │\n",
    "│  Step 2: 從 policy 反推 reward                                          │\n",
    "│  ─────────────────────────────                                          │\n",
    "│  重排得：                                                                │\n",
    "│  r(x,y) = β · log(π*(y|x) / π_ref(y|x)) + β·log(Z(x))                 │\n",
    "│                                                                         │\n",
    "│  Step 3: 代入 Bradley-Terry Model                                       │\n",
    "│  ─────────────────────────────                                          │\n",
    "│  P(y_w > y_l | x) = σ(r(x,y_w) - r(x,y_l))                             │\n",
    "│                                                                         │\n",
    "│  代入後（Z(x) 被消掉）：                                                  │\n",
    "│  P(y_w > y_l | x) = σ(β·[log(π(y_w|x)/π_ref(y_w|x))                   │\n",
    "│                         - log(π(y_l|x)/π_ref(y_l|x))])                 │\n",
    "│                                                                         │\n",
    "│  Step 4: DPO Loss                                                       │\n",
    "│  ─────────────────                                                      │\n",
    "│  L_DPO = -E[log σ(β·(log(π(y_w|x)/π_ref(y_w|x))                        │\n",
    "│                   - log(π(y_l|x)/π_ref(y_l|x))))]                       │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpo_loss(\n",
    "    policy_chosen_logps: torch.Tensor,\n",
    "    policy_rejected_logps: torch.Tensor,\n",
    "    reference_chosen_logps: torch.Tensor,\n",
    "    reference_rejected_logps: torch.Tensor,\n",
    "    beta: float = 0.1,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    計算 DPO Loss\n",
    "    \n",
    "    Args:\n",
    "        policy_chosen_logps: log π_θ(y_w|x)\n",
    "        policy_rejected_logps: log π_θ(y_l|x)\n",
    "        reference_chosen_logps: log π_ref(y_w|x)\n",
    "        reference_rejected_logps: log π_ref(y_l|x)\n",
    "        beta: temperature parameter\n",
    "    \n",
    "    Returns:\n",
    "        loss, chosen_rewards, rejected_rewards\n",
    "    \"\"\"\n",
    "    # 計算 log ratios\n",
    "    chosen_logratios = policy_chosen_logps - reference_chosen_logps\n",
    "    rejected_logratios = policy_rejected_logps - reference_rejected_logps\n",
    "    \n",
    "    # Implicit rewards (這就是 DPO 隱式學到的 reward)\n",
    "    chosen_rewards = beta * chosen_logratios\n",
    "    rejected_rewards = beta * rejected_logratios\n",
    "    \n",
    "    # DPO loss: -log(σ(β * (log_ratio_chosen - log_ratio_rejected)))\n",
    "    logits = chosen_logratios - rejected_logratios\n",
    "    loss = -F.logsigmoid(beta * logits).mean()\n",
    "    \n",
    "    return loss, chosen_rewards.detach(), rejected_rewards.detach()\n",
    "\n",
    "\n",
    "# 示範 DPO loss 計算\n",
    "batch_size = 4\n",
    "\n",
    "# 模擬 log probabilities\n",
    "policy_chosen = torch.randn(batch_size)      # log π_θ(y_w|x)\n",
    "policy_rejected = torch.randn(batch_size)    # log π_θ(y_l|x)\n",
    "ref_chosen = torch.randn(batch_size)         # log π_ref(y_w|x)\n",
    "ref_rejected = torch.randn(batch_size)       # log π_ref(y_l|x)\n",
    "\n",
    "loss, chosen_r, rejected_r = dpo_loss(\n",
    "    policy_chosen, policy_rejected, \n",
    "    ref_chosen, ref_rejected,\n",
    "    beta=0.1\n",
    ")\n",
    "\n",
    "print(f\"DPO Loss: {loss.item():.4f}\")\n",
    "print(f\"Chosen implicit rewards: {chosen_r.tolist()}\")\n",
    "print(f\"Rejected implicit rewards: {rejected_r.tolist()}\")\n",
    "print(f\"Reward margins (chosen - rejected): {(chosen_r - rejected_r).tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPOTrainer:\n",
    "    \"\"\"\n",
    "    DPO Trainer for LLM\n",
    "    簡化版實現，展示核心概念\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        ref_model: nn.Module,\n",
    "        beta: float = 0.1,\n",
    "        learning_rate: float = 1e-6,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.ref_model = ref_model\n",
    "        self.beta = beta\n",
    "        \n",
    "        # Freeze reference model\n",
    "        for param in self.ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=learning_rate\n",
    "        )\n",
    "    \n",
    "    def get_batch_logps(\n",
    "        self, \n",
    "        model: nn.Module,\n",
    "        input_ids: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        計算整個序列的 log probability\n",
    "        \"\"\"\n",
    "        # 這是簡化版，實際需要根據模型架構調整\n",
    "        logits = model(input_ids)  # [batch, seq, vocab]\n",
    "        \n",
    "        # Shift for language modeling\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = labels[:, 1:].contiguous()\n",
    "        \n",
    "        # Per-token log probabilities\n",
    "        log_probs = F.log_softmax(shift_logits, dim=-1)\n",
    "        \n",
    "        # Gather the log probs of the actual tokens\n",
    "        token_log_probs = log_probs.gather(-1, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Sum over sequence (or apply mask)\n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask[:, 1:]\n",
    "            token_log_probs = token_log_probs * mask\n",
    "            sequence_log_probs = token_log_probs.sum(dim=-1) / mask.sum(dim=-1).clamp(min=1)\n",
    "        else:\n",
    "            sequence_log_probs = token_log_probs.mean(dim=-1)\n",
    "        \n",
    "        return sequence_log_probs\n",
    "    \n",
    "    def train_step(\n",
    "        self,\n",
    "        chosen_ids: torch.Tensor,\n",
    "        rejected_ids: torch.Tensor,\n",
    "        chosen_mask: torch.Tensor = None,\n",
    "        rejected_mask: torch.Tensor = None,\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        執行一步 DPO 訓練\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # 計算 policy model 的 log probs\n",
    "        policy_chosen_logps = self.get_batch_logps(\n",
    "            self.model, chosen_ids, chosen_ids, chosen_mask\n",
    "        )\n",
    "        policy_rejected_logps = self.get_batch_logps(\n",
    "            self.model, rejected_ids, rejected_ids, rejected_mask\n",
    "        )\n",
    "        \n",
    "        # 計算 reference model 的 log probs (no grad)\n",
    "        with torch.no_grad():\n",
    "            ref_chosen_logps = self.get_batch_logps(\n",
    "                self.ref_model, chosen_ids, chosen_ids, chosen_mask\n",
    "            )\n",
    "            ref_rejected_logps = self.get_batch_logps(\n",
    "                self.ref_model, rejected_ids, rejected_ids, rejected_mask\n",
    "            )\n",
    "        \n",
    "        # 計算 DPO loss\n",
    "        loss, chosen_rewards, rejected_rewards = dpo_loss(\n",
    "            policy_chosen_logps, policy_rejected_logps,\n",
    "            ref_chosen_logps, ref_rejected_logps,\n",
    "            beta=self.beta\n",
    "        )\n",
    "        \n",
    "        # 反向傳播\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 計算指標\n",
    "        reward_margin = (chosen_rewards - rejected_rewards).mean().item()\n",
    "        accuracy = (chosen_rewards > rejected_rewards).float().mean().item()\n",
    "        \n",
    "        return {\n",
    "            'loss': loss.item(),\n",
    "            'reward_margin': reward_margin,\n",
    "            'accuracy': accuracy,\n",
    "            'chosen_reward': chosen_rewards.mean().item(),\n",
    "            'rejected_reward': rejected_rewards.mean().item(),\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"DPO Trainer 已定義\")\n",
    "print(\"\\nDPO vs RLHF (PPO):\")\n",
    "print(\"┌─────────────────────────────────────────────────────────────┐\")\n",
    "print(\"│              RLHF (PPO)           │           DPO           │\")\n",
    "print(\"├─────────────────────────────────────────────────────────────┤\")\n",
    "print(\"│  需要 Reward Model               │  不需要 Reward Model     │\")\n",
    "print(\"│  需要 Value Network              │  不需要 Value Network    │\")\n",
    "print(\"│  PPO 訓練複雜                    │  簡單的 supervised loss │\")\n",
    "print(\"│  需要 online sampling            │  可以完全 offline       │\")\n",
    "print(\"│  4 個模型（policy, ref, RM, V）  │  2 個模型（policy, ref）│\")\n",
    "print(\"│  訓練不穩定                      │  穩定收斂               │\")\n",
    "print(\"└─────────────────────────────────────────────────────────────┘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DPO 變體與進階方法\n",
    "\n",
    "### 6.1 IPO (Identity Preference Optimization)\n",
    "\n",
    "DPO 可能過度擬合偏好資料，IPO 使用不同的損失函數來改善。\n",
    "\n",
    "### 6.2 KTO (Kahneman-Tversky Optimization)\n",
    "\n",
    "不需要成對的偏好資料，只需要知道每個回應是好是壞。\n",
    "\n",
    "### 6.3 ORPO (Odds Ratio Preference Optimization)\n",
    "\n",
    "結合 SFT 和偏好優化到單一訓練階段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipo_loss(\n",
    "    policy_chosen_logps: torch.Tensor,\n",
    "    policy_rejected_logps: torch.Tensor,\n",
    "    reference_chosen_logps: torch.Tensor,\n",
    "    reference_rejected_logps: torch.Tensor,\n",
    "    beta: float = 0.1,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    IPO (Identity Preference Optimization) Loss\n",
    "    \n",
    "    使用 squared loss 而非 logistic loss，更加 robust\n",
    "    \"\"\"\n",
    "    chosen_logratios = policy_chosen_logps - reference_chosen_logps\n",
    "    rejected_logratios = policy_rejected_logps - reference_rejected_logps\n",
    "    \n",
    "    # IPO loss: (logratios_diff - 1/2β)^2\n",
    "    logits = chosen_logratios - rejected_logratios\n",
    "    loss = (logits - 1 / (2 * beta)) ** 2\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def kto_loss(\n",
    "    policy_logps: torch.Tensor,\n",
    "    reference_logps: torch.Tensor,\n",
    "    is_desirable: torch.Tensor,  # 1 if good, 0 if bad\n",
    "    kl_penalty: torch.Tensor,\n",
    "    beta: float = 0.1,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    KTO (Kahneman-Tversky Optimization) Loss\n",
    "    \n",
    "    不需要成對資料，只需要好/壞標籤\n",
    "    基於前景理論 (Prospect Theory)\n",
    "    \"\"\"\n",
    "    logratios = policy_logps - reference_logps\n",
    "    \n",
    "    # Desirable (good) responses: want to increase probability\n",
    "    desirable_loss = 1 - F.sigmoid(beta * (logratios - kl_penalty))\n",
    "    \n",
    "    # Undesirable (bad) responses: want to decrease probability\n",
    "    undesirable_loss = 1 - F.sigmoid(beta * (kl_penalty - logratios))\n",
    "    \n",
    "    # Combine based on labels\n",
    "    loss = is_desirable * desirable_loss + (1 - is_desirable) * undesirable_loss\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "print(\"DPO 變體比較：\")\n",
    "print(\"\")\n",
    "print(\"┌────────────────────────────────────────────────────────────────┐\")\n",
    "print(\"│  方法  │  資料需求           │  特點                          │\")\n",
    "print(\"├────────────────────────────────────────────────────────────────┤\")\n",
    "print(\"│  DPO   │  成對偏好 (A > B)  │  簡單有效，可能過擬合          │\")\n",
    "print(\"│  IPO   │  成對偏好 (A > B)  │  使用 squared loss，更穩定     │\")\n",
    "print(\"│  KTO   │  單一標籤 (好/壞)  │  不需成對，更易收集資料        │\")\n",
    "print(\"│  ORPO  │  成對偏好          │  結合 SFT，一階段訓練          │\")\n",
    "print(\"└────────────────────────────────────────────────────────────────┘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 使用 TRL 庫進行實作\n",
    "\n",
    "TRL (Transformer Reinforcement Learning) 是 Hugging Face 的官方 RLHF 庫。\n",
    "\n",
    "### 7.1 TRL 核心組件\n",
    "\n",
    "```python\n",
    "# TRL 主要 Trainer 類別\n",
    "from trl import (\n",
    "    SFTTrainer,        # Supervised Fine-Tuning\n",
    "    RewardTrainer,     # Reward Model Training\n",
    "    PPOTrainer,        # PPO for RLHF\n",
    "    DPOTrainer,        # Direct Preference Optimization\n",
    "    KTOTrainer,        # Kahneman-Tversky Optimization\n",
    "    ORPOTrainer,       # Odds Ratio Preference Optimization\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRL DPO 使用範例（概念展示）\n",
    "trl_dpo_example = '''\n",
    "# TRL DPO 完整範例\n",
    "# 注意：需要安裝 trl: pip install trl\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. 載入模型和 tokenizer\n",
    "model_name = \"microsoft/DialoGPT-small\"  # 或其他小型模型\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Reference model (frozen copy)\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 2. 準備偏好資料集\n",
    "# 格式: {\"prompt\": str, \"chosen\": str, \"rejected\": str}\n",
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:1000]\")\n",
    "\n",
    "def format_dataset(example):\n",
    "    # 根據資料集格式調整\n",
    "    return {\n",
    "        \"prompt\": example[\"prompt\"],\n",
    "        \"chosen\": example[\"chosen\"],\n",
    "        \"rejected\": example[\"rejected\"],\n",
    "    }\n",
    "\n",
    "formatted_dataset = dataset.map(format_dataset)\n",
    "\n",
    "# 3. 設定 DPO 訓練參數\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"./dpo_output\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-7,\n",
    "    beta=0.1,  # DPO temperature\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    fp16=True,  # 混合精度\n",
    "    max_length=512,\n",
    "    max_prompt_length=256,\n",
    ")\n",
    "\n",
    "# 4. 建立 DPOTrainer\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    args=training_args,\n",
    "    train_dataset=formatted_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 5. 開始訓練\n",
    "trainer.train()\n",
    "\n",
    "# 6. 儲存模型\n",
    "trainer.save_model(\"./dpo_final_model\")\n",
    "'''\n",
    "\n",
    "print(\"TRL DPO 使用範例：\")\n",
    "print(\"=\"*60)\n",
    "print(trl_dpo_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRL PPO 使用範例（概念展示）\n",
    "trl_ppo_example = '''\n",
    "# TRL PPO 完整範例\n",
    "# 注意：PPO 比 DPO 複雜，需要更多設定\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "import torch\n",
    "\n",
    "# 1. 載入模型（需要 value head）\n",
    "model_name = \"gpt2\"\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. 載入 Reward Model（需要預先訓練好）\n",
    "reward_model = ...  # 您的 reward model\n",
    "\n",
    "# 3. PPO 設定\n",
    "ppo_config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=16,\n",
    "    mini_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    ppo_epochs=4,  # PPO update epochs per batch\n",
    "    init_kl_coef=0.2,  # Initial KL penalty coefficient\n",
    "    target_kl=6.0,  # Target KL divergence\n",
    ")\n",
    "\n",
    "# 4. 建立 PPOTrainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 5. 訓練迴圈\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\": 128,\n",
    "}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        query_tensors = batch[\"input_ids\"]\n",
    "        \n",
    "        # 生成回應\n",
    "        response_tensors = ppo_trainer.generate(\n",
    "            query_tensors,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "        \n",
    "        # 計算 reward\n",
    "        texts = [tokenizer.decode(r) for r in response_tensors]\n",
    "        rewards = [reward_model(text) for text in texts]\n",
    "        \n",
    "        # PPO 更新\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        \n",
    "        print(f\"Epoch {epoch}, Mean reward: {torch.mean(rewards):.4f}\")\n",
    "'''\n",
    "\n",
    "print(\"TRL PPO 使用範例：\")\n",
    "print(\"=\"*60)\n",
    "print(trl_ppo_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 對齊評估方法\n",
    "\n",
    "### 8.1 常見評估指標\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                      對齊評估方法                                    │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│  1. Win Rate（勝率）                                                │\n",
    "│     - 讓人類或 GPT-4 比較兩個模型的輸出                              │\n",
    "│     - 計算新模型「贏」的比例                                         │\n",
    "│                                                                     │\n",
    "│  2. Reward Model Score                                             │\n",
    "│     - 使用 Reward Model 自動評分                                    │\n",
    "│     - 注意：可能有 reward hacking                                   │\n",
    "│                                                                     │\n",
    "│  3. Benchmark 評估                                                  │\n",
    "│     - MT-Bench：多輪對話能力                                        │\n",
    "│     - AlpacaEval：指令遵循能力                                      │\n",
    "│     - TruthfulQA：誠實性                                           │\n",
    "│     - HarmBench：安全性                                            │\n",
    "│                                                                     │\n",
    "│  4. Human Evaluation                                               │\n",
    "│     - 最可靠但最昂貴                                                │\n",
    "│     - 評估維度：有幫助、無害、誠實                                   │\n",
    "│                                                                     │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_win_rate(model_a_scores: List[float], \n",
    "                       model_b_scores: List[float]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    計算 Model A vs Model B 的勝率\n",
    "    \n",
    "    Args:\n",
    "        model_a_scores: Model A 在每個問題上的分數\n",
    "        model_b_scores: Model B 在每個問題上的分數\n",
    "    \n",
    "    Returns:\n",
    "        win_rate, lose_rate, tie_rate\n",
    "    \"\"\"\n",
    "    assert len(model_a_scores) == len(model_b_scores)\n",
    "    n = len(model_a_scores)\n",
    "    \n",
    "    wins = sum(1 for a, b in zip(model_a_scores, model_b_scores) if a > b)\n",
    "    losses = sum(1 for a, b in zip(model_a_scores, model_b_scores) if a < b)\n",
    "    ties = sum(1 for a, b in zip(model_a_scores, model_b_scores) if a == b)\n",
    "    \n",
    "    return {\n",
    "        'win_rate': wins / n,\n",
    "        'lose_rate': losses / n,\n",
    "        'tie_rate': ties / n,\n",
    "        'wins': wins,\n",
    "        'losses': losses,\n",
    "        'ties': ties,\n",
    "        'total': n,\n",
    "    }\n",
    "\n",
    "\n",
    "# 模擬評估結果\n",
    "np.random.seed(42)\n",
    "\n",
    "# 假設這是 RLHF 模型 vs 原始 SFT 模型的比較\n",
    "n_samples = 100\n",
    "rlhf_scores = np.random.normal(7.5, 1.5, n_samples)  # RLHF 模型較好\n",
    "sft_scores = np.random.normal(6.0, 1.5, n_samples)   # SFT 模型\n",
    "\n",
    "results = calculate_win_rate(rlhf_scores.tolist(), sft_scores.tolist())\n",
    "\n",
    "print(\"RLHF Model vs SFT Model 評估結果：\")\n",
    "print(f\"Win Rate:  {results['win_rate']:.1%} ({results['wins']}/{results['total']})\")\n",
    "print(f\"Lose Rate: {results['lose_rate']:.1%} ({results['losses']}/{results['total']})\")\n",
    "print(f\"Tie Rate:  {results['tie_rate']:.1%} ({results['ties']}/{results['total']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化對齊訓練過程\n",
    "def visualize_alignment_training():\n",
    "    \"\"\"模擬並視覺化對齊訓練過程\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    steps = np.arange(0, 1000, 10)\n",
    "    \n",
    "    # 1. Reward 曲線\n",
    "    reward_mean = 2 + 3 * (1 - np.exp(-steps / 300))\n",
    "    reward_std = 0.5 * np.exp(-steps / 500) + 0.1\n",
    "    \n",
    "    axes[0, 0].plot(steps, reward_mean, 'b-', linewidth=2, label='Mean Reward')\n",
    "    axes[0, 0].fill_between(steps, reward_mean - reward_std, reward_mean + reward_std, \n",
    "                            alpha=0.3, label='±1 std')\n",
    "    axes[0, 0].set_xlabel('Training Steps')\n",
    "    axes[0, 0].set_ylabel('Reward')\n",
    "    axes[0, 0].set_title('Reward During Training')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. KL Divergence\n",
    "    kl = 0.01 * steps ** 0.5\n",
    "    kl_target = np.ones_like(steps) * 6.0\n",
    "    \n",
    "    axes[0, 1].plot(steps, kl, 'r-', linewidth=2, label='Actual KL')\n",
    "    axes[0, 1].plot(steps, kl_target, 'k--', linewidth=1, label='Target KL')\n",
    "    axes[0, 1].set_xlabel('Training Steps')\n",
    "    axes[0, 1].set_ylabel('KL Divergence')\n",
    "    axes[0, 1].set_title('KL Divergence from Reference Model')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Win Rate Over Time\n",
    "    win_rate = 50 + 25 * (1 - np.exp(-steps / 400))\n",
    "    \n",
    "    axes[1, 0].plot(steps, win_rate, 'g-', linewidth=2)\n",
    "    axes[1, 0].axhline(y=50, color='gray', linestyle='--', alpha=0.5, label='50% baseline')\n",
    "    axes[1, 0].set_xlabel('Training Steps')\n",
    "    axes[1, 0].set_ylabel('Win Rate (%)')\n",
    "    axes[1, 0].set_title('Win Rate vs Base Model')\n",
    "    axes[1, 0].set_ylim([40, 80])\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Reward vs KL Trade-off\n",
    "    kl_values = np.linspace(0, 10, 100)\n",
    "    reward_for_kl = 5 - 2 * np.exp(-kl_values / 2) + 0.3 * np.sqrt(kl_values)\n",
    "    \n",
    "    axes[1, 1].plot(kl_values, reward_for_kl, 'purple', linewidth=2)\n",
    "    axes[1, 1].axvline(x=6, color='orange', linestyle='--', label='Typical KL target')\n",
    "    axes[1, 1].set_xlabel('KL Divergence')\n",
    "    axes[1, 1].set_ylabel('Expected Reward')\n",
    "    axes[1, 1].set_title('Reward vs KL Trade-off (Pareto Frontier)')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 標註最佳點\n",
    "    optimal_kl = 6\n",
    "    optimal_reward = 5 - 2 * np.exp(-optimal_kl / 2) + 0.3 * np.sqrt(optimal_kl)\n",
    "    axes[1, 1].scatter([optimal_kl], [optimal_reward], color='red', s=100, zorder=5)\n",
    "    axes[1, 1].annotate('Optimal', (optimal_kl, optimal_reward), \n",
    "                        textcoords=\"offset points\", xytext=(10, 10))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n對齊訓練的關鍵觀察：\")\n",
    "    print(\"1. Reward 隨訓練增加，但有上限（避免 reward hacking）\")\n",
    "    print(\"2. KL divergence 需要控制，太大會導致能力退化\")\n",
    "    print(\"3. Win rate 是最終指標，反映實際對齊效果\")\n",
    "    print(\"4. Reward vs KL 有 trade-off，需要找到平衡點\")\n",
    "\n",
    "visualize_alignment_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 練習題\n",
    "\n",
    "### 練習 1：實作完整的 Reward Model 訓練\n",
    "\n",
    "使用提供的框架，實作一個可以訓練的 Reward Model。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1：實作 Reward Model 訓練\n",
    "class PreferenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    TODO: 實作偏好資料集\n",
    "    \n",
    "    每個樣本包含：\n",
    "    - chosen_ids: 被偏好的回應的 token ids\n",
    "    - rejected_ids: 被拒絕的回應的 token ids\n",
    "    \"\"\"\n",
    "    def __init__(self, data: List[PreferenceExample], max_length: int = 128):\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "        # 簡化：使用字元級別的 tokenization\n",
    "        self.char_to_idx = {chr(i): i for i in range(256)}\n",
    "    \n",
    "    def tokenize(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"簡單的字元級 tokenization\"\"\"\n",
    "        # TODO: 實作 tokenization\n",
    "        # 1. 將文字轉換為 indices\n",
    "        # 2. 截斷或填充到 max_length\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: 返回 chosen 和 rejected 的 token ids\n",
    "        pass\n",
    "\n",
    "\n",
    "def train_reward_model(model, dataset, num_epochs=5, lr=1e-4):\n",
    "    \"\"\"\n",
    "    TODO: 實作 Reward Model 訓練迴圈\n",
    "    \n",
    "    步驟：\n",
    "    1. 建立 DataLoader\n",
    "    2. 建立 optimizer\n",
    "    3. 訓練迴圈：\n",
    "       - 前向傳播得到 chosen 和 rejected 的 reward\n",
    "       - 計算 Bradley-Terry loss\n",
    "       - 反向傳播並更新\n",
    "       - 記錄 loss 和 accuracy\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "print(\"練習 1：實作偏好資料集的 tokenize 和 __getitem__ 方法，\")\n",
    "print(\"然後實作 train_reward_model 函數\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：比較 DPO 和 IPO\n",
    "\n",
    "在相同的偏好資料上，比較 DPO 和 IPO 的訓練穩定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2：比較 DPO 和 IPO\n",
    "def compare_dpo_ipo():\n",
    "    \"\"\"\n",
    "    TODO: 比較 DPO 和 IPO 的訓練行為\n",
    "    \n",
    "    步驟：\n",
    "    1. 生成模擬的 log probability 資料\n",
    "    2. 計算不同 beta 值下的 DPO loss 和 IPO loss\n",
    "    3. 繪製 loss landscape\n",
    "    4. 分析哪種方法更穩定\n",
    "    \"\"\"\n",
    "    # 模擬資料\n",
    "    n_samples = 100\n",
    "    \n",
    "    # 模擬 log probability differences\n",
    "    # 正確情況：chosen 的 log ratio 高於 rejected\n",
    "    log_ratio_diff = torch.randn(n_samples) + 0.5  # 平均正向\n",
    "    \n",
    "    # TODO: 計算不同 beta 值下的 loss\n",
    "    betas = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "    \n",
    "    # TODO: 繪製比較圖\n",
    "    pass\n",
    "\n",
    "print(\"練習 2：實作 compare_dpo_ipo 函數，\")\n",
    "print(\"比較兩種方法在不同 beta 值下的行為\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3：實作簡單的 KTO\n",
    "\n",
    "KTO 不需要成對資料，只需要好/壞標籤，實作並測試這種方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 3：實作 KTO Trainer\n",
    "@dataclass\n",
    "class PointwiseExample:\n",
    "    \"\"\"單點資料（非成對）\"\"\"\n",
    "    prompt: str\n",
    "    response: str\n",
    "    is_good: bool  # True = 好回應, False = 壞回應\n",
    "\n",
    "\n",
    "class KTOTrainer:\n",
    "    \"\"\"\n",
    "    TODO: 實作 KTO Trainer\n",
    "    \n",
    "    KTO 的關鍵：\n",
    "    1. 不需要成對資料\n",
    "    2. 基於 Kahneman-Tversky 前景理論\n",
    "    3. 對好回應和壞回應使用不同的損失\n",
    "    \"\"\"\n",
    "    def __init__(self, model, ref_model, beta=0.1, desirable_weight=1.0, undesirable_weight=1.0):\n",
    "        self.model = model\n",
    "        self.ref_model = ref_model\n",
    "        self.beta = beta\n",
    "        self.desirable_weight = desirable_weight\n",
    "        self.undesirable_weight = undesirable_weight\n",
    "    \n",
    "    def compute_loss(self, logps, ref_logps, is_desirable):\n",
    "        \"\"\"\n",
    "        TODO: 計算 KTO loss\n",
    "        \n",
    "        Hint:\n",
    "        - log_ratio = logps - ref_logps\n",
    "        - 對於好回應：loss = 1 - sigmoid(beta * (log_ratio - kl_penalty))\n",
    "        - 對於壞回應：loss = 1 - sigmoid(beta * (kl_penalty - log_ratio))\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "print(\"練習 3：實作 KTOTrainer 的 compute_loss 方法\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 總結\n",
    "\n",
    "### 10.1 RLHF Pipeline 完整回顧\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                      RLHF 完整流程                                       │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  ┌─────────────┐                                                        │\n",
    "│  │ Pretrained  │                                                        │\n",
    "│  │    LLM      │                                                        │\n",
    "│  └──────┬──────┘                                                        │\n",
    "│         │                                                               │\n",
    "│         ▼                                                               │\n",
    "│  ┌─────────────┐     ┌─────────────────────┐                            │\n",
    "│  │    SFT      │◄────│ Demonstration Data  │                            │\n",
    "│  │  Training   │     │ (Human-written)     │                            │\n",
    "│  └──────┬──────┘     └─────────────────────┘                            │\n",
    "│         │                                                               │\n",
    "│         ▼                                                               │\n",
    "│  ┌─────────────┐                                                        │\n",
    "│  │  SFT Model  │────────────────────┐                                   │\n",
    "│  │ (Reference) │                    │                                   │\n",
    "│  └──────┬──────┘                    │                                   │\n",
    "│         │                           │                                   │\n",
    "│         │     Option A: RLHF        │     Option B: DPO                 │\n",
    "│         │     ═══════════════       │     ═══════════════               │\n",
    "│         ▼                           │                                   │\n",
    "│  ┌─────────────┐                    │                                   │\n",
    "│  │   Reward    │◄───Preference      │                                   │\n",
    "│  │   Model     │    Data            │                                   │\n",
    "│  └──────┬──────┘                    │                                   │\n",
    "│         │                           │                                   │\n",
    "│         ▼                           ▼                                   │\n",
    "│  ┌─────────────┐             ┌─────────────┐                            │\n",
    "│  │    PPO      │             │    DPO      │◄───Preference               │\n",
    "│  │  Training   │             │  Training   │    Data                    │\n",
    "│  └──────┬──────┘             └──────┬──────┘                            │\n",
    "│         │                           │                                   │\n",
    "│         └───────────┬───────────────┘                                   │\n",
    "│                     │                                                   │\n",
    "│                     ▼                                                   │\n",
    "│              ┌─────────────┐                                            │\n",
    "│              │   Aligned   │                                            │\n",
    "│              │    Model    │                                            │\n",
    "│              └─────────────┘                                            │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 10.2 關鍵要點\n",
    "\n",
    "| 概念 | 說明 |\n",
    "|------|------|\n",
    "| Alignment Gap | 預訓練目標與使用者期望的差距 |\n",
    "| Reward Model | 學習人類偏好的評分模型 |\n",
    "| PPO | 使用 clipped objective 的穩定 RL 算法 |\n",
    "| KL Penalty | 防止模型偏離原始能力太遠 |\n",
    "| DPO | 直接優化偏好，不需要 Reward Model |\n",
    "| Win Rate | 評估對齊效果的核心指標 |\n",
    "\n",
    "### 10.3 實際應用建議\n",
    "\n",
    "1. **小規模實驗**：先用 DPO，簡單且穩定\n",
    "2. **資料收集**：偏好資料品質比數量更重要\n",
    "3. **評估**：多維度評估（helpfulness, harmlessness, honesty）\n",
    "4. **迭代**：對齊是持續過程，需要不斷改進"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"RLHF 與 LLM 對齊 - 學習完成！\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n你已經學會：\")\n",
    "print(\"✓ 理解 LLM 對齊問題的本質\")\n",
    "print(\"✓ Reward Model 的訓練方法\")\n",
    "print(\"✓ PPO 在 LLM 上的應用\")\n",
    "print(\"✓ DPO 的原理與實作\")\n",
    "print(\"✓ 對齊效果的評估方法\")\n",
    "print(\"\\n下一步學習建議：\")\n",
    "print(\"1. 使用 TRL 在真實資料上訓練 DPO\")\n",
    "print(\"2. 嘗試其他對齊方法（IPO, KTO, ORPO）\")\n",
    "print(\"3. 研究 Constitutional AI 和 RLAIF\")\n",
    "print(\"4. 探索多維度對齊（安全性、有幫助、誠實）\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
