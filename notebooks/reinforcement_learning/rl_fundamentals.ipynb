{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¼·åŒ–å­¸ç¿’åŸºç¤ (Reinforcement Learning)\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "- ç†è§£å¼·åŒ–å­¸ç¿’çš„åŸºæœ¬æ¦‚å¿µ\n",
    "- å¯¦ä½œ Policy Gradient\n",
    "- å­¸ç¿’ Actor-Critic æ–¹æ³•\n",
    "- è§£æ±º CartPole å•é¡Œ\n",
    "\n",
    "## å°æ‡‰èª²ç¨‹\n",
    "- [æå®æ¯… ML 2021 - Reinforcement Learning](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php)\n",
    "- [HW12: Reinforcement Learning](https://github.com/ga642381/ML2021-Spring/tree/main/HW12)\n",
    "\n",
    "## å¼·åŒ–å­¸ç¿’æ¦‚è¿°\n",
    "\n",
    "```\n",
    "å¼·åŒ–å­¸ç¿’æ¡†æ¶\n",
    "    Agent â†â†’ Environment\n",
    "      â”‚         â”‚\n",
    "      â”‚ action  â”‚ state, reward\n",
    "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "æ ¸å¿ƒæ¦‚å¿µ:\n",
    "â”œâ”€â”€ State (s): ç’°å¢ƒç‹€æ…‹\n",
    "â”œâ”€â”€ Action (a): ä»£ç†å‹•ä½œ\n",
    "â”œâ”€â”€ Reward (r): çå‹µä¿¡è™Ÿ\n",
    "â”œâ”€â”€ Policy (Ï€): ç­–ç•¥å‡½æ•¸ Ï€(a|s)\n",
    "â””â”€â”€ Value (V): åƒ¹å€¼å‡½æ•¸ V(s)\n",
    "\n",
    "ä¸»è¦æ–¹æ³•:\n",
    "â”œâ”€â”€ Value-based: Q-Learning, DQN\n",
    "â”œâ”€â”€ Policy-based: REINFORCE, PPO\n",
    "â””â”€â”€ Actor-Critic: A2C, A3C, SAC\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# å˜—è©¦å°å…¥ gym\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    HAS_GYM = True\n",
    "    print(\"Gymnasium å·²è¼‰å…¥\")\n",
    "except ImportError:\n",
    "    try:\n",
    "        import gym\n",
    "        HAS_GYM = True\n",
    "        print(\"Gym å·²è¼‰å…¥\")\n",
    "    except ImportError:\n",
    "        HAS_GYM = False\n",
    "        print(\"è«‹å®‰è£: pip install gymnasium æˆ– pip install gym\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "# å›ºå®šéš¨æ©Ÿç¨®å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: ç’°å¢ƒä»‹ç´¹ - CartPole\n",
    "\n",
    "CartPole æ˜¯ä¸€å€‹ç¶“å…¸çš„æ§åˆ¶å•é¡Œï¼šæ§åˆ¶ä¸€å€‹å°è»Šï¼Œä½¿å…¶ä¸Šçš„æ†å­ä¿æŒå¹³è¡¡ã€‚\n",
    "\n",
    "- **State**: [è»Šä½ç½®, è»Šé€Ÿåº¦, æ†è§’åº¦, æ†è§’é€Ÿåº¦]\n",
    "- **Action**: 0=å‘å·¦, 1=å‘å³\n",
    "- **Reward**: æ¯æ­¥ +1ï¼ˆæ†å­æ²’å€’ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ç°¡æ˜“ CartPole ç’°å¢ƒï¼ˆä¸éœ€è¦ gymï¼‰==========\n",
    "\n",
    "class SimpleCartPole:\n",
    "    \"\"\"\n",
    "    ç°¡åŒ–ç‰ˆ CartPole ç’°å¢ƒ\n",
    "    \n",
    "    ç”¨æ–¼åœ¨æ²’æœ‰ gym æ™‚æ¼”ç¤º RL æ¦‚å¿µ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02\n",
    "        \n",
    "        # é–¾å€¼\n",
    "        self.theta_threshold = 12 * 2 * np.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "        \n",
    "        self.state = None\n",
    "        self.steps = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.random.uniform(-0.05, 0.05, size=(4,))\n",
    "        self.steps = 0\n",
    "        return self.state.copy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        \n",
    "        costheta = np.cos(theta)\n",
    "        sintheta = np.sin(theta)\n",
    "        \n",
    "        temp = (force + self.polemass_length * theta_dot**2 * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0/3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "        \n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * xacc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * thetaacc\n",
    "        \n",
    "        self.state = np.array([x, x_dot, theta, theta_dot])\n",
    "        self.steps += 1\n",
    "        \n",
    "        done = (\n",
    "            x < -self.x_threshold or\n",
    "            x > self.x_threshold or\n",
    "            theta < -self.theta_threshold or\n",
    "            theta > self.theta_threshold or\n",
    "            self.steps >= 500\n",
    "        )\n",
    "        \n",
    "        reward = 1.0 if not done else 0.0\n",
    "        \n",
    "        return self.state.copy(), reward, done, {}\n",
    "    \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        class Space:\n",
    "            shape = (4,)\n",
    "        return Space()\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        class Space:\n",
    "            n = 2\n",
    "        return Space()\n",
    "\n",
    "# å»ºç«‹ç’°å¢ƒ\n",
    "if HAS_GYM:\n",
    "    env = gym.make('CartPole-v1')\n",
    "else:\n",
    "    env = SimpleCartPole()\n",
    "\n",
    "print(f\"è§€å¯Ÿç©ºé–“: {env.observation_space.shape}\")\n",
    "print(f\"å‹•ä½œç©ºé–“: {env.action_space.n}\")\n",
    "\n",
    "# æ¸¬è©¦ç’°å¢ƒ\n",
    "state = env.reset() if not HAS_GYM else env.reset()[0]\n",
    "print(f\"\\nåˆå§‹ç‹€æ…‹: {state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== éš¨æ©Ÿç­–ç•¥æ¸¬è©¦ ==========\n",
    "\n",
    "def run_random_policy(env, n_episodes=10):\n",
    "    \"\"\"\n",
    "    ç”¨éš¨æ©Ÿç­–ç•¥æ¸¬è©¦ç’°å¢ƒ\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(n_episodes):\n",
    "        state = env.reset() if not HAS_GYM else env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "            result = env.step(action)\n",
    "            if HAS_GYM:\n",
    "                state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                state, reward, done, _ = result\n",
    "            episode_reward += reward\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "random_rewards = run_random_policy(env, n_episodes=100)\n",
    "print(f\"éš¨æ©Ÿç­–ç•¥å¹³å‡çå‹µ: {np.mean(random_rewards):.2f} Â± {np.std(random_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Policy Gradient (REINFORCE)\n",
    "\n",
    "### æ ¸å¿ƒå…¬å¼\n",
    "\n",
    "ç›®æ¨™ï¼šæœ€å¤§åŒ–æœŸæœ›å›å ±\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]$$\n",
    "\n",
    "æ¢¯åº¦ï¼š\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau}\\left[\\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t\\right]$$\n",
    "\n",
    "å…¶ä¸­ $G_t = \\sum_{k=t}^{T} \\gamma^{k-t} r_k$ æ˜¯å¾æ™‚åˆ» t é–‹å§‹çš„æŠ˜æ‰£å›å ±ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Policy Network ==========\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    ç­–ç•¥ç¶²è·¯\n",
    "    \n",
    "    è¼¸å…¥: ç‹€æ…‹\n",
    "    è¼¸å‡º: å‹•ä½œçš„æ¦‚ç‡åˆ†å¸ƒ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "    \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        \"\"\"\n",
    "        æ ¹æ“šç‹€æ…‹é¸æ“‡å‹•ä½œ\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.forward(state)\n",
    "        \n",
    "        if deterministic:\n",
    "            action = probs.argmax(dim=-1).item()\n",
    "        else:\n",
    "            dist = Categorical(probs)\n",
    "            action = dist.sample().item()\n",
    "        \n",
    "        return action, probs\n",
    "\n",
    "# æ¸¬è©¦\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "print(f\"ç­–ç•¥ç¶²è·¯:\")\n",
    "print(f\"  è¼¸å…¥ç¶­åº¦: {state_dim}\")\n",
    "print(f\"  è¼¸å‡ºç¶­åº¦: {action_dim}\")\n",
    "print(f\"  åƒæ•¸é‡: {sum(p.numel() for p in policy.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== REINFORCE æ¼”ç®—æ³• ==========\n",
    "\n",
    "class REINFORCE:\n",
    "    \"\"\"\n",
    "    REINFORCE æ¼”ç®—æ³•ï¼ˆå¸¶ baselineï¼‰\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):\n",
    "        self.policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # å„²å­˜è»Œè·¡\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"é¸æ“‡å‹•ä½œä¸¦è¨˜éŒ„ log probability\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs = self.policy(state)\n",
    "        \n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        self.log_probs.append(dist.log_prob(action))\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"å„²å­˜çå‹µ\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"æ›´æ–°ç­–ç•¥\"\"\"\n",
    "        # è¨ˆç®—æŠ˜æ‰£å›å ±\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        \n",
    "        # æ­£è¦åŒ–ï¼ˆbaselineï¼‰\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # è¨ˆç®—æå¤±\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(self.log_probs, returns):\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        \n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        # æ›´æ–°\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # æ¸…ç©ºè»Œè·¡\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        return policy_loss.item()\n",
    "\n",
    "# å»ºç«‹ agent\n",
    "agent = REINFORCE(state_dim, action_dim, lr=1e-3, gamma=0.99)\n",
    "print(\"REINFORCE Agent å»ºç«‹å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è¨“ç·´ REINFORCE ==========\n",
    "\n",
    "def train_reinforce(env, agent, n_episodes=500, print_every=50):\n",
    "    \"\"\"\n",
    "    è¨“ç·´ REINFORCE agent\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    avg_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset() if not HAS_GYM else env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # é¸æ“‡å‹•ä½œ\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # åŸ·è¡Œå‹•ä½œ\n",
    "            result = env.step(action)\n",
    "            if HAS_GYM:\n",
    "                next_state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                next_state, reward, done, _ = result\n",
    "            \n",
    "            # å„²å­˜çå‹µ\n",
    "            agent.store_reward(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "        \n",
    "        # Episode çµæŸï¼Œæ›´æ–°ç­–ç•¥\n",
    "        agent.update()\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        avg_reward = np.mean(episode_rewards[-100:])\n",
    "        avg_rewards.append(avg_reward)\n",
    "        \n",
    "        if (episode + 1) % print_every == 0:\n",
    "            print(f\"Episode {episode+1}: Reward = {episode_reward:.0f}, Avg(100) = {avg_reward:.1f}\")\n",
    "        \n",
    "        # æå‰çµæŸ\n",
    "        if avg_reward >= 475:\n",
    "            print(f\"\\nç’°å¢ƒå·²è§£æ±ºï¼åœ¨ {episode+1} å€‹ episode å¾Œ\")\n",
    "            break\n",
    "    \n",
    "    return episode_rewards, avg_rewards\n",
    "\n",
    "# è¨“ç·´\n",
    "rewards, avg_rewards = train_reinforce(env, agent, n_episodes=500, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== å¯è¦–åŒ–è¨“ç·´çµæœ ==========\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# æ¯å€‹ episode çš„çå‹µ\n",
    "axes[0].plot(rewards, alpha=0.3)\n",
    "axes[0].plot(avg_rewards, linewidth=2)\n",
    "axes[0].axhline(y=475, color='r', linestyle='--', label='Solved')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Reward')\n",
    "axes[0].set_title('Training Progress')\n",
    "axes[0].legend(['Episode Reward', 'Avg (100)', 'Solved Threshold'])\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# çå‹µåˆ†å¸ƒ\n",
    "axes[1].hist(rewards, bins=30, edgecolor='black')\n",
    "axes[1].axvline(x=np.mean(rewards), color='r', linestyle='--', label=f'Mean: {np.mean(rewards):.1f}')\n",
    "axes[1].set_xlabel('Reward')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Reward Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Actor-Critic\n",
    "\n",
    "Actor-Critic çµåˆäº† Policy Gradient å’Œ Value Functionï¼š\n",
    "- **Actor**: å­¸ç¿’ç­–ç•¥ $\\pi(a|s)$\n",
    "- **Critic**: å­¸ç¿’åƒ¹å€¼å‡½æ•¸ $V(s)$ï¼Œç”¨æ–¼æ¸›å°‘æ–¹å·®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Actor-Critic ç¶²è·¯ ==========\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Actor-Critic ç¶²è·¯\n",
    "    \n",
    "    å…±äº«åº•å±¤ç‰¹å¾µæå–ï¼Œåˆ†åˆ¥è¼¸å‡ºï¼š\n",
    "    - Actor: å‹•ä½œæ¦‚ç‡\n",
    "    - Critic: ç‹€æ…‹åƒ¹å€¼\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # å…±äº«å±¤\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Actor é ­\n",
    "        self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        # Critic é ­\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.shared(x)\n",
    "        \n",
    "        action_probs = F.softmax(self.actor(features), dim=-1)\n",
    "        state_value = self.critic(features)\n",
    "        \n",
    "        return action_probs, state_value\n",
    "\n",
    "# æ¸¬è©¦\n",
    "ac_net = ActorCritic(state_dim, action_dim).to(device)\n",
    "state_tensor = torch.randn(1, state_dim).to(device)\n",
    "probs, value = ac_net(state_tensor)\n",
    "\n",
    "print(f\"Actor-Critic ç¶²è·¯:\")\n",
    "print(f\"  å‹•ä½œæ¦‚ç‡: {probs.shape}\")\n",
    "print(f\"  ç‹€æ…‹åƒ¹å€¼: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== A2C (Advantage Actor-Critic) ==========\n",
    "\n",
    "class A2C:\n",
    "    \"\"\"\n",
    "    Advantage Actor-Critic\n",
    "    \n",
    "    ä½¿ç”¨ Advantage = R + Î³V(s') - V(s) ä»£æ›¿å›å ±\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, value_coef=0.5, entropy_coef=0.01):\n",
    "        self.network = ActorCritic(state_dim, action_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        # è»Œè·¡å„²å­˜\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        probs, value = self.network(state)\n",
    "        \n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        self.log_probs.append(dist.log_prob(action))\n",
    "        self.values.append(value)\n",
    "        self.entropies.append(dist.entropy())\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def store_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def update(self, next_state, done):\n",
    "        # è¨ˆç®— bootstrap value\n",
    "        if done:\n",
    "            R = 0\n",
    "        else:\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "            _, next_value = self.network(next_state)\n",
    "            R = next_value.item()\n",
    "        \n",
    "        # è¨ˆç®—å›å ±\n",
    "        returns = []\n",
    "        for r in reversed(self.rewards):\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.tensor(returns).to(device)\n",
    "        values = torch.cat(self.values).squeeze()\n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        entropies = torch.stack(self.entropies)\n",
    "        \n",
    "        # è¨ˆç®— advantage\n",
    "        advantages = returns - values.detach()\n",
    "        \n",
    "        # è¨ˆç®—æå¤±\n",
    "        actor_loss = -(log_probs * advantages).mean()\n",
    "        critic_loss = F.mse_loss(values, returns)\n",
    "        entropy_loss = -entropies.mean()\n",
    "        \n",
    "        total_loss = actor_loss + self.value_coef * critic_loss + self.entropy_coef * entropy_loss\n",
    "        \n",
    "        # æ›´æ–°\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), 0.5)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # æ¸…ç©º\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "        \n",
    "        return total_loss.item()\n",
    "\n",
    "# å»ºç«‹ A2C agent\n",
    "a2c_agent = A2C(state_dim, action_dim, lr=1e-3)\n",
    "print(\"A2C Agent å»ºç«‹å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è¨“ç·´ A2C ==========\n",
    "\n",
    "def train_a2c(env, agent, n_episodes=500, print_every=50):\n",
    "    \"\"\"\n",
    "    è¨“ç·´ A2C agent\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    avg_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset() if not HAS_GYM else env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            result = env.step(action)\n",
    "            if HAS_GYM:\n",
    "                next_state, reward, terminated, truncated, _ = result\n",
    "                done = terminated or truncated\n",
    "            else:\n",
    "                next_state, reward, done, _ = result\n",
    "            \n",
    "            agent.store_reward(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "        \n",
    "        # æ›´æ–°\n",
    "        agent.update(state, done)\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        avg_reward = np.mean(episode_rewards[-100:])\n",
    "        avg_rewards.append(avg_reward)\n",
    "        \n",
    "        if (episode + 1) % print_every == 0:\n",
    "            print(f\"Episode {episode+1}: Reward = {episode_reward:.0f}, Avg(100) = {avg_reward:.1f}\")\n",
    "        \n",
    "        if avg_reward >= 475:\n",
    "            print(f\"\\nç’°å¢ƒå·²è§£æ±ºï¼åœ¨ {episode+1} å€‹ episode å¾Œ\")\n",
    "            break\n",
    "    \n",
    "    return episode_rewards, avg_rewards\n",
    "\n",
    "# è¨“ç·´ A2C\n",
    "a2c_rewards, a2c_avg_rewards = train_a2c(env, a2c_agent, n_episodes=500, print_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: æ¯”è¼ƒä¸åŒæ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ¯”è¼ƒ REINFORCE vs A2C ==========\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(avg_rewards, label='REINFORCE', alpha=0.8)\n",
    "ax.plot(a2c_avg_rewards, label='A2C', alpha=0.8)\n",
    "ax.axhline(y=475, color='r', linestyle='--', label='Solved')\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Average Reward (100 episodes)')\n",
    "ax.set_title('REINFORCE vs A2C on CartPole')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\næœ€çµ‚è¡¨ç¾:\")\n",
    "print(f\"  REINFORCE: {avg_rewards[-1]:.1f}\")\n",
    "print(f\"  A2C: {a2c_avg_rewards[-1]:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¯ ç¸½çµ\n",
    "\n",
    "### RL æ–¹æ³•æ¯”è¼ƒ\n",
    "\n",
    "| æ–¹æ³• | å„ªé» | ç¼ºé» |\n",
    "|------|------|------|\n",
    "| **REINFORCE** | ç°¡å–®ã€ç›´è§€ | é«˜æ–¹å·®ã€æ¨£æœ¬æ•ˆç‡ä½ |\n",
    "| **A2C** | è¼ƒä½æ–¹å·® | å¯èƒ½æ”¶æ–‚åˆ°å±€éƒ¨æœ€å„ª |\n",
    "| **PPO** | ç©©å®šã€é€šç”¨ | è¶…åƒæ•¸è¼ƒå¤š |\n",
    "| **DQN** | æ¨£æœ¬æ•ˆç‡é«˜ | åªé©ç”¨é›¢æ•£å‹•ä½œ |\n",
    "\n",
    "### æå®æ¯… HW12 æŠ€å·§\n",
    "\n",
    "```\n",
    "Policy Gradient è¦é»:\n",
    "1. ä½¿ç”¨ baseline æ¸›å°‘æ–¹å·®\n",
    "2. æ­£è¦åŒ–å›å ±\n",
    "3. é©ç•¶çš„å­¸ç¿’ç‡\n",
    "4. æ·»åŠ  entropy bonus å¢åŠ æ¢ç´¢\n",
    "\n",
    "è¨“ç·´æŠ€å·§:\n",
    "1. æ¢¯åº¦è£å‰ª\n",
    "2. å¤šç’°å¢ƒä¸¦è¡Œ\n",
    "3. é©ç•¶çš„ç¶²è·¯å¤§å°\n",
    "4. ç›£æ§è¨“ç·´æ›²ç·š\n",
    "```\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å‰å¾€ `deployment/model_compression.ipynb` å­¸ç¿’æ¨¡å‹å£“ç¸®ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## ğŸ‹ï¸ ç·´ç¿’\n\n### ç·´ç¿’ 1: æ¯”è¼ƒä¸åŒè¶…åƒæ•¸å° REINFORCE çš„å½±éŸ¿\n\næ¸¬è©¦ä¸åŒå­¸ç¿’ç‡å’ŒæŠ˜æ‰£å› å­å°è¨“ç·´æ•ˆæœçš„å½±éŸ¿ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== ç·´ç¿’ 1: è¶…åƒæ•¸å¯¦é©— ==========\n\ndef train_reinforce_short(env, lr, gamma, n_episodes=200):\n    \"\"\"\n    è¨“ç·´ REINFORCE ä¸¦è¿”å›å­¸ç¿’æ›²ç·š\n    \"\"\"\n    agent = REINFORCE(state_dim, action_dim, lr=lr, gamma=gamma)\n    episode_rewards = []\n    \n    for episode in range(n_episodes):\n        state = env.reset() if not HAS_GYM else env.reset()[0]\n        episode_reward = 0\n        done = False\n        \n        while not done:\n            action = agent.select_action(state)\n            result = env.step(action)\n            if HAS_GYM:\n                next_state, reward, terminated, truncated, _ = result\n                done = terminated or truncated\n            else:\n                next_state, reward, done, _ = result\n            agent.store_reward(reward)\n            state = next_state\n            episode_reward += reward\n        \n        agent.update()\n        episode_rewards.append(episode_reward)\n    \n    # è¿”å›æ»‘å‹•å¹³å‡\n    window = 20\n    avg_rewards = [np.mean(episode_rewards[max(0,i-window):i+1]) for i in range(len(episode_rewards))]\n    return avg_rewards\n\ndef experiment_hyperparameters():\n    \"\"\"\n    æ¯”è¼ƒä¸åŒè¶…åƒæ•¸çµ„åˆ\n    \"\"\"\n    configs = [\n        {'lr': 1e-2, 'gamma': 0.99, 'label': 'lr=1e-2, Î³=0.99'},\n        {'lr': 1e-3, 'gamma': 0.99, 'label': 'lr=1e-3, Î³=0.99'},\n        {'lr': 1e-4, 'gamma': 0.99, 'label': 'lr=1e-4, Î³=0.99'},\n        {'lr': 1e-3, 'gamma': 0.9, 'label': 'lr=1e-3, Î³=0.9'},\n        {'lr': 1e-3, 'gamma': 0.95, 'label': 'lr=1e-3, Î³=0.95'},\n    ]\n    \n    results = {}\n    \n    print(\"ç·´ç¿’ 1: REINFORCE è¶…åƒæ•¸å¯¦é©—\")\n    print(\"=\" * 50)\n    \n    for config in configs:\n        print(f\"è¨“ç·´ä¸­: {config['label']}...\")\n        rewards = train_reinforce_short(env, config['lr'], config['gamma'], n_episodes=200)\n        results[config['label']] = rewards\n        print(f\"  æœ€çµ‚å¹³å‡çå‹µ: {rewards[-1]:.1f}\")\n    \n    return results\n\n# åŸ·è¡Œå¯¦é©—\nresults = experiment_hyperparameters()\n\n# è¦–è¦ºåŒ–\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# å­¸ç¿’æ›²ç·šæ¯”è¼ƒ\ncolors = plt.cm.tab10(np.linspace(0, 1, len(results)))\nfor (label, rewards), color in zip(results.items(), colors):\n    axes[0].plot(rewards, label=label, color=color, alpha=0.8)\n\naxes[0].axhline(y=200, color='gray', linestyle='--', alpha=0.5)\naxes[0].set_xlabel('Episode')\naxes[0].set_ylabel('Average Reward (window=20)')\naxes[0].set_title('Learning Curves with Different Hyperparameters')\naxes[0].legend(fontsize=8)\naxes[0].grid(True, alpha=0.3)\n\n# æœ€çµ‚è¡¨ç¾æ¯”è¼ƒ\nlabels = list(results.keys())\nfinal_rewards = [results[l][-1] for l in labels]\nbars = axes[1].barh(range(len(labels)), final_rewards, color=colors)\naxes[1].set_yticks(range(len(labels)))\naxes[1].set_yticklabels([l.split(',')[0] for l in labels], fontsize=9)\naxes[1].set_xlabel('Final Average Reward')\naxes[1].set_title('Final Performance Comparison')\n\nfor i, (bar, reward) in enumerate(zip(bars, final_rewards)):\n    axes[1].text(bar.get_width() + 2, bar.get_y() + bar.get_height()/2, \n                 f'{reward:.0f}', va='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nçµè«–ï¼š\")\nprint(\"- å­¸ç¿’ç‡å¤ªé«˜å¯èƒ½å°è‡´ä¸ç©©å®šï¼Œå¤ªä½æ”¶æ–‚æ…¢\")\nprint(\"- æŠ˜æ‰£å› å­ Î³ å½±éŸ¿å°é•·æœŸçå‹µçš„é‡è¦–ç¨‹åº¦\")\nprint(\"- CartPole é€šå¸¸ lr=1e-3, Î³=0.99 æ•ˆæœè¼ƒå¥½\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}