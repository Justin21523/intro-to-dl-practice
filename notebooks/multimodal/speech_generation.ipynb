{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Generation (語音合成)\n",
    "\n",
    "**對應課程**: 李宏毅 2025 Fall GenAI-ML HW10 - Speech Generation\n",
    "\n",
    "本 notebook 介紹語音合成（Text-to-Speech, TTS）技術：\n",
    "- **傳統方法**: 拼接合成、參數合成\n",
    "- **神經網路方法**: Tacotron、FastSpeech\n",
    "- **聲碼器 (Vocoder)**: WaveNet、HiFi-GAN\n",
    "- **端到端模型**: VITS、Bark\n",
    "\n",
    "```\n",
    "TTS 系統演進：\n",
    "\n",
    "傳統方法 (2010s前)          神經網路方法 (2016+)         端到端方法 (2020+)\n",
    "─────────────────          ──────────────────          ─────────────────\n",
    "┌──────────────┐           ┌──────────────┐           ┌──────────────┐\n",
    "│ 文字分析     │           │ 文字編碼     │           │              │\n",
    "│ (G2P, 韻律)  │           │ (Embedding)  │           │              │\n",
    "└──────┬───────┘           └──────┬───────┘           │    VITS /    │\n",
    "       │                          │                   │    Bark      │\n",
    "       ▼                          ▼                   │              │\n",
    "┌──────────────┐           ┌──────────────┐           │  Text ──►    │\n",
    "│ 單位選擇     │           │ Tacotron /   │           │   Audio      │\n",
    "│ (拼接合成)   │           │ FastSpeech   │           │              │\n",
    "└──────┬───────┘           └──────┬───────┘           │              │\n",
    "       │                          │ Mel               │              │\n",
    "       ▼                          ▼                   │              │\n",
    "┌──────────────┐           ┌──────────────┐           │              │\n",
    "│ 信號處理     │           │ WaveNet /    │           │              │\n",
    "│ (PSOLA)      │           │ HiFi-GAN     │           │              │\n",
    "└──────────────┘           └──────────────┘           └──────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: 語音基礎知識\n",
    "\n",
    "理解語音信號的基本表示方式。\n",
    "\n",
    "```\n",
    "語音信號表示：\n",
    "\n",
    "1. 波形 (Waveform)\n",
    "   - 原始音頻信號\n",
    "   - 採樣率: 16kHz, 22.05kHz, 24kHz\n",
    "   - 振幅隨時間變化\n",
    "\n",
    "2. 頻譜圖 (Spectrogram)\n",
    "   - 時頻表示\n",
    "   - STFT 計算得到\n",
    "   - 顯示頻率隨時間的變化\n",
    "\n",
    "3. 梅爾頻譜 (Mel Spectrogram)\n",
    "   - 符合人耳感知的頻率尺度\n",
    "   - 常用 80 個 mel bins\n",
    "   - TTS 系統的標準中間表示\n",
    "\n",
    "Mel 尺度轉換：\n",
    "m = 2595 * log10(1 + f/700)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    \"\"\"音頻處理工具\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_rate: int = 22050,\n",
    "        n_fft: int = 1024,\n",
    "        hop_length: int = 256,\n",
    "        n_mels: int = 80,\n",
    "        f_min: float = 0.0,\n",
    "        f_max: float = 8000.0\n",
    "    ):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_mels = n_mels\n",
    "        self.f_min = f_min\n",
    "        self.f_max = f_max\n",
    "        \n",
    "        # 創建 Mel 濾波器組\n",
    "        self.mel_filterbank = self._create_mel_filterbank()\n",
    "        \n",
    "    def _hz_to_mel(self, hz: float) -> float:\n",
    "        \"\"\"Hz 轉 Mel\"\"\"\n",
    "        return 2595 * np.log10(1 + hz / 700)\n",
    "    \n",
    "    def _mel_to_hz(self, mel: float) -> float:\n",
    "        \"\"\"Mel 轉 Hz\"\"\"\n",
    "        return 700 * (10 ** (mel / 2595) - 1)\n",
    "    \n",
    "    def _create_mel_filterbank(self) -> np.ndarray:\n",
    "        \"\"\"創建 Mel 濾波器組\"\"\"\n",
    "        # Mel 頻率點\n",
    "        mel_min = self._hz_to_mel(self.f_min)\n",
    "        mel_max = self._hz_to_mel(self.f_max)\n",
    "        mel_points = np.linspace(mel_min, mel_max, self.n_mels + 2)\n",
    "        hz_points = np.array([self._mel_to_hz(m) for m in mel_points])\n",
    "        \n",
    "        # 頻率 bin 索引\n",
    "        bin_points = np.floor((self.n_fft + 1) * hz_points / self.sample_rate).astype(int)\n",
    "        \n",
    "        # 創建濾波器\n",
    "        filterbank = np.zeros((self.n_mels, self.n_fft // 2 + 1))\n",
    "        \n",
    "        for i in range(self.n_mels):\n",
    "            left = bin_points[i]\n",
    "            center = bin_points[i + 1]\n",
    "            right = bin_points[i + 2]\n",
    "            \n",
    "            # 上升斜坡\n",
    "            for j in range(left, center):\n",
    "                if j < filterbank.shape[1]:\n",
    "                    filterbank[i, j] = (j - left) / (center - left + 1e-8)\n",
    "            # 下降斜坡\n",
    "            for j in range(center, right):\n",
    "                if j < filterbank.shape[1]:\n",
    "                    filterbank[i, j] = (right - j) / (right - center + 1e-8)\n",
    "                    \n",
    "        return filterbank\n",
    "    \n",
    "    def waveform_to_mel(self, waveform: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        波形轉梅爾頻譜\n",
    "        \n",
    "        Args:\n",
    "            waveform: 音頻波形 [T]\n",
    "            \n",
    "        Returns:\n",
    "            梅爾頻譜 [n_mels, frames]\n",
    "        \"\"\"\n",
    "        # STFT\n",
    "        window = np.hanning(self.n_fft)\n",
    "        \n",
    "        # 計算幀數\n",
    "        num_frames = 1 + (len(waveform) - self.n_fft) // self.hop_length\n",
    "        \n",
    "        # 分幀\n",
    "        frames = np.zeros((num_frames, self.n_fft))\n",
    "        for i in range(num_frames):\n",
    "            start = i * self.hop_length\n",
    "            frames[i] = waveform[start:start + self.n_fft] * window\n",
    "            \n",
    "        # FFT\n",
    "        spectrogram = np.abs(np.fft.rfft(frames, axis=1)) ** 2\n",
    "        \n",
    "        # 應用 Mel 濾波器\n",
    "        mel_spec = np.dot(spectrogram, self.mel_filterbank.T)\n",
    "        \n",
    "        # Log 壓縮\n",
    "        mel_spec = np.log(np.maximum(mel_spec, 1e-5))\n",
    "        \n",
    "        return mel_spec.T  # [n_mels, frames]\n",
    "\n",
    "\n",
    "# 示範\n",
    "processor = AudioProcessor()\n",
    "print(f\"Mel 濾波器形狀: {processor.mel_filterbank.shape}\")\n",
    "\n",
    "# 生成測試信號\n",
    "t = np.linspace(0, 1, 22050)\n",
    "test_wave = np.sin(2 * np.pi * 440 * t)  # 440 Hz 正弦波\n",
    "mel_spec = processor.waveform_to_mel(test_wave)\n",
    "print(f\"Mel 頻譜形狀: {mel_spec.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_audio_representations():\n",
    "    \"\"\"視覺化不同的音頻表示\"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "    \n",
    "    # 生成複合信號\n",
    "    sr = 22050\n",
    "    duration = 0.5\n",
    "    t = np.linspace(0, duration, int(sr * duration))\n",
    "    \n",
    "    # 模擬語音：基頻 + 諧波\n",
    "    f0 = 150  # 基頻\n",
    "    wave = np.zeros_like(t)\n",
    "    for harmonic in range(1, 6):\n",
    "        amplitude = 1.0 / harmonic\n",
    "        wave += amplitude * np.sin(2 * np.pi * f0 * harmonic * t)\n",
    "    \n",
    "    # 添加振幅包絡\n",
    "    envelope = np.concatenate([\n",
    "        np.linspace(0, 1, len(t)//4),\n",
    "        np.ones(len(t)//2),\n",
    "        np.linspace(1, 0, len(t) - len(t)//4 - len(t)//2)\n",
    "    ])\n",
    "    wave = wave * envelope\n",
    "    wave = wave / np.max(np.abs(wave))\n",
    "    \n",
    "    # 1. 波形\n",
    "    axes[0].plot(t[:2000], wave[:2000], 'b-', linewidth=0.5)\n",
    "    axes[0].set_xlabel('Time (s)')\n",
    "    axes[0].set_ylabel('Amplitude')\n",
    "    axes[0].set_title('Waveform')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. 頻譜圖\n",
    "    n_fft = 512\n",
    "    hop = 128\n",
    "    window = np.hanning(n_fft)\n",
    "    num_frames = 1 + (len(wave) - n_fft) // hop\n",
    "    \n",
    "    spec = np.zeros((n_fft // 2 + 1, num_frames))\n",
    "    for i in range(num_frames):\n",
    "        frame = wave[i * hop:i * hop + n_fft] * window\n",
    "        spec[:, i] = np.abs(np.fft.rfft(frame))\n",
    "    \n",
    "    axes[1].imshow(\n",
    "        20 * np.log10(spec + 1e-6),\n",
    "        aspect='auto',\n",
    "        origin='lower',\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    axes[1].set_xlabel('Frame')\n",
    "    axes[1].set_ylabel('Frequency Bin')\n",
    "    axes[1].set_title('Spectrogram (dB)')\n",
    "    \n",
    "    # 3. Mel 頻譜\n",
    "    processor = AudioProcessor(sample_rate=sr, n_fft=n_fft, hop_length=hop)\n",
    "    mel_spec = processor.waveform_to_mel(wave)\n",
    "    \n",
    "    axes[2].imshow(\n",
    "        mel_spec,\n",
    "        aspect='auto',\n",
    "        origin='lower',\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    axes[2].set_xlabel('Frame')\n",
    "    axes[2].set_ylabel('Mel Bin')\n",
    "    axes[2].set_title('Mel Spectrogram')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_audio_representations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tacotron 架構\n",
    "\n",
    "Tacotron 是經典的 sequence-to-sequence TTS 模型。\n",
    "\n",
    "```\n",
    "Tacotron 2 架構：\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                        Encoder                              │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│  文字輸入 \"Hello world\"                                     │\n",
    "│         │                                                   │\n",
    "│         ▼                                                   │\n",
    "│  ┌─────────────┐                                            │\n",
    "│  │ Character   │                                            │\n",
    "│  │ Embedding   │                                            │\n",
    "│  └──────┬──────┘                                            │\n",
    "│         │                                                   │\n",
    "│         ▼                                                   │\n",
    "│  ┌─────────────┐                                            │\n",
    "│  │ 3 Conv + BN │  (5×1 kernels)                            │\n",
    "│  └──────┬──────┘                                            │\n",
    "│         │                                                   │\n",
    "│         ▼                                                   │\n",
    "│  ┌─────────────┐                                            │\n",
    "│  │ BiLSTM      │                                            │\n",
    "│  └──────┬──────┘                                            │\n",
    "│         │                                                   │\n",
    "│         ▼ Encoder outputs                                   │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                    │\n",
    "                    ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                        Decoder                              │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  Previous mel frame ──► PreNet ──┐                         │\n",
    "│                                   │                         │\n",
    "│  Encoder outputs ──► Attention ◄──┘                         │\n",
    "│                          │                                  │\n",
    "│                          ▼                                  │\n",
    "│                    ┌──────────┐                             │\n",
    "│                    │ 2 LSTM   │                             │\n",
    "│                    └────┬─────┘                             │\n",
    "│                         │                                   │\n",
    "│              ┌──────────┴──────────┐                        │\n",
    "│              ▼                     ▼                        │\n",
    "│        ┌──────────┐          ┌──────────┐                   │\n",
    "│        │ Linear   │          │ Linear   │                   │\n",
    "│        │ (mel)    │          │ (stop)   │                   │\n",
    "│        └────┬─────┘          └──────────┘                   │\n",
    "│             │                                               │\n",
    "│             ▼                                               │\n",
    "│        PostNet (5 Conv)                                     │\n",
    "│             │                                               │\n",
    "│             ▼                                               │\n",
    "│        Mel Spectrogram                                      │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TacotronEncoder(nn.Module):\n",
    "    \"\"\"Tacotron 2 編碼器\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 100,\n",
    "        embedding_dim: int = 512,\n",
    "        encoder_dim: int = 512,\n",
    "        num_conv_layers: int = 3,\n",
    "        conv_kernel_size: int = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 字符嵌入\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 卷積層\n",
    "        conv_layers = []\n",
    "        for i in range(num_conv_layers):\n",
    "            in_channels = embedding_dim if i == 0 else encoder_dim\n",
    "            conv_layers.extend([\n",
    "                nn.Conv1d(\n",
    "                    in_channels, encoder_dim,\n",
    "                    kernel_size=conv_kernel_size,\n",
    "                    padding=conv_kernel_size // 2\n",
    "                ),\n",
    "                nn.BatchNorm1d(encoder_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5)\n",
    "            ])\n",
    "        self.convolutions = nn.Sequential(*conv_layers)\n",
    "        \n",
    "        # BiLSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            encoder_dim,\n",
    "            encoder_dim // 2,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, text: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text: 文字序列 [batch, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            編碼器輸出 [batch, seq_len, encoder_dim]\n",
    "        \"\"\"\n",
    "        # 嵌入\n",
    "        x = self.embedding(text)  # [batch, seq_len, embed_dim]\n",
    "        \n",
    "        # 卷積（需要轉置）\n",
    "        x = x.transpose(1, 2)  # [batch, embed_dim, seq_len]\n",
    "        x = self.convolutions(x)\n",
    "        x = x.transpose(1, 2)  # [batch, seq_len, encoder_dim]\n",
    "        \n",
    "        # BiLSTM\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# 測試\n",
    "encoder = TacotronEncoder()\n",
    "test_text = torch.randint(0, 100, (2, 20))  # batch=2, seq_len=20\n",
    "encoder_output = encoder(test_text)\n",
    "print(f\"編碼器輸出形狀: {encoder_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationSensitiveAttention(nn.Module):\n",
    "    \"\"\"位置敏感注意力（Tacotron 2 使用）\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        attention_dim: int = 128,\n",
    "        encoder_dim: int = 512,\n",
    "        decoder_dim: int = 1024,\n",
    "        attention_location_n_filters: int = 32,\n",
    "        attention_location_kernel_size: int = 31\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 查詢投影\n",
    "        self.query_layer = nn.Linear(decoder_dim, attention_dim, bias=False)\n",
    "        \n",
    "        # 記憶投影\n",
    "        self.memory_layer = nn.Linear(encoder_dim, attention_dim, bias=False)\n",
    "        \n",
    "        # 位置卷積\n",
    "        self.location_conv = nn.Conv1d(\n",
    "            2, attention_location_n_filters,\n",
    "            kernel_size=attention_location_kernel_size,\n",
    "            padding=attention_location_kernel_size // 2\n",
    "        )\n",
    "        self.location_dense = nn.Linear(attention_location_n_filters, attention_dim, bias=False)\n",
    "        \n",
    "        # 能量計算\n",
    "        self.v = nn.Linear(attention_dim, 1, bias=False)\n",
    "        self.score_mask_value = -float('inf')\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        memory: torch.Tensor,\n",
    "        processed_memory: torch.Tensor,\n",
    "        attention_weights_cat: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: 解碼器狀態 [batch, decoder_dim]\n",
    "            memory: 編碼器輸出 [batch, seq_len, encoder_dim]\n",
    "            processed_memory: 預處理的記憶 [batch, seq_len, attention_dim]\n",
    "            attention_weights_cat: 之前的注意力權重 [batch, 2, seq_len]\n",
    "            mask: 填充遮罩\n",
    "            \n",
    "        Returns:\n",
    "            context: 上下文向量 [batch, encoder_dim]\n",
    "            attention_weights: 注意力權重 [batch, seq_len]\n",
    "        \"\"\"\n",
    "        # 處理查詢\n",
    "        processed_query = self.query_layer(query.unsqueeze(1))  # [batch, 1, attention_dim]\n",
    "        \n",
    "        # 處理位置特徵\n",
    "        processed_location = self.location_conv(attention_weights_cat)  # [batch, filters, seq_len]\n",
    "        processed_location = processed_location.transpose(1, 2)  # [batch, seq_len, filters]\n",
    "        processed_location = self.location_dense(processed_location)  # [batch, seq_len, attention_dim]\n",
    "        \n",
    "        # 計算能量\n",
    "        energies = self.v(torch.tanh(\n",
    "            processed_query + processed_memory + processed_location\n",
    "        )).squeeze(-1)  # [batch, seq_len]\n",
    "        \n",
    "        # 應用遮罩\n",
    "        if mask is not None:\n",
    "            energies.masked_fill_(mask, self.score_mask_value)\n",
    "            \n",
    "        # 計算注意力權重\n",
    "        attention_weights = F.softmax(energies, dim=1)\n",
    "        \n",
    "        # 計算上下文\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), memory).squeeze(1)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "\n",
    "# 測試\n",
    "attention = LocationSensitiveAttention()\n",
    "query = torch.randn(2, 1024)\n",
    "memory = torch.randn(2, 20, 512)\n",
    "processed_memory = torch.randn(2, 20, 128)\n",
    "attn_weights_cat = torch.randn(2, 2, 20)\n",
    "\n",
    "context, weights = attention(query, memory, processed_memory, attn_weights_cat)\n",
    "print(f\"上下文形狀: {context.shape}\")\n",
    "print(f\"注意力權重形狀: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNet(nn.Module):\n",
    "    \"\"\"Tacotron PreNet：對先前的 mel 幀進行處理\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim: int = 80, hidden_dim: int = 256, out_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # 注意：推理時也使用 dropout\n",
    "            nn.Linear(hidden_dim, out_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class PostNet(nn.Module):\n",
    "    \"\"\"Tacotron PostNet：改善 mel 頻譜品質\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_mels: int = 80,\n",
    "        postnet_dim: int = 512,\n",
    "        postnet_kernel_size: int = 5,\n",
    "        num_layers: int = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            in_channels = n_mels if i == 0 else postnet_dim\n",
    "            out_channels = n_mels if i == num_layers - 1 else postnet_dim\n",
    "            \n",
    "            layers.extend([\n",
    "                nn.Conv1d(\n",
    "                    in_channels, out_channels,\n",
    "                    kernel_size=postnet_kernel_size,\n",
    "                    padding=postnet_kernel_size // 2\n",
    "                ),\n",
    "                nn.BatchNorm1d(out_channels)\n",
    "            ])\n",
    "            \n",
    "            if i < num_layers - 1:\n",
    "                layers.append(nn.Tanh())\n",
    "            layers.append(nn.Dropout(0.5))\n",
    "            \n",
    "        self.convolutions = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: mel 頻譜 [batch, n_mels, frames]\n",
    "        Returns:\n",
    "            殘差 [batch, n_mels, frames]\n",
    "        \"\"\"\n",
    "        return self.convolutions(x)\n",
    "\n",
    "\n",
    "# 測試\n",
    "prenet = PreNet()\n",
    "postnet = PostNet()\n",
    "\n",
    "mel_frame = torch.randn(2, 80)\n",
    "print(f\"PreNet 輸出: {prenet(mel_frame).shape}\")\n",
    "\n",
    "mel_spec = torch.randn(2, 80, 100)\n",
    "print(f\"PostNet 輸出: {postnet(mel_spec).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: FastSpeech 架構\n",
    "\n",
    "FastSpeech 使用非自回歸架構，實現快速並行合成。\n",
    "\n",
    "```\n",
    "FastSpeech 2 架構：\n",
    "\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                     Phoneme Encoder                          │\n",
    "│  ┌─────────────────────────────────────────────────────┐    │\n",
    "│  │  Phoneme Embedding + Positional Encoding            │    │\n",
    "│  │              │                                      │    │\n",
    "│  │              ▼                                      │    │\n",
    "│  │  FFT Block × N (Self-Attention + Conv FFN)          │    │\n",
    "│  └─────────────────────────────────────────────────────┘    │\n",
    "└────────────────────────┬─────────────────────────────────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                   Variance Adaptor                           │\n",
    "│  ┌────────────────────────────────────────────────────────┐ │\n",
    "│  │                                                        │ │\n",
    "│  │  ┌──────────────┐ ┌──────────────┐ ┌──────────────┐   │ │\n",
    "│  │  │   Duration   │ │    Pitch     │ │   Energy     │   │ │\n",
    "│  │  │  Predictor   │ │  Predictor   │ │  Predictor   │   │ │\n",
    "│  │  └──────┬───────┘ └──────┬───────┘ └──────┬───────┘   │ │\n",
    "│  │         │                │                │           │ │\n",
    "│  │         ▼                ▼                ▼           │ │\n",
    "│  │  Length Regulator   Pitch Embed     Energy Embed      │ │\n",
    "│  │         │                │                │           │ │\n",
    "│  │         └────────────────┴────────────────┘           │ │\n",
    "│  │                          │                             │ │\n",
    "│  │                          ▼                             │ │\n",
    "│  │                       Add All                          │ │\n",
    "│  └────────────────────────────────────────────────────────┘ │\n",
    "└────────────────────────┬─────────────────────────────────────┘\n",
    "                         │\n",
    "                         ▼\n",
    "┌──────────────────────────────────────────────────────────────┐\n",
    "│                      Mel Decoder                             │\n",
    "│  ┌─────────────────────────────────────────────────────┐    │\n",
    "│  │  FFT Block × N (Self-Attention + Conv FFN)          │    │\n",
    "│  │              │                                      │    │\n",
    "│  │              ▼                                      │    │\n",
    "│  │         Linear → Mel Spectrogram                    │    │\n",
    "│  └─────────────────────────────────────────────────────┘    │\n",
    "└──────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFTBlock(nn.Module):\n",
    "    \"\"\"Feed-Forward Transformer Block\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 256,\n",
    "        num_heads: int = 2,\n",
    "        d_ff: int = 1024,\n",
    "        kernel_size: int = 9,\n",
    "        dropout: float = 0.2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            d_model, num_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Convolutional FFN\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            d_model, d_ff, kernel_size,\n",
    "            padding=kernel_size // 2\n",
    "        )\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            d_ff, d_model, kernel_size,\n",
    "            padding=kernel_size // 2\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 輸入 [batch, seq_len, d_model]\n",
    "            mask: 注意力遮罩\n",
    "        \"\"\"\n",
    "        # Self-Attention\n",
    "        residual = x\n",
    "        x, _ = self.self_attention(x, x, x, key_padding_mask=mask)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm1(residual + x)\n",
    "        \n",
    "        # Conv FFN\n",
    "        residual = x\n",
    "        x = x.transpose(1, 2)  # [batch, d_model, seq_len]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.transpose(1, 2)  # [batch, seq_len, d_model]\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm2(residual + x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# 測試\n",
    "fft_block = FFTBlock()\n",
    "x = torch.randn(2, 20, 256)\n",
    "out = fft_block(x)\n",
    "print(f\"FFT Block 輸出形狀: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariancePredictor(nn.Module):\n",
    "    \"\"\"變異預測器：預測 duration/pitch/energy\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 256,\n",
    "        d_hidden: int = 256,\n",
    "        kernel_size: int = 3,\n",
    "        dropout: float = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(d_model, d_hidden, kernel_size, padding=kernel_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(d_hidden),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(d_hidden, d_hidden, kernel_size, padding=kernel_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(d_hidden),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.linear = nn.Linear(d_hidden, 1)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, d_model]\n",
    "        Returns:\n",
    "            預測值 [batch, seq_len]\n",
    "        \"\"\"\n",
    "        x = x.transpose(1, 2)  # [batch, d_model, seq_len]\n",
    "        x = self.layers(x)\n",
    "        x = x.transpose(1, 2)  # [batch, seq_len, d_hidden]\n",
    "        x = self.linear(x).squeeze(-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LengthRegulator(nn.Module):\n",
    "    \"\"\"長度調節器：根據 duration 擴展序列\"\"\"\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        durations: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 編碼器輸出 [batch, seq_len, d_model]\n",
    "            durations: 每個 phoneme 的持續時間 [batch, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            擴展後的序列 [batch, mel_len, d_model]\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        for i in range(x.shape[0]):\n",
    "            output = []\n",
    "            for j in range(x.shape[1]):\n",
    "                duration = int(durations[i, j].item())\n",
    "                if duration > 0:\n",
    "                    output.append(x[i, j].unsqueeze(0).expand(duration, -1))\n",
    "            if output:\n",
    "                outputs.append(torch.cat(output, dim=0))\n",
    "            else:\n",
    "                outputs.append(x[i, :1])  # 至少保留一幀\n",
    "                \n",
    "        # 填充到相同長度\n",
    "        max_len = max(o.shape[0] for o in outputs)\n",
    "        padded = torch.zeros(len(outputs), max_len, x.shape[-1], device=x.device)\n",
    "        for i, output in enumerate(outputs):\n",
    "            padded[i, :output.shape[0]] = output\n",
    "            \n",
    "        return padded\n",
    "\n",
    "\n",
    "# 測試\n",
    "var_predictor = VariancePredictor()\n",
    "length_reg = LengthRegulator()\n",
    "\n",
    "x = torch.randn(2, 10, 256)  # 10 個 phonemes\n",
    "durations = torch.tensor([[2, 3, 1, 2, 2, 1, 3, 2, 2, 2],\n",
    "                          [3, 2, 2, 1, 3, 2, 2, 1, 2, 2]])\n",
    "\n",
    "pred_dur = var_predictor(x)\n",
    "print(f\"預測的 duration 形狀: {pred_dur.shape}\")\n",
    "\n",
    "expanded = length_reg(x, durations)\n",
    "print(f\"擴展後序列形狀: {expanded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: 聲碼器 (Vocoder)\n",
    "\n",
    "聲碼器將 mel 頻譜轉換為音頻波形。\n",
    "\n",
    "```\n",
    "聲碼器發展：\n",
    "\n",
    "WaveNet (2016)         WaveRNN (2018)         HiFi-GAN (2020)\n",
    "─────────────          ──────────────          ────────────────\n",
    "自回歸                 自回歸                 非自回歸\n",
    "Dilated Conv           GRU + Sparse           GAN-based\n",
    "高品質                 較快                   最快\n",
    "極慢                   中等                   實時\n",
    "\n",
    "HiFi-GAN 架構：\n",
    "\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                        Generator                            │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│  Mel Spectrogram                                            │\n",
    "│       │                                                     │\n",
    "│       ▼                                                     │\n",
    "│  ┌─────────────┐                                            │\n",
    "│  │   Conv1d    │                                            │\n",
    "│  └──────┬──────┘                                            │\n",
    "│         │                                                   │\n",
    "│         ▼                                                   │\n",
    "│  ┌─────────────────────────────────────────────────┐       │\n",
    "│  │           Upsample Block × 4                    │       │\n",
    "│  │  ┌───────────────────────────────────────┐      │       │\n",
    "│  │  │ TransposeConv1d (upsample)            │      │       │\n",
    "│  │  │         │                             │      │       │\n",
    "│  │  │         ▼                             │      │       │\n",
    "│  │  │ Multi-Receptive Field Fusion (MRF)   │      │       │\n",
    "│  │  │  ├─ ResBlock (kernel=3, dilation=1)  │      │       │\n",
    "│  │  │  ├─ ResBlock (kernel=7, dilation=1)  │      │       │\n",
    "│  │  │  └─ ResBlock (kernel=11, dilation=1) │      │       │\n",
    "│  │  └───────────────────────────────────────┘      │       │\n",
    "│  └─────────────────────────────────────────────────┘       │\n",
    "│         │                                                   │\n",
    "│         ▼                                                   │\n",
    "│  ┌─────────────┐                                            │\n",
    "│  │   Conv1d    │                                            │\n",
    "│  │   + tanh    │                                            │\n",
    "│  └──────┬──────┘                                            │\n",
    "│         │                                                   │\n",
    "│         ▼                                                   │\n",
    "│    Waveform                                                 │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"HiFi-GAN 殘差塊\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        dilations: List[int] = [1, 3, 5]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convs1 = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                channels, channels, kernel_size,\n",
    "                dilation=d, padding=d * (kernel_size - 1) // 2\n",
    "            ) for d in dilations\n",
    "        ])\n",
    "        \n",
    "        self.convs2 = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                channels, channels, kernel_size,\n",
    "                dilation=1, padding=(kernel_size - 1) // 2\n",
    "            ) for _ in dilations\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for conv1, conv2 in zip(self.convs1, self.convs2):\n",
    "            residual = x\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            x = conv1(x)\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            x = conv2(x)\n",
    "            x = x + residual\n",
    "        return x\n",
    "\n",
    "\n",
    "class MRF(nn.Module):\n",
    "    \"\"\"Multi-Receptive Field Fusion\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        channels: int,\n",
    "        kernel_sizes: List[int] = [3, 7, 11],\n",
    "        dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.resblocks = nn.ModuleList([\n",
    "            ResBlock(channels, k, d)\n",
    "            for k, d in zip(kernel_sizes, dilations)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = [block(x) for block in self.resblocks]\n",
    "        return sum(outputs) / len(outputs)\n",
    "\n",
    "\n",
    "class HiFiGANGenerator(nn.Module):\n",
    "    \"\"\"簡化版 HiFi-GAN 生成器\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_mels: int = 80,\n",
    "        upsample_initial_channel: int = 512,\n",
    "        upsample_rates: List[int] = [8, 8, 2, 2],\n",
    "        upsample_kernel_sizes: List[int] = [16, 16, 4, 4],\n",
    "        resblock_kernel_sizes: List[int] = [3, 7, 11],\n",
    "        resblock_dilations: List[List[int]] = [[1, 3, 5], [1, 3, 5], [1, 3, 5]]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 初始卷積\n",
    "        self.conv_pre = nn.Conv1d(n_mels, upsample_initial_channel, 7, padding=3)\n",
    "        \n",
    "        # 上採樣層\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.mrfs = nn.ModuleList()\n",
    "        \n",
    "        ch = upsample_initial_channel\n",
    "        for i, (u_rate, u_kernel) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose1d(\n",
    "                    ch, ch // 2, u_kernel,\n",
    "                    stride=u_rate, padding=(u_kernel - u_rate) // 2\n",
    "                )\n",
    "            )\n",
    "            self.mrfs.append(\n",
    "                MRF(ch // 2, resblock_kernel_sizes, resblock_dilations)\n",
    "            )\n",
    "            ch = ch // 2\n",
    "            \n",
    "        # 最終卷積\n",
    "        self.conv_post = nn.Conv1d(ch, 1, 7, padding=3)\n",
    "        \n",
    "    def forward(self, mel: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mel: Mel 頻譜 [batch, n_mels, frames]\n",
    "            \n",
    "        Returns:\n",
    "            波形 [batch, 1, samples]\n",
    "        \"\"\"\n",
    "        x = self.conv_pre(mel)\n",
    "        \n",
    "        for up, mrf in zip(self.ups, self.mrfs):\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            x = up(x)\n",
    "            x = mrf(x)\n",
    "            \n",
    "        x = F.leaky_relu(x, 0.1)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# 測試\n",
    "vocoder = HiFiGANGenerator()\n",
    "mel = torch.randn(2, 80, 100)  # 100 frames\n",
    "waveform = vocoder(mel)\n",
    "\n",
    "print(f\"Mel 輸入形狀: {mel.shape}\")\n",
    "print(f\"波形輸出形狀: {waveform.shape}\")\n",
    "print(f\"上採樣倍率: {waveform.shape[-1] / mel.shape[-1]}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: 現代端到端模型\n",
    "\n",
    "介紹 VITS 和 Bark 等端到端語音合成模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_modern_tts_architectures():\n",
    "    \"\"\"印出現代 TTS 架構比較\"\"\"\n",
    "    print(\"\"\"\n",
    "╔═══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                        現代端到端 TTS 模型比較                                ║\n",
    "╠════════════════╦══════════════════════╦══════════════════════════════════════╣\n",
    "║     模型       ║        特點          ║              適用場景                ║\n",
    "╠════════════════╬══════════════════════╬══════════════════════════════════════╣\n",
    "║                ║ - VAE + Flow + GAN   ║                                      ║\n",
    "║     VITS       ║ - 端到端訓練         ║ 高品質單說話人/多說話人 TTS          ║\n",
    "║  (2021)        ║ - 無需額外 vocoder   ║ 需要自然語調的應用                   ║\n",
    "║                ║ - 支援多說話人       ║ 實時合成                             ║\n",
    "╠════════════════╬══════════════════════╬══════════════════════════════════════╣\n",
    "║                ║ - GPT-style 自回歸   ║                                      ║\n",
    "║     Bark       ║ - 支援多語言         ║ 多語言、多情感語音合成               ║\n",
    "║  (2023)        ║ - 可控制情感/音效    ║ 播客、有聲書、創意內容               ║\n",
    "║                ║ - 開源可用           ║ 需要表現力的應用                     ║\n",
    "╠════════════════╬══════════════════════╬══════════════════════════════════════╣\n",
    "║                ║ - 超長上下文         ║                                      ║\n",
    "║   Tortoise     ║ - CLIP 引導          ║ 最高品質、離線處理                   ║\n",
    "║   TTS          ║ - 高品質但較慢       ║ 後製配音、高端應用                   ║\n",
    "║                ║ - 可複製任意聲音     ║ 聲音複製                             ║\n",
    "╠════════════════╬══════════════════════╬══════════════════════════════════════╣\n",
    "║                ║ - Neural Codec       ║                                      ║\n",
    "║    VALL-E      ║ - Zero-shot 複製     ║ 零樣本語音複製                       ║\n",
    "║  (Microsoft)   ║ - 僅需 3 秒樣本      ║ 個人化 TTS                           ║\n",
    "║                ║ - 保留說話風格       ║ 語音轉換                             ║\n",
    "╚════════════════╩══════════════════════╩══════════════════════════════════════╝\n",
    "\n",
    "VITS 架構概覽：\n",
    "┌──────────────────────────────────────────────────────────────────────────────┐\n",
    "│                                                                              │\n",
    "│   Text ──► Text Encoder ──► Stochastic Duration Predictor                   │\n",
    "│                │                                                             │\n",
    "│                ▼                                                             │\n",
    "│         Normalizing Flow                                                     │\n",
    "│                │                                                             │\n",
    "│                ▼                                                             │\n",
    "│          VAE Decoder ──► Waveform                                            │\n",
    "│                                                                              │\n",
    "│   訓練時額外有：                                                              │\n",
    "│   - Posterior Encoder (從音頻學習隱空間)                                     │\n",
    "│   - GAN Discriminator (對抗學習)                                             │\n",
    "│                                                                              │\n",
    "└──────────────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Bark 架構概覽：\n",
    "┌──────────────────────────────────────────────────────────────────────────────┐\n",
    "│                                                                              │\n",
    "│   Text ──► GPT-style Text Model ──► Semantic Tokens                         │\n",
    "│                                           │                                  │\n",
    "│                                           ▼                                  │\n",
    "│                                    Coarse Acoustic Model ──► Coarse Tokens   │\n",
    "│                                                                │             │\n",
    "│                                                                ▼             │\n",
    "│                                    Fine Acoustic Model ──► Fine Tokens       │\n",
    "│                                                                │             │\n",
    "│                                                                ▼             │\n",
    "│                                           Encodec Decoder ──► Waveform       │\n",
    "│                                                                              │\n",
    "└──────────────────────────────────────────────────────────────────────────────┘\n",
    "\"\"\")\n",
    "\n",
    "print_modern_tts_architectures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Bark 的範例程式碼\n",
    "bark_example = '''\n",
    "# 安裝\n",
    "# pip install git+https://github.com/suno-ai/bark.git\n",
    "\n",
    "from bark import SAMPLE_RATE, generate_audio, preload_models\n",
    "from scipy.io.wavfile import write as write_wav\n",
    "\n",
    "# 預載入模型\n",
    "preload_models()\n",
    "\n",
    "# 基本文字轉語音\n",
    "text = \"Hello, my name is Bark. I can speak in many languages!\"\n",
    "audio_array = generate_audio(text)\n",
    "write_wav(\"bark_output.wav\", SAMPLE_RATE, audio_array)\n",
    "\n",
    "# 帶情感標記的語音\n",
    "text_with_emotion = \"\"\"\n",
    "    [laughs] Oh wow, that's hilarious!\n",
    "    [sighs] But sometimes I feel a bit sad...\n",
    "    [clears throat] Anyway, let me continue.\n",
    "\"\"\"\n",
    "audio_array = generate_audio(text_with_emotion)\n",
    "\n",
    "# 多語言支援\n",
    "chinese_text = \"你好，我是 Bark。我可以說中文！\"\n",
    "audio_array = generate_audio(chinese_text)\n",
    "\n",
    "# 使用特定說話人\n",
    "audio_array = generate_audio(\n",
    "    text,\n",
    "    history_prompt=\"v2/en_speaker_6\"  # 預設說話人\n",
    ")\n",
    "\n",
    "# 生成音樂音效\n",
    "music_text = \"♪ La la la, singing a song ♪\"\n",
    "audio_array = generate_audio(music_text)\n",
    "'''\n",
    "print(\"=== Bark 使用範例 ===\")\n",
    "print(bark_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 VITS / Coqui TTS 的範例\n",
    "vits_example = '''\n",
    "# 安裝\n",
    "# pip install TTS\n",
    "\n",
    "from TTS.api import TTS\n",
    "import torch\n",
    "\n",
    "# 列出可用模型\n",
    "print(TTS().list_models())\n",
    "\n",
    "# 載入 VITS 模型\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tts = TTS(\"tts_models/en/ljspeech/vits\").to(device)\n",
    "\n",
    "# 基本文字轉語音\n",
    "tts.tts_to_file(\n",
    "    text=\"Hello world, this is a test of VITS text to speech.\",\n",
    "    file_path=\"vits_output.wav\"\n",
    ")\n",
    "\n",
    "# 多說話人模型\n",
    "tts = TTS(\"tts_models/en/vctk/vits\").to(device)\n",
    "tts.tts_to_file(\n",
    "    text=\"Hello world!\",\n",
    "    speaker=\"p225\",  # VCTK 說話人 ID\n",
    "    file_path=\"vits_speaker.wav\"\n",
    ")\n",
    "\n",
    "# 聲音複製\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
    "tts.tts_to_file(\n",
    "    text=\"This is a voice cloning test.\",\n",
    "    speaker_wav=\"reference_audio.wav\",  # 參考音頻\n",
    "    language=\"en\",\n",
    "    file_path=\"cloned_output.wav\"\n",
    ")\n",
    "\n",
    "# 中文 TTS\n",
    "tts = TTS(\"tts_models/zh-CN/baker/tacotron2-DDC-GST\").to(device)\n",
    "tts.tts_to_file(\n",
    "    text=\"你好，這是中文語音合成測試。\",\n",
    "    file_path=\"chinese_output.wav\"\n",
    ")\n",
    "'''\n",
    "print(\"=== VITS (Coqui TTS) 使用範例 ===\")\n",
    "print(vits_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: 評估指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTSEvaluator:\n",
    "    \"\"\"TTS 評估指標計算\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def mel_cepstral_distortion(\n",
    "        generated_mel: np.ndarray,\n",
    "        reference_mel: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        計算 Mel Cepstral Distortion (MCD)\n",
    "        越低越好，通常 < 8 dB 為可接受品質\n",
    "        \n",
    "        Args:\n",
    "            generated_mel: 生成的 mel 頻譜 [n_mels, frames]\n",
    "            reference_mel: 參考的 mel 頻譜\n",
    "        \"\"\"\n",
    "        # 對齊長度\n",
    "        min_len = min(generated_mel.shape[1], reference_mel.shape[1])\n",
    "        gen = generated_mel[:, :min_len]\n",
    "        ref = reference_mel[:, :min_len]\n",
    "        \n",
    "        # 計算 MCD\n",
    "        diff = gen - ref\n",
    "        mcd = np.mean(np.sqrt(np.sum(diff ** 2, axis=0))) * (10 / np.log(10))\n",
    "        \n",
    "        return mcd\n",
    "    \n",
    "    @staticmethod\n",
    "    def f0_rmse(\n",
    "        generated_f0: np.ndarray,\n",
    "        reference_f0: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        計算基頻 RMSE\n",
    "        \n",
    "        Args:\n",
    "            generated_f0: 生成的 F0 序列\n",
    "            reference_f0: 參考的 F0 序列\n",
    "        \"\"\"\n",
    "        # 對齊長度\n",
    "        min_len = min(len(generated_f0), len(reference_f0))\n",
    "        gen = generated_f0[:min_len]\n",
    "        ref = reference_f0[:min_len]\n",
    "        \n",
    "        # 只計算有聲段\n",
    "        voiced_mask = (gen > 0) & (ref > 0)\n",
    "        if np.sum(voiced_mask) == 0:\n",
    "            return float('inf')\n",
    "            \n",
    "        rmse = np.sqrt(np.mean((gen[voiced_mask] - ref[voiced_mask]) ** 2))\n",
    "        return rmse\n",
    "    \n",
    "    @staticmethod\n",
    "    def character_error_rate(\n",
    "        generated_text: str,\n",
    "        reference_text: str\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        計算 Character Error Rate (CER)\n",
    "        用於評估可懂度（需配合 ASR）\n",
    "        \"\"\"\n",
    "        # 簡化的編輯距離計算\n",
    "        m, n = len(reference_text), len(generated_text)\n",
    "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "        \n",
    "        for i in range(m + 1):\n",
    "            dp[i][0] = i\n",
    "        for j in range(n + 1):\n",
    "            dp[0][j] = j\n",
    "            \n",
    "        for i in range(1, m + 1):\n",
    "            for j in range(1, n + 1):\n",
    "                if reference_text[i-1] == generated_text[j-1]:\n",
    "                    dp[i][j] = dp[i-1][j-1]\n",
    "                else:\n",
    "                    dp[i][j] = min(\n",
    "                        dp[i-1][j] + 1,    # 刪除\n",
    "                        dp[i][j-1] + 1,    # 插入\n",
    "                        dp[i-1][j-1] + 1   # 替換\n",
    "                    )\n",
    "                    \n",
    "        return dp[m][n] / m if m > 0 else 0.0\n",
    "\n",
    "\n",
    "# 測試\n",
    "evaluator = TTSEvaluator()\n",
    "\n",
    "# 模擬數據\n",
    "gen_mel = np.random.randn(80, 100)\n",
    "ref_mel = gen_mel + np.random.randn(80, 100) * 0.1  # 加入小擾動\n",
    "\n",
    "mcd = evaluator.mel_cepstral_distortion(gen_mel, ref_mel)\n",
    "print(f\"MCD: {mcd:.2f} dB\")\n",
    "\n",
    "cer = evaluator.character_error_rate(\"hello world\", \"helo wrld\")\n",
    "print(f\"CER: {cer:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_summary():\n",
    "    \"\"\"印出 TTS 評估指標總結\"\"\"\n",
    "    print(\"\"\"\n",
    "╔═══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                          TTS 評估指標總結                                     ║\n",
    "╠══════════════════════╦════════════════════════════════════════════════════════╣\n",
    "║       指標           ║                    說明                                ║\n",
    "╠══════════════════════╬════════════════════════════════════════════════════════╣\n",
    "║ MCD (Mel Cepstral    ║ 客觀指標，衡量頻譜相似度                               ║\n",
    "║     Distortion)      ║ 越低越好，< 8 dB 通常可接受                           ║\n",
    "╠══════════════════════╬════════════════════════════════════════════════════════╣\n",
    "║ F0 RMSE              ║ 衡量基頻（音高）的準確性                               ║\n",
    "║                      ║ 越低越好，影響語調自然度                               ║\n",
    "╠══════════════════════╬════════════════════════════════════════════════════════╣\n",
    "║ MOS (Mean Opinion    ║ 主觀指標，人工評分 1-5 分                              ║\n",
    "║      Score)          ║ 4.0+ 為高品質，4.5+ 接近真人                           ║\n",
    "╠══════════════════════╬════════════════════════════════════════════════════════╣\n",
    "║ CER/WER              ║ 可懂度指標（配合 ASR）                                 ║\n",
    "║                      ║ 越低越好                                               ║\n",
    "╠══════════════════════╬════════════════════════════════════════════════════════╣\n",
    "║ RTF (Real-Time       ║ 合成速度指標                                           ║\n",
    "║      Factor)         ║ < 1.0 表示實時合成                                     ║\n",
    "╠══════════════════════╬════════════════════════════════════════════════════════╣\n",
    "║ Speaker Similarity   ║ 說話人相似度（用於聲音複製）                           ║\n",
    "║                      ║ 使用說話人嵌入的餘弦相似度                             ║\n",
    "╚══════════════════════╩════════════════════════════════════════════════════════╝\n",
    "\n",
    "典型 MOS 分數參考：\n",
    "┌────────────────────────────────────────────────────────────┐\n",
    "│  系統               │ MOS                                 │\n",
    "├────────────────────────────────────────────────────────────┤\n",
    "│  Ground Truth       │ ~4.5                                │\n",
    "│  VITS               │ ~4.3                                │\n",
    "│  FastSpeech 2       │ ~4.0                                │\n",
    "│  Tacotron 2         │ ~3.9                                │\n",
    "│  傳統拼接合成        │ ~3.0                                │\n",
    "└────────────────────────────────────────────────────────────┘\n",
    "\"\"\")\n",
    "\n",
    "print_evaluation_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習\n",
    "\n",
    "### Exercise 1: 實作簡單的 G2P (Grapheme-to-Phoneme)\n",
    "\n",
    "實作一個簡單的英文文字轉音素函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleG2P:\n",
    "    \"\"\"簡單的 Grapheme-to-Phoneme 轉換\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 基本的字母到音素映射（簡化版）\n",
    "        self.letter_to_phoneme = {\n",
    "            'a': 'AH', 'b': 'B', 'c': 'K', 'd': 'D', 'e': 'EH',\n",
    "            'f': 'F', 'g': 'G', 'h': 'HH', 'i': 'IH', 'j': 'JH',\n",
    "            'k': 'K', 'l': 'L', 'm': 'M', 'n': 'N', 'o': 'OW',\n",
    "            'p': 'P', 'q': 'K', 'r': 'R', 's': 'S', 't': 'T',\n",
    "            'u': 'UH', 'v': 'V', 'w': 'W', 'x': 'K S', 'y': 'Y',\n",
    "            'z': 'Z', ' ': 'SIL'\n",
    "        }\n",
    "        \n",
    "        # 常見的字母組合\n",
    "        self.digraph_to_phoneme = {\n",
    "            'th': 'TH', 'sh': 'SH', 'ch': 'CH', 'ph': 'F',\n",
    "            'wh': 'W', 'ng': 'NG', 'ck': 'K', 'gh': 'G'\n",
    "        }\n",
    "        \n",
    "    def convert(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        將文字轉換為音素序列\n",
    "        \n",
    "        Args:\n",
    "            text: 輸入文字\n",
    "            \n",
    "        Returns:\n",
    "            音素列表\n",
    "        \"\"\"\n",
    "        # TODO: 實作 G2P 轉換\n",
    "        # 提示：\n",
    "        # 1. 轉換為小寫\n",
    "        # 2. 先檢查 digraph\n",
    "        # 3. 再檢查單字母\n",
    "        pass\n",
    "\n",
    "# 測試\n",
    "# g2p = SimpleG2P()\n",
    "# phonemes = g2p.convert(\"hello world\")\n",
    "# print(phonemes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: 實作 Duration 預測器訓練\n",
    "\n",
    "實作一個簡單的 Duration 預測器並訓練它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DurationPredictor(nn.Module):\n",
    "    \"\"\"Duration 預測器\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int = 256):\n",
    "        super().__init__()\n",
    "        # TODO: 定義網路架構\n",
    "        # 提示：使用卷積 + LayerNorm + ReLU\n",
    "        pass\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: 實作前向傳播\n",
    "        pass\n",
    "\n",
    "\n",
    "def train_duration_predictor(\n",
    "    model: nn.Module,\n",
    "    train_data: List[Tuple[torch.Tensor, torch.Tensor]],\n",
    "    num_epochs: int = 10,\n",
    "    lr: float = 1e-3\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    訓練 Duration 預測器\n",
    "    \n",
    "    Args:\n",
    "        model: Duration 預測器\n",
    "        train_data: [(encoder_output, target_duration), ...]\n",
    "        num_epochs: 訓練輪數\n",
    "        lr: 學習率\n",
    "        \n",
    "    Returns:\n",
    "        每個 epoch 的損失\n",
    "    \"\"\"\n",
    "    # TODO: 實作訓練迴圈\n",
    "    # 提示：使用 MSE loss\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: 比較不同聲碼器\n",
    "\n",
    "設計實驗比較不同聲碼器的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class VocoderBenchmark:\n",
    "    \"\"\"聲碼器性能評估結果\"\"\"\n",
    "    name: str\n",
    "    rtf: float  # Real-Time Factor\n",
    "    mcd: float  # Mel Cepstral Distortion\n",
    "    params: int  # 參數量\n",
    "    \n",
    "def benchmark_vocoder(\n",
    "    vocoder: nn.Module,\n",
    "    test_mels: List[torch.Tensor],\n",
    "    reference_wavs: List[np.ndarray],\n",
    "    sample_rate: int = 22050\n",
    ") -> VocoderBenchmark:\n",
    "    \"\"\"\n",
    "    評估聲碼器性能\n",
    "    \n",
    "    Args:\n",
    "        vocoder: 聲碼器模型\n",
    "        test_mels: 測試用 mel 頻譜\n",
    "        reference_wavs: 參考波形\n",
    "        sample_rate: 採樣率\n",
    "        \n",
    "    Returns:\n",
    "        評估結果\n",
    "    \"\"\"\n",
    "    # TODO: 實作聲碼器評估\n",
    "    # 提示：\n",
    "    # 1. 測量推理時間計算 RTF\n",
    "    # 2. 計算 MCD\n",
    "    # 3. 統計參數量\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "```\n",
    "語音合成重點回顧：\n",
    "\n",
    "1. 音頻表示\n",
    "   ├─ 波形：原始信號\n",
    "   ├─ 頻譜圖：時頻分析\n",
    "   └─ Mel 頻譜：人耳感知尺度\n",
    "\n",
    "2. 經典架構\n",
    "   ├─ Tacotron 2：自回歸，高品質\n",
    "   ├─ FastSpeech 2：非自回歸，快速\n",
    "   └─ 關鍵組件：Encoder, Attention, Decoder, PostNet\n",
    "\n",
    "3. 聲碼器\n",
    "   ├─ WaveNet：自回歸，高品質但慢\n",
    "   ├─ HiFi-GAN：GAN-based，快速\n",
    "   └─ 上採樣 + MRF 結構\n",
    "\n",
    "4. 現代端到端模型\n",
    "   ├─ VITS：VAE + Flow + GAN\n",
    "   ├─ Bark：GPT-style，多功能\n",
    "   └─ VALL-E：零樣本聲音複製\n",
    "\n",
    "5. 評估指標\n",
    "   ├─ 客觀：MCD, F0 RMSE, RTF\n",
    "   └─ 主觀：MOS (1-5 分)\n",
    "\n",
    "實際應用建議：\n",
    "- RTX 5080 (16GB) 可運行 VITS、Bark\n",
    "- 生產環境優先考慮 VITS（速度快、品質好）\n",
    "- 需要情感/效果時使用 Bark\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
