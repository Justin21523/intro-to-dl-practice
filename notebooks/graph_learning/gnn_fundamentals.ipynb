{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9: 圖神經網路 (GNN)\n",
    "\n",
    "## 學習目標\n",
    "- 理解圖結構資料的特點\n",
    "- 掌握訊息傳遞機制 (Message Passing)\n",
    "- 實現 GCN 和 GAT\n",
    "- 使用 PyTorch Geometric 進行節點分類\n",
    "\n",
    "## 為什麼要學 GNN？\n",
    "\n",
    "許多真實世界的資料天然具有**圖結構**：\n",
    "- 社交網路（人與人的關係）\n",
    "- 分子結構（原子與化學鍵）\n",
    "- 知識圖譜（實體與關係）\n",
    "- 推薦系統（用戶-物品互動）\n",
    "\n",
    "傳統神經網路假設資料是獨立的向量或網格，無法處理這種關係結構。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安裝 PyTorch Geometric（如果尚未安裝）\n",
    "# !pip install torch-geometric\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用設備: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: 圖的基本概念\n",
    "\n",
    "### 圖的組成\n",
    "\n",
    "一個圖 $G = (V, E)$ 由以下元素組成：\n",
    "- **節點 (Nodes/Vertices)** $V$：圖中的實體\n",
    "- **邊 (Edges)** $E$：節點之間的連接\n",
    "- **節點特徵** $X$：每個節點的屬性向量\n",
    "- **邊特徵**（可選）：邊的屬性\n",
    "\n",
    "```\n",
    "簡單圖示例：\n",
    "    \n",
    "    (A) ─── (B)\n",
    "     │ \\   / │\n",
    "     │  \\ /  │\n",
    "     │   X   │\n",
    "     │  / \\  │\n",
    "     │ /   \\ │\n",
    "    (C) ─── (D)\n",
    "\n",
    "節點: {A, B, C, D}\n",
    "邊: {(A,B), (A,C), (A,D), (B,C), (B,D), (C,D)}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph_basics():\n",
    "    \"\"\"\n",
    "    可視化圖的基本結構\n",
    "    \"\"\"\n",
    "    # 建立一個簡單的圖\n",
    "    G = nx.karate_club_graph()  # 著名的空手道俱樂部社交網路\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 圖結構可視化\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    # 根據社群上色\n",
    "    clubs = [G.nodes[i]['club'] for i in G.nodes()]\n",
    "    colors = ['#FF6B6B' if c == 'Mr. Hi' else '#4ECDC4' for c in clubs]\n",
    "    \n",
    "    nx.draw(G, pos, ax=axes[0], node_color=colors, \n",
    "            with_labels=True, node_size=300, font_size=8)\n",
    "    axes[0].set_title(\"空手道俱樂部社交網路\")\n",
    "    \n",
    "    # 鄰接矩陣可視化\n",
    "    adj_matrix = nx.adjacency_matrix(G).todense()\n",
    "    im = axes[1].imshow(adj_matrix, cmap='Blues')\n",
    "    axes[1].set_title(\"鄰接矩陣 A\")\n",
    "    axes[1].set_xlabel(\"節點 j\")\n",
    "    axes[1].set_ylabel(\"節點 i\")\n",
    "    plt.colorbar(im, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"節點數: {G.number_of_nodes()}\")\n",
    "    print(f\"邊數: {G.number_of_edges()}\")\n",
    "    print(f\"平均度數: {np.mean([d for n, d in G.degree()]):.2f}\")\n",
    "\n",
    "visualize_graph_basics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 圖的表示方式\n",
    "\n",
    "在深度學習中，圖通常用以下方式表示：\n",
    "\n",
    "1. **鄰接矩陣 A**：$A_{ij} = 1$ 如果節點 i 和 j 相連\n",
    "2. **邊列表**：所有邊的列表 [(i, j), ...]\n",
    "3. **節點特徵矩陣 X**：$X \\in \\mathbb{R}^{N \\times F}$，N 個節點，每個 F 維特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 三種表示方式\n",
    "def graph_representations():\n",
    "    \"\"\"\n",
    "    展示圖的不同表示方式\n",
    "    \"\"\"\n",
    "    # 簡單的 4 節點圖\n",
    "    # 0 -- 1\n",
    "    # |    |\n",
    "    # 2 -- 3\n",
    "    \n",
    "    num_nodes = 4\n",
    "    \n",
    "    # 1. 鄰接矩陣\n",
    "    adj_matrix = torch.tensor([\n",
    "        [0, 1, 1, 0],\n",
    "        [1, 0, 0, 1],\n",
    "        [1, 0, 0, 1],\n",
    "        [0, 1, 1, 0]\n",
    "    ], dtype=torch.float32)\n",
    "    \n",
    "    # 2. 邊列表 (COO 格式，PyTorch Geometric 常用)\n",
    "    edge_index = torch.tensor([\n",
    "        [0, 0, 1, 1, 2, 2, 3, 3],  # 源節點\n",
    "        [1, 2, 0, 3, 0, 3, 1, 2]   # 目標節點\n",
    "    ], dtype=torch.long)\n",
    "    \n",
    "    # 3. 節點特徵\n",
    "    node_features = torch.randn(num_nodes, 8)  # 4 個節點，每個 8 維特徵\n",
    "    \n",
    "    print(\"1. 鄰接矩陣 A:\")\n",
    "    print(adj_matrix)\n",
    "    print(f\"\\n2. 邊列表 (edge_index):\")\n",
    "    print(f\"   源節點:   {edge_index[0].tolist()}\")\n",
    "    print(f\"   目標節點: {edge_index[1].tolist()}\")\n",
    "    print(f\"\\n3. 節點特徵 X: 形狀 {node_features.shape}\")\n",
    "    \n",
    "    return adj_matrix, edge_index, node_features\n",
    "\n",
    "adj_matrix, edge_index, node_features = graph_representations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: 訊息傳遞機制 (Message Passing)\n",
    "\n",
    "### 核心思想\n",
    "\n",
    "GNN 的核心是**訊息傳遞**：每個節點通過聚合鄰居的資訊來更新自己的表示。\n",
    "\n",
    "```\n",
    "訊息傳遞三步驟：\n",
    "\n",
    "1. 訊息 (Message): 每個鄰居發送資訊\n",
    "   m_{j→i} = MSG(h_j)\n",
    "\n",
    "2. 聚合 (Aggregate): 收集所有鄰居的訊息\n",
    "   m_i = AGG({m_{j→i} : j ∈ N(i)})\n",
    "\n",
    "3. 更新 (Update): 用聚合的訊息更新節點表示\n",
    "   h'_i = UPDATE(h_i, m_i)\n",
    "\n",
    "圖示:\n",
    "        ┌─────┐\n",
    "    ┌───│ 鄰居1│───┐\n",
    "    │   └─────┘   │\n",
    "    │             │\n",
    "    ▼   ┌─────┐   ▼\n",
    "    └───│ 節點 │───┘ ← 聚合鄰居訊息\n",
    "        └─────┘\n",
    "            │\n",
    "            ▼\n",
    "        更新表示\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMessagePassing(nn.Module):\n",
    "    \"\"\"\n",
    "    最簡單的訊息傳遞層\n",
    "    \n",
    "    聚合方式：平均鄰居特徵\n",
    "    更新方式：線性變換\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 節點特徵 [N, F]\n",
    "            adj: 鄰接矩陣 [N, N]\n",
    "        \"\"\"\n",
    "        # 計算度數矩陣（每個節點的鄰居數）\n",
    "        degree = adj.sum(dim=1, keepdim=True) + 1e-6\n",
    "        \n",
    "        # 聚合：鄰居特徵的平均\n",
    "        # adj @ x 對每個節點求鄰居特徵的和\n",
    "        neighbor_sum = torch.matmul(adj, x)\n",
    "        neighbor_mean = neighbor_sum / degree\n",
    "        \n",
    "        # 更新：線性變換 + 非線性\n",
    "        out = self.linear(neighbor_mean)\n",
    "        \n",
    "        return F.relu(out)\n",
    "\n",
    "# 測試\n",
    "layer = SimpleMessagePassing(8, 16)\n",
    "out = layer(node_features, adj_matrix)\n",
    "print(f\"輸入形狀: {node_features.shape}\")\n",
    "print(f\"輸出形狀: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: GCN (Graph Convolutional Network)\n",
    "\n",
    "### GCN 的直觀理解\n",
    "\n",
    "GCN 是最經典的 GNN，它的更新公式是：\n",
    "\n",
    "$$H^{(l+1)} = \\sigma(\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(l)} W^{(l)})$$\n",
    "\n",
    "其中：\n",
    "- $\\tilde{A} = A + I$（加入自連接）\n",
    "- $\\tilde{D}$ 是 $\\tilde{A}$ 的度數矩陣\n",
    "- $W^{(l)}$ 是可學習的權重\n",
    "\n",
    "**簡化理解**：對每個節點，取自己和鄰居特徵的加權平均，然後做線性變換。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Convolutional Layer\n",
    "    \n",
    "    實現標準的 GCN 層\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        GCN 前向傳播\n",
    "        \n",
    "        H' = D^(-1/2) A D^(-1/2) X W\n",
    "        \"\"\"\n",
    "        # 加入自連接\n",
    "        adj_hat = adj + torch.eye(adj.size(0)).to(adj.device)\n",
    "        \n",
    "        # 計算度數矩陣的逆平方根\n",
    "        degree = adj_hat.sum(dim=1)\n",
    "        d_inv_sqrt = torch.diag(degree.pow(-0.5))\n",
    "        \n",
    "        # 對稱歸一化: D^(-1/2) A D^(-1/2)\n",
    "        adj_norm = torch.mm(torch.mm(d_inv_sqrt, adj_hat), d_inv_sqrt)\n",
    "        \n",
    "        # 聚合鄰居特徵\n",
    "        support = torch.mm(x, self.weight)\n",
    "        output = torch.mm(adj_norm, support)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        \n",
    "        return output\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \"\"\"\n",
    "    兩層 GCN 用於節點分類\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, n_hidden, n_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.gc1 = GCNLayer(n_features, n_hidden)\n",
    "        self.gc2 = GCNLayer(n_hidden, n_classes)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# 測試\n",
    "model = GCN(n_features=8, n_hidden=16, n_classes=2)\n",
    "out = model(node_features, adj_matrix)\n",
    "print(f\"GCN 輸出形狀: {out.shape}\")\n",
    "print(f\"預測類別機率:\\n{torch.exp(out)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: GAT (Graph Attention Network)\n",
    "\n",
    "### GAT 的改進\n",
    "\n",
    "GCN 對所有鄰居一視同仁，但實際上不同鄰居的重要性可能不同。\n",
    "\n",
    "GAT 使用**注意力機制**為每個鄰居分配不同的權重：\n",
    "\n",
    "$$\\alpha_{ij} = \\frac{\\exp(\\text{LeakyReLU}(a^T [Wh_i || Wh_j]))}{\\sum_{k \\in N(i)} \\exp(\\text{LeakyReLU}(a^T [Wh_i || Wh_k]))}$$\n",
    "\n",
    "$$h'_i = \\sigma\\left(\\sum_{j \\in N(i)} \\alpha_{ij} W h_j\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Attention Layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, n_heads=1, dropout=0.6, concat=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.concat = concat\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # 每個注意力頭的變換矩陣\n",
    "        self.W = nn.Parameter(torch.FloatTensor(n_heads, in_features, out_features))\n",
    "        \n",
    "        # 注意力向量\n",
    "        self.a = nn.Parameter(torch.FloatTensor(n_heads, 2 * out_features, 1))\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.xavier_uniform_(self.a)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 節點特徵 [N, F]\n",
    "            adj: 鄰接矩陣 [N, N]\n",
    "        \"\"\"\n",
    "        N = x.size(0)\n",
    "        \n",
    "        # 線性變換: [N, F] -> [n_heads, N, out_features]\n",
    "        h = torch.einsum('nf,hfo->hno', x, self.W)\n",
    "        \n",
    "        # 計算注意力係數\n",
    "        # 建立所有節點對的特徵拼接\n",
    "        # [n_heads, N, out_features] -> [n_heads, N, N, 2*out_features]\n",
    "        h_repeat_i = h.unsqueeze(2).repeat(1, 1, N, 1)  # [h, N, N, F]\n",
    "        h_repeat_j = h.unsqueeze(1).repeat(1, N, 1, 1)  # [h, N, N, F]\n",
    "        concat_features = torch.cat([h_repeat_i, h_repeat_j], dim=-1)  # [h, N, N, 2F]\n",
    "        \n",
    "        # 計算注意力分數\n",
    "        e = self.leaky_relu(torch.einsum('hnmf,hfo->hnm', concat_features, self.a).squeeze(-1))\n",
    "        \n",
    "        # 遮罩非鄰居\n",
    "        adj_with_self = adj + torch.eye(N).to(adj.device)\n",
    "        mask = (adj_with_self == 0)\n",
    "        e = e.masked_fill(mask.unsqueeze(0), float('-inf'))\n",
    "        \n",
    "        # Softmax 歸一化\n",
    "        attention = F.softmax(e, dim=-1)  # [n_heads, N, N]\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        # 應用注意力權重聚合特徵\n",
    "        h_prime = torch.einsum('hnm,hmo->hno', attention, h)  # [n_heads, N, out_features]\n",
    "        \n",
    "        # 合併多頭\n",
    "        if self.concat:\n",
    "            # 拼接所有頭: [N, n_heads * out_features]\n",
    "            return h_prime.permute(1, 0, 2).contiguous().view(N, -1)\n",
    "        else:\n",
    "            # 平均所有頭: [N, out_features]\n",
    "            return h_prime.mean(dim=0)\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    \"\"\"\n",
    "    兩層 GAT 用於節點分類\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, n_hidden, n_classes, n_heads=4, dropout=0.6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gat1 = GATLayer(n_features, n_hidden, n_heads=n_heads, dropout=dropout, concat=True)\n",
    "        self.gat2 = GATLayer(n_hidden * n_heads, n_classes, n_heads=1, dropout=dropout, concat=False)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.gat1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gat2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# 測試\n",
    "gat_model = GAT(n_features=8, n_hidden=8, n_classes=2, n_heads=4)\n",
    "out = gat_model(node_features, adj_matrix)\n",
    "print(f\"GAT 輸出形狀: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: 使用 PyTorch Geometric\n",
    "\n",
    "PyTorch Geometric (PyG) 是專門用於圖神經網路的庫，提供了許多預實現的層和資料集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嘗試導入 PyG（如果有安裝的話）\n",
    "try:\n",
    "    from torch_geometric.datasets import Planetoid\n",
    "    from torch_geometric.nn import GCNConv, GATConv\n",
    "    from torch_geometric.transforms import NormalizeFeatures\n",
    "    HAS_PYG = True\n",
    "    print(\"PyTorch Geometric 已載入\")\n",
    "except ImportError:\n",
    "    HAS_PYG = False\n",
    "    print(\"PyTorch Geometric 未安裝，將使用自定義實現\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PYG:\n",
    "    # 載入 Cora 資料集\n",
    "    dataset = Planetoid(root='./data', name='Cora', transform=NormalizeFeatures())\n",
    "    data = dataset[0]\n",
    "    \n",
    "    print(f\"資料集: {dataset}\")\n",
    "    print(f\"節點數: {data.num_nodes}\")\n",
    "    print(f\"邊數: {data.num_edges}\")\n",
    "    print(f\"特徵維度: {data.num_features}\")\n",
    "    print(f\"類別數: {dataset.num_classes}\")\n",
    "    print(f\"訓練節點: {data.train_mask.sum().item()}\")\n",
    "    print(f\"驗證節點: {data.val_mask.sum().item()}\")\n",
    "    print(f\"測試節點: {data.test_mask.sum().item()}\")\n",
    "else:\n",
    "    # 建立模擬資料\n",
    "    print(\"使用模擬資料\")\n",
    "    G = nx.karate_club_graph()\n",
    "    \n",
    "    # 準備資料\n",
    "    num_nodes = G.number_of_nodes()\n",
    "    num_features = 34  # 與節點數相同的特徵（one-hot）\n",
    "    num_classes = 2\n",
    "    \n",
    "    # 節點特徵：one-hot 編碼\n",
    "    x = torch.eye(num_nodes)\n",
    "    \n",
    "    # 標籤：俱樂部歸屬\n",
    "    y = torch.tensor([0 if G.nodes[i]['club'] == 'Mr. Hi' else 1 for i in range(num_nodes)])\n",
    "    \n",
    "    # 邊列表\n",
    "    edges = list(G.edges())\n",
    "    edge_index = torch.tensor([[e[0] for e in edges] + [e[1] for e in edges],\n",
    "                               [e[1] for e in edges] + [e[0] for e in edges]], dtype=torch.long)\n",
    "    \n",
    "    # 鄰接矩陣\n",
    "    adj = torch.zeros(num_nodes, num_nodes)\n",
    "    for i, j in edges:\n",
    "        adj[i, j] = 1\n",
    "        adj[j, i] = 1\n",
    "    \n",
    "    # 訓練/測試分割\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    train_mask[:20] = True\n",
    "    test_mask = ~train_mask\n",
    "    \n",
    "    print(f\"節點數: {num_nodes}\")\n",
    "    print(f\"邊數: {len(edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PYG:\n",
    "    class PyGGCN(nn.Module):\n",
    "        \"\"\"使用 PyG 的 GCN\"\"\"\n",
    "        def __init__(self, n_features, n_hidden, n_classes, dropout=0.5):\n",
    "            super().__init__()\n",
    "            self.conv1 = GCNConv(n_features, n_hidden)\n",
    "            self.conv2 = GCNConv(n_hidden, n_classes)\n",
    "            self.dropout = dropout\n",
    "            \n",
    "        def forward(self, x, edge_index):\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    class PyGGAT(nn.Module):\n",
    "        \"\"\"使用 PyG 的 GAT\"\"\"\n",
    "        def __init__(self, n_features, n_hidden, n_classes, n_heads=4, dropout=0.6):\n",
    "            super().__init__()\n",
    "            self.conv1 = GATConv(n_features, n_hidden, heads=n_heads, dropout=dropout)\n",
    "            self.conv2 = GATConv(n_hidden * n_heads, n_classes, heads=1, concat=False, dropout=dropout)\n",
    "            self.dropout = dropout\n",
    "            \n",
    "        def forward(self, x, edge_index):\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    # 訓練函數\n",
    "    def train_pyg_model(model, data, epochs=200, lr=0.01, weight_decay=5e-4):\n",
    "        model = model.to(device)\n",
    "        data = data.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        best_val_acc = 0\n",
    "        history = {'train_loss': [], 'val_acc': [], 'test_acc': []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 評估\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model(data.x, data.edge_index)\n",
    "                pred = out.argmax(dim=1)\n",
    "                \n",
    "                val_acc = (pred[data.val_mask] == data.y[data.val_mask]).sum() / data.val_mask.sum()\n",
    "                test_acc = (pred[data.test_mask] == data.y[data.test_mask]).sum() / data.test_mask.sum()\n",
    "            \n",
    "            history['train_loss'].append(loss.item())\n",
    "            history['val_acc'].append(val_acc.item())\n",
    "            history['test_acc'].append(test_acc.item())\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    # 訓練 GCN\n",
    "    print(\"\\n訓練 GCN:\")\n",
    "    gcn_pyg = PyGGCN(dataset.num_features, 16, dataset.num_classes)\n",
    "    gcn_history = train_pyg_model(gcn_pyg, data, epochs=200)\n",
    "    \n",
    "    # 訓練 GAT\n",
    "    print(\"\\n訓練 GAT:\")\n",
    "    gat_pyg = PyGGAT(dataset.num_features, 8, dataset.num_classes, n_heads=8)\n",
    "    gat_history = train_pyg_model(gat_pyg, data, epochs=200)\n",
    "    \n",
    "else:\n",
    "    # 使用自定義實現\n",
    "    def train_custom_model(model, x, adj, y, train_mask, test_mask, epochs=200, lr=0.01):\n",
    "        model = model.to(device)\n",
    "        x = x.to(device)\n",
    "        adj = adj.to(device)\n",
    "        y = y.to(device)\n",
    "        train_mask = train_mask.to(device)\n",
    "        test_mask = test_mask.to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "        \n",
    "        history = {'train_loss': [], 'test_acc': []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model(x, adj)\n",
    "            loss = F.nll_loss(out[train_mask], y[train_mask])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out = model(x, adj)\n",
    "                pred = out.argmax(dim=1)\n",
    "                test_acc = (pred[test_mask] == y[test_mask]).sum() / test_mask.sum()\n",
    "            \n",
    "            history['train_loss'].append(loss.item())\n",
    "            history['test_acc'].append(test_acc.item())\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    print(\"\\n訓練 GCN (自定義實現):\")\n",
    "    gcn_custom = GCN(num_features, 16, num_classes)\n",
    "    gcn_history = train_custom_model(gcn_custom, x, adj, y, train_mask, test_mask, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繪製訓練曲線\n",
    "if HAS_PYG:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].plot(gcn_history['train_loss'], label='GCN')\n",
    "    axes[0].plot(gat_history['train_loss'], label='GAT')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Training Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_title('訓練損失')\n",
    "    \n",
    "    axes[1].plot(gcn_history['test_acc'], label='GCN')\n",
    "    axes[1].plot(gat_history['test_acc'], label='GAT')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Test Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_title('測試準確率')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n最終測試準確率:\")\n",
    "    print(f\"  GCN: {gcn_history['test_acc'][-1]*100:.2f}%\")\n",
    "    print(f\"  GAT: {gat_history['test_acc'][-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 練習題\n",
    "\n",
    "### 練習 1: GraphSAGE 層\n",
    "\n",
    "**目標**: 實現 GraphSAGE 的採樣聚合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1: GraphSAGE\n",
    "\n",
    "class GraphSAGELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAGE 層\n",
    "    \n",
    "    與 GCN 的差異：\n",
    "    1. 明確分開自己和鄰居的表示\n",
    "    2. 可以使用不同的聚合函數（mean, max, LSTM 等）\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, aggregator='mean'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.aggregator = aggregator\n",
    "        \n",
    "        # 自己特徵的變換\n",
    "        self.W_self = nn.Linear(in_features, out_features, bias=False)\n",
    "        \n",
    "        # 鄰居特徵的變換\n",
    "        self.W_neighbor = nn.Linear(in_features, out_features, bias=False)\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        h' = W_self * h + W_neighbor * AGG(h_neighbors)\n",
    "        \"\"\"\n",
    "        # 聚合鄰居\n",
    "        degree = adj.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "        \n",
    "        if self.aggregator == 'mean':\n",
    "            neighbor_sum = torch.mm(adj, x)\n",
    "            neighbor_agg = neighbor_sum / degree\n",
    "        elif self.aggregator == 'max':\n",
    "            # 使用遮罩實現 max pooling\n",
    "            mask = (adj > 0).float()\n",
    "            x_expanded = x.unsqueeze(0).expand(x.size(0), -1, -1)  # [N, N, F]\n",
    "            masked = x_expanded * mask.unsqueeze(-1) - (1 - mask.unsqueeze(-1)) * 1e9\n",
    "            neighbor_agg = masked.max(dim=1)[0]\n",
    "        else:\n",
    "            neighbor_sum = torch.mm(adj, x)\n",
    "            neighbor_agg = neighbor_sum / degree\n",
    "        \n",
    "        # 結合自己和鄰居的表示\n",
    "        self_transformed = self.W_self(x)\n",
    "        neighbor_transformed = self.W_neighbor(neighbor_agg)\n",
    "        \n",
    "        out = self_transformed + neighbor_transformed + self.bias\n",
    "        \n",
    "        # L2 歸一化（GraphSAGE 特有）\n",
    "        out = F.normalize(out, p=2, dim=1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_classes, aggregator='mean'):\n",
    "        super().__init__()\n",
    "        self.sage1 = GraphSAGELayer(n_features, n_hidden, aggregator)\n",
    "        self.sage2 = GraphSAGELayer(n_hidden, n_classes, aggregator)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.sage1(x, adj))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.sage2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# 測試\n",
    "sage_model = GraphSAGE(8, 16, 2, aggregator='mean')\n",
    "out = sage_model(node_features, adj_matrix)\n",
    "print(f\"GraphSAGE 輸出形狀: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2: 圖級別預測\n",
    "\n",
    "**目標**: 實現圖級別的分類（而非節點級別）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2: 圖分類\n",
    "\n",
    "class GraphClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    圖級別分類器\n",
    "    \n",
    "    步驟：\n",
    "    1. 用 GNN 層學習節點表示\n",
    "    2. 使用 Readout 函數聚合所有節點表示\n",
    "    3. 用 MLP 進行分類\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, n_hidden, n_classes, readout='mean'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.readout = readout\n",
    "        \n",
    "        # GNN 層\n",
    "        self.conv1 = GCNLayer(n_features, n_hidden)\n",
    "        self.conv2 = GCNLayer(n_hidden, n_hidden)\n",
    "        self.conv3 = GCNLayer(n_hidden, n_hidden)\n",
    "        \n",
    "        # 分類器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(n_hidden, n_classes)\n",
    "        )\n",
    "        \n",
    "    def graph_readout(self, x):\n",
    "        \"\"\"\n",
    "        將節點表示聚合為圖表示\n",
    "        \"\"\"\n",
    "        if self.readout == 'mean':\n",
    "            return x.mean(dim=0, keepdim=True)\n",
    "        elif self.readout == 'max':\n",
    "            return x.max(dim=0, keepdim=True)[0]\n",
    "        elif self.readout == 'sum':\n",
    "            return x.sum(dim=0, keepdim=True)\n",
    "        else:\n",
    "            return x.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        # 節點嵌入\n",
    "        x = F.relu(self.conv1(x, adj))\n",
    "        x = F.relu(self.conv2(x, adj))\n",
    "        x = F.relu(self.conv3(x, adj))\n",
    "        \n",
    "        # 圖級別表示\n",
    "        graph_embedding = self.graph_readout(x)\n",
    "        \n",
    "        # 分類\n",
    "        out = self.classifier(graph_embedding)\n",
    "        return out\n",
    "\n",
    "# 測試\n",
    "graph_clf = GraphClassifier(8, 32, 2, readout='mean')\n",
    "out = graph_clf(node_features, adj_matrix)\n",
    "print(f\"圖分類輸出形狀: {out.shape}\")\n",
    "print(f\"預測: {F.softmax(out, dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3: 可視化注意力權重\n",
    "\n",
    "**目標**: 可視化 GAT 的注意力權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 3: 注意力可視化\n",
    "\n",
    "class GATWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    GAT 層，同時返回注意力權重\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.a = nn.Linear(2 * out_features, 1, bias=False)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x, adj, return_attention=False):\n",
    "        N = x.size(0)\n",
    "        h = self.W(x)  # [N, out_features]\n",
    "        \n",
    "        # 計算注意力分數\n",
    "        h_repeat_i = h.unsqueeze(1).expand(-1, N, -1)  # [N, N, F]\n",
    "        h_repeat_j = h.unsqueeze(0).expand(N, -1, -1)  # [N, N, F]\n",
    "        concat = torch.cat([h_repeat_i, h_repeat_j], dim=-1)  # [N, N, 2F]\n",
    "        \n",
    "        e = self.leaky_relu(self.a(concat).squeeze(-1))  # [N, N]\n",
    "        \n",
    "        # 遮罩\n",
    "        adj_with_self = adj + torch.eye(N).to(adj.device)\n",
    "        e = e.masked_fill(adj_with_self == 0, float('-inf'))\n",
    "        \n",
    "        attention = F.softmax(e, dim=1)  # [N, N]\n",
    "        \n",
    "        h_prime = torch.mm(attention, h)  # [N, out_features]\n",
    "        \n",
    "        if return_attention:\n",
    "            return h_prime, attention\n",
    "        return h_prime\n",
    "\n",
    "def visualize_attention(model, x, adj, node_labels=None):\n",
    "    \"\"\"\n",
    "    可視化 GAT 的注意力權重\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, attention = model(x, adj, return_attention=True)\n",
    "    \n",
    "    attention = attention.cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 注意力矩陣\n",
    "    im = axes[0].imshow(attention, cmap='hot')\n",
    "    axes[0].set_title('注意力權重矩陣')\n",
    "    axes[0].set_xlabel('目標節點 j')\n",
    "    axes[0].set_ylabel('源節點 i')\n",
    "    plt.colorbar(im, ax=axes[0])\n",
    "    \n",
    "    # 圖可視化（邊粗細表示注意力）\n",
    "    G = nx.from_numpy_array(adj.cpu().numpy())\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    # 繪製節點\n",
    "    if node_labels is not None:\n",
    "        colors = ['#FF6B6B' if l == 0 else '#4ECDC4' for l in node_labels]\n",
    "    else:\n",
    "        colors = '#4ECDC4'\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, ax=axes[1], node_color=colors, node_size=300)\n",
    "    nx.draw_networkx_labels(G, pos, ax=axes[1], font_size=8)\n",
    "    \n",
    "    # 繪製邊（粗細與注意力相關）\n",
    "    for i, j in G.edges():\n",
    "        weight = (attention[i, j] + attention[j, i]) / 2  # 對稱化\n",
    "        nx.draw_networkx_edges(G, pos, ax=axes[1], edgelist=[(i, j)],\n",
    "                              width=weight * 5, alpha=max(0.2, weight))\n",
    "    \n",
    "    axes[1].set_title('圖可視化（邊粗細 = 注意力權重）')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 測試\n",
    "gat_vis = GATWithAttention(8, 16)\n",
    "visualize_attention(gat_vis, node_features, adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Part 6: 進階 GNN 技巧與實務\n\n### 6.1 過度平滑問題 (Over-smoothing)\n\n當 GNN 層數增加時，所有節點的表示會越來越相似，最終趨於同一個值。這是 GNN 的一個重要限制。",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== 過度平滑問題 ==========\n\ndef measure_smoothness(x):\n    \"\"\"\n    測量節點表示的平滑度（越小越相似）\n    使用節點表示間的平均距離\n    \"\"\"\n    n = x.size(0)\n    x_norm = F.normalize(x, p=2, dim=1)\n    \n    # 計算所有節點對的相似度\n    similarity = torch.mm(x_norm, x_norm.t())\n    \n    # 排除自己\n    mask = ~torch.eye(n, dtype=torch.bool).to(x.device)\n    avg_similarity = similarity[mask].mean().item()\n    \n    return avg_similarity\n\ndef demonstrate_oversmoothing(x, adj, max_layers=10):\n    \"\"\"\n    展示 GNN 層數增加時的過度平滑問題\n    \"\"\"\n    results = {'layers': [], 'smoothness': []}\n    \n    # 初始平滑度\n    h = x.clone()\n    results['layers'].append(0)\n    results['smoothness'].append(measure_smoothness(h))\n    \n    # 簡單的 GCN propagation\n    adj_hat = adj + torch.eye(adj.size(0)).to(adj.device)\n    degree = adj_hat.sum(dim=1)\n    d_inv_sqrt = torch.diag(degree.pow(-0.5))\n    adj_norm = torch.mm(torch.mm(d_inv_sqrt, adj_hat), d_inv_sqrt)\n    \n    for layer in range(1, max_layers + 1):\n        h = torch.mm(adj_norm, h)\n        h = F.relu(h)\n        \n        results['layers'].append(layer)\n        results['smoothness'].append(measure_smoothness(h))\n    \n    # 可視化\n    plt.figure(figsize=(10, 4))\n    plt.plot(results['layers'], results['smoothness'], 'b-o', linewidth=2)\n    plt.xlabel('GNN 層數')\n    plt.ylabel('節點表示相似度 (越高越平滑)')\n    plt.title('Over-smoothing: 層數增加導致節點表示趨同')\n    plt.grid(True, alpha=0.3)\n    plt.axhline(y=1.0, color='r', linestyle='--', label='完全相同')\n    plt.legend()\n    plt.show()\n    \n    print(\"💡 觀察：隨著層數增加，節點表示越來越相似\")\n    print(\"   這就是為什麼大多數 GNN 只用 2-3 層\")\n\n# 展示過度平滑\ndemonstrate_oversmoothing(node_features, adj_matrix, max_layers=15)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ========== 緩解 Over-smoothing 的方法 ==========\n\nclass ResidualGCN(nn.Module):\n    \"\"\"\n    方法 1: 殘差連接 (Residual Connection)\n    \n    h' = h + GCN(h)\n    保留原始資訊，避免完全被鄰居同化\n    \"\"\"\n    \n    def __init__(self, in_features, hidden_features, out_features, n_layers=5):\n        super().__init__()\n        \n        self.input_proj = nn.Linear(in_features, hidden_features)\n        \n        self.layers = nn.ModuleList([\n            GCNLayer(hidden_features, hidden_features) for _ in range(n_layers)\n        ])\n        \n        self.output_proj = nn.Linear(hidden_features, out_features)\n        \n    def forward(self, x, adj):\n        x = self.input_proj(x)\n        \n        for layer in self.layers:\n            x_new = layer(x, adj)\n            x = x + F.relu(x_new)  # 殘差連接\n        \n        return self.output_proj(x)\n\n\nclass JumpingKnowledgeGCN(nn.Module):\n    \"\"\"\n    方法 2: Jumping Knowledge (JK)\n    \n    聚合所有層的輸出，而不只用最後一層\n    可以使用 concat、max 或 LSTM 聚合\n    \"\"\"\n    \n    def __init__(self, in_features, hidden_features, out_features, n_layers=5, mode='concat'):\n        super().__init__()\n        \n        self.mode = mode\n        self.n_layers = n_layers\n        \n        self.layers = nn.ModuleList()\n        self.layers.append(GCNLayer(in_features, hidden_features))\n        \n        for _ in range(n_layers - 1):\n            self.layers.append(GCNLayer(hidden_features, hidden_features))\n        \n        if mode == 'concat':\n            self.output_proj = nn.Linear(hidden_features * n_layers, out_features)\n        else:  # max\n            self.output_proj = nn.Linear(hidden_features, out_features)\n    \n    def forward(self, x, adj):\n        layer_outputs = []\n        \n        for layer in self.layers:\n            x = F.relu(layer(x, adj))\n            layer_outputs.append(x)\n        \n        # 聚合所有層\n        if self.mode == 'concat':\n            x = torch.cat(layer_outputs, dim=-1)\n        else:  # max\n            x = torch.stack(layer_outputs, dim=0).max(dim=0)[0]\n        \n        return self.output_proj(x)\n\n\nclass DropEdgeGCN(nn.Module):\n    \"\"\"\n    方法 3: DropEdge\n    \n    訓練時隨機丟棄邊，類似 Dropout\n    減緩訊息傳播，避免過度平滑\n    \"\"\"\n    \n    def __init__(self, in_features, hidden_features, out_features, n_layers=5, drop_rate=0.3):\n        super().__init__()\n        \n        self.drop_rate = drop_rate\n        \n        self.layers = nn.ModuleList()\n        self.layers.append(GCNLayer(in_features, hidden_features))\n        \n        for _ in range(n_layers - 1):\n            self.layers.append(GCNLayer(hidden_features, hidden_features))\n        \n        self.output_proj = nn.Linear(hidden_features, out_features)\n    \n    def drop_edge(self, adj):\n        \"\"\"隨機丟棄邊\"\"\"\n        if not self.training:\n            return adj\n        \n        mask = torch.rand_like(adj) > self.drop_rate\n        return adj * mask.float()\n    \n    def forward(self, x, adj):\n        for layer in self.layers:\n            adj_dropped = self.drop_edge(adj)\n            x = F.relu(layer(x, adj_dropped))\n        \n        return self.output_proj(x)\n\n\n# 測試並比較\nprint(\"緩解 Over-smoothing 的方法：\")\nprint(\"\\n1. 殘差連接 (ResGCN):\")\nres_gcn = ResidualGCN(8, 16, 2, n_layers=5)\nout = res_gcn(node_features, adj_matrix)\nprint(f\"   5 層 ResGCN 輸出形狀: {out.shape}\")\n\nprint(\"\\n2. Jumping Knowledge:\")\njk_gcn = JumpingKnowledgeGCN(8, 16, 2, n_layers=5, mode='concat')\nout = jk_gcn(node_features, adj_matrix)\nprint(f\"   5 層 JK-GCN 輸出形狀: {out.shape}\")\n\nprint(\"\\n3. DropEdge:\")\ndrop_gcn = DropEdgeGCN(8, 16, 2, n_layers=5, drop_rate=0.3)\nout = drop_gcn(node_features, adj_matrix)\nprint(f\"   5 層 DropEdge-GCN 輸出形狀: {out.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 6.2 連結預測 (Link Prediction)\n\n連結預測是 GNN 的重要應用：預測兩個節點之間是否應該存在邊。\n\n應用場景：\n- 社交網路：好友推薦\n- 知識圖譜：關係補全\n- 生物網路：蛋白質交互作用預測",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 🎯 總結：GNN 完整知識框架\n\n### 📊 本模組重點回顧\n\n| 主題 | 核心概念 | 關鍵公式/技巧 |\n|------|----------|---------------|\n| **圖表示** | 鄰接矩陣、邊列表 | $A_{ij} = 1$ 表示連接 |\n| **訊息傳遞** | MSG → AGG → UPDATE | 每層聚合 1 跳鄰居 |\n| **GCN** | 對稱歸一化 | $H' = \\sigma(\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}HW)$ |\n| **GAT** | 注意力加權 | $\\alpha_{ij} = \\text{softmax}(e_{ij})$ |\n| **GraphSAGE** | 可學習聚合 | $h' = W_1h + W_2 \\cdot \\text{AGG}(N)$ |\n| **Over-smoothing** | 深層 GNN 問題 | 殘差、JK、DropEdge |\n| **連結預測** | 節點對評分 | $\\sigma(z_i^T z_j)$ 或 MLP |\n\n### 🔄 GNN 方法深度比較\n\n```\n┌────────────┬─────────────────┬─────────────────┬─────────────────┐\n│ 方法       │ 聚合方式        │ 優點            │ 缺點            │\n├────────────┼─────────────────┼─────────────────┼─────────────────┤\n│ GCN        │ 度歸一化平均    │ 簡單高效        │ 固定權重        │\n│ GAT        │ 注意力加權      │ 可學習重要性    │ 計算量大        │\n│ GraphSAGE  │ 分離自己/鄰居   │ 歸納學習        │ 需採樣          │\n│ GIN        │ Sum + MLP       │ 理論最強表達    │ 可能過擬合      │\n│ MPNN       │ 通用框架        │ 靈活性高        │ 設計空間大      │\n└────────────┴─────────────────┴─────────────────┴─────────────────┘\n```\n\n### 🛠️ GNN 實戰問題診斷\n\n```\n問題 1: 過度平滑 (Over-smoothing)\n  症狀：深層 GNN 性能下降，節點表示趨同\n  解決：\n    - 減少層數（通常 2-3 層最佳）\n    - 殘差連接（ResGCN）\n    - Jumping Knowledge\n    - DropEdge / DropNode\n\n問題 2: 訓練數據不平衡\n  症狀：少數類別性能差\n  解決：\n    - 重新採樣\n    - 類別權重\n    - Focal Loss\n\n問題 3: 大規模圖處理困難\n  症狀：內存溢出，速度慢\n  解決：\n    - Mini-batch 訓練（鄰居採樣）\n    - GraphSAGE 風格採樣\n    - Cluster-GCN\n    - 分佈式訓練\n```\n\n### 📝 GNN 訓練 Checklist\n\n**模型設計**：\n- [ ] 確定任務類型（節點/邊/圖級別）\n- [ ] 選擇合適的 GNN 變體\n- [ ] 設定層數（通常 2-3 層）\n- [ ] 選擇聚合函數（mean, sum, max）\n\n**數據處理**：\n- [ ] 構建圖結構（鄰接矩陣/邊列表）\n- [ ] 設計節點特徵\n- [ ] 處理自環（是否添加）\n- [ ] 訓練/驗證/測試分割\n\n**訓練技巧**：\n- [ ] 使用適當的正則化（Dropout, DropEdge）\n- [ ] 監控過度平滑指標\n- [ ] 對於大圖使用採樣\n- [ ] 早停 (Early Stopping)\n\n### 🔗 常見 GNN 應用場景\n\n| 任務類型 | 應用場景 | 推薦方法 |\n|----------|----------|----------|\n| **節點分類** | 社交網路用戶分類 | GCN, GAT |\n| **連結預測** | 好友推薦、藥物交互 | GraphSAGE + Decoder |\n| **圖分類** | 分子性質預測 | GIN + Pooling |\n| **節點聚類** | 社區發現 | DGI, GRACE |\n| **圖生成** | 藥物發現 | GraphVAE, GraphRNN |\n\n### 🔗 延伸學習資源\n\n| 資源 | 說明 |\n|------|------|\n| [GCN 原論文](https://arxiv.org/abs/1609.02907) | Kipf & Welling, 2016 |\n| [GAT 原論文](https://arxiv.org/abs/1710.10903) | Veličković et al., 2017 |\n| [GraphSAGE](https://arxiv.org/abs/1706.02216) | Hamilton et al., 2017 |\n| [GIN 論文](https://arxiv.org/abs/1810.00826) | Xu et al., 2018 |\n| [PyG 文檔](https://pytorch-geometric.readthedocs.io/) | PyTorch Geometric |\n| [Stanford CS224W](http://web.stanford.edu/class/cs224w/) | 圖機器學習課程 |\n\n### ⏭️ 下一步\n\n在下一個模組中，我們將學習 **性能優化與部署**：\n- DataLoader 優化（num_workers, pin_memory）\n- 混合精度訓練（AMP）\n- 模型量化與剪枝\n- 模型導出與部署（TorchScript, ONNX）",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 總結\n",
    "\n",
    "### 本模組重點\n",
    "\n",
    "1. **圖的基本概念**\n",
    "   - 節點、邊、鄰接矩陣\n",
    "   - 節點特徵表示\n",
    "\n",
    "2. **訊息傳遞機制**\n",
    "   - 訊息 → 聚合 → 更新\n",
    "   - 每一層聚合一跳鄰居\n",
    "\n",
    "3. **GCN**\n",
    "   - 對稱歸一化的鄰接矩陣\n",
    "   - $H' = \\sigma(\\hat{A} H W)$\n",
    "\n",
    "4. **GAT**\n",
    "   - 注意力機制分配鄰居權重\n",
    "   - 多頭注意力增強表達能力\n",
    "\n",
    "### GNN 方法比較\n",
    "\n",
    "```\n",
    "┌──────────┬────────────────┬────────────────┐\n",
    "│ 方法     │ 聚合方式       │ 特點           │\n",
    "├──────────┼────────────────┼────────────────┤\n",
    "│ GCN      │ 平均 + 歸一化  │ 簡單高效       │\n",
    "│ GAT      │ 注意力加權     │ 可學習權重     │\n",
    "│ GraphSAGE│ 可選聚合函數   │ 歸納學習       │\n",
    "│ GIN      │ sum + MLP      │ 理論保證       │\n",
    "└──────────┴────────────────┴────────────────┘\n",
    "```\n",
    "\n",
    "### 常見應用\n",
    "\n",
    "- **節點分類**：社交網路用戶分類\n",
    "- **連結預測**：推薦系統\n",
    "- **圖分類**：分子性質預測\n",
    "- **圖生成**：藥物發現\n",
    "\n",
    "### 下一步\n",
    "\n",
    "在下一個模組中，我們將學習**性能優化與部署**，包括 DataLoader 優化、混合精度訓練、模型保存與載入。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}