{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3：卷積神經網路 (CNN) 與影像任務\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 理解卷積運算的原理與直覺\n",
    "2. 掌握 CNN 的核心組件：卷積層、池化層\n",
    "3. 了解為什麼 CNN 適合處理影像\n",
    "4. 學會資料增強技術\n",
    "5. 理解並實作 ResNet 的殘差連接\n",
    "6. 實作：用 CNN 分類 CIFAR-10\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1：為什麼需要 CNN？\n",
    "\n",
    "### 1.1 MLP 處理影像的問題\n",
    "\n",
    "假設用 MLP 處理 224×224 的 RGB 影像：\n",
    "- 輸入維度：224 × 224 × 3 = **150,528**\n",
    "- 如果第一層有 1000 個神經元：150,528 × 1000 = **1.5 億參數**！\n",
    "\n",
    "**問題：**\n",
    "1. 參數太多，容易過擬合\n",
    "2. 沒有利用影像的空間結構\n",
    "3. 不具備平移不變性（同一物體在不同位置要重新學）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參數量比較\n",
    "\n",
    "# MLP 處理 32x32 RGB 影像\n",
    "input_size = 32 * 32 * 3  # 3072\n",
    "hidden_size = 512\n",
    "\n",
    "mlp_params = input_size * hidden_size + hidden_size  # 第一層\n",
    "print(f\"MLP 第一層參數量: {mlp_params:,}\")\n",
    "\n",
    "# CNN 處理同樣影像\n",
    "# 3x3 kernel, 3 input channels, 64 output channels\n",
    "cnn_params = 3 * 3 * 3 * 64 + 64  # kernel weights + bias\n",
    "print(f\"CNN 第一層參數量: {cnn_params:,}\")\n",
    "\n",
    "print(f\"\\nCNN 參數量只有 MLP 的 {cnn_params/mlp_params*100:.2f}%！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 CNN 的核心思想\n",
    "\n",
    "1. **局部連接 (Local Connectivity)**：每個神經元只看輸入的一小塊區域\n",
    "2. **權重共享 (Weight Sharing)**：同一個 filter 在整張圖上滑動\n",
    "3. **平移不變性 (Translation Invariance)**：同一物體在不同位置會被同樣識別\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2：卷積運算 (Convolution)\n",
    "\n",
    "### 2.1 卷積的直覺\n",
    "\n",
    "**概念：** 用一個小的 filter（濾波器/kernel）在圖片上「滑動」，每個位置做內積。\n",
    "\n",
    "```\n",
    "輸入圖片          Filter          輸出\n",
    "┌─┬─┬─┬─┐       ┌─┬─┬─┐\n",
    "│1│2│3│0│       │1│0│1│      滑動並做內積\n",
    "├─┼─┼─┼─┤   ×   ├─┼─┼─┤   →   得到 feature map\n",
    "│0│1│2│3│       │0│1│0│\n",
    "├─┼─┼─┼─┤       ├─┼─┼─┤\n",
    "│3│0│1│2│       │1│0│1│\n",
    "├─┼─┼─┼─┤       └─┴─┴─┘\n",
    "│2│3│0│1│\n",
    "└─┴─┴─┴─┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷積運算視覺化\n",
    "\n",
    "def visualize_convolution(image, kernel, title=\"Convolution\"):\n",
    "    \"\"\"視覺化卷積運算\"\"\"\n",
    "    # 轉成 tensor 並加上 batch 和 channel 維度\n",
    "    img_tensor = torch.tensor(image, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    kernel_tensor = torch.tensor(kernel, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # 執行卷積\n",
    "    output = F.conv2d(img_tensor, kernel_tensor, padding=0)\n",
    "    \n",
    "    # 視覺化\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].imshow(image, cmap='gray')\n",
    "    axes[0].set_title('Input Image')\n",
    "    \n",
    "    axes[1].imshow(kernel, cmap='gray')\n",
    "    axes[1].set_title('Kernel/Filter')\n",
    "    \n",
    "    axes[2].imshow(output.squeeze().numpy(), cmap='gray')\n",
    "    axes[2].set_title('Output (Feature Map)')\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return output.squeeze().numpy()\n",
    "\n",
    "# 建立簡單的測試圖片（有垂直邊緣）\n",
    "image = np.array([\n",
    "    [0, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 0, 1, 1, 1],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "# 垂直邊緣檢測 kernel\n",
    "vertical_edge_kernel = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "    [-1, 0, 1],\n",
    "], dtype=np.float32)\n",
    "\n",
    "output = visualize_convolution(image, vertical_edge_kernel, \"Vertical Edge Detection\")\n",
    "print(\"\\n觀察：垂直邊緣（從暗到亮）產生正值，被檢測出來了！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同的 kernel 效果\n",
    "\n",
    "# 載入真實圖片\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "\n",
    "# 使用 torchvision 的範例圖片或建立一個簡單的測試圖片\n",
    "# 建立一個有邊緣的合成圖片\n",
    "np.random.seed(42)\n",
    "test_image = np.zeros((64, 64), dtype=np.float32)\n",
    "test_image[16:48, 16:48] = 1.0  # 中間一個白色方塊\n",
    "test_image += np.random.randn(64, 64) * 0.1  # 加點噪音\n",
    "test_image = np.clip(test_image, 0, 1)\n",
    "\n",
    "# 定義不同的 kernel\n",
    "kernels = {\n",
    "    'Identity': np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]]),\n",
    "    'Edge Detection': np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]),\n",
    "    'Sharpen': np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]]),\n",
    "    'Blur': np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]]) / 9.0,\n",
    "    'Sobel X': np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]),\n",
    "    'Sobel Y': np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 原圖\n",
    "axes[0].imshow(test_image, cmap='gray')\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# 各種 kernel 效果\n",
    "for i, (name, kernel) in enumerate(kernels.items()):\n",
    "    img_tensor = torch.tensor(test_image).unsqueeze(0).unsqueeze(0)\n",
    "    kernel_tensor = torch.tensor(kernel, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    output = F.conv2d(img_tensor, kernel_tensor, padding=1)\n",
    "    \n",
    "    axes[i+1].imshow(output.squeeze().numpy(), cmap='gray')\n",
    "    axes[i+1].set_title(name)\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "axes[-1].axis('off')  # 隱藏多餘的 subplot\n",
    "plt.suptitle('Different Convolution Kernels', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 卷積的重要參數\n",
    "\n",
    "| 參數 | 說明 | 常見值 |\n",
    "|------|------|--------|\n",
    "| **kernel_size** | filter 的大小 | 3×3, 5×5, 7×7 |\n",
    "| **stride** | 每次滑動的步長 | 1, 2 |\n",
    "| **padding** | 在邊緣補零 | 0, 1, 'same' |\n",
    "| **in_channels** | 輸入通道數 | RGB=3, 灰階=1 |\n",
    "| **out_channels** | 輸出通道數（filter 數量） | 32, 64, 128... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷積參數示範\n",
    "\n",
    "# 輸入：batch=1, channels=3 (RGB), height=32, width=32\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "print(f\"輸入 shape: {x.shape}\")\n",
    "\n",
    "# 不同參數的卷積層\n",
    "conv_configs = [\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 0},  # 基本\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 1},  # same padding\n",
    "    {'kernel_size': 3, 'stride': 2, 'padding': 1},  # stride=2 降維\n",
    "    {'kernel_size': 5, 'stride': 1, 'padding': 2},  # 大 kernel\n",
    "    {'kernel_size': 7, 'stride': 2, 'padding': 3},  # 更大 kernel\n",
    "]\n",
    "\n",
    "print(\"\\n不同參數的輸出 shape：\")\n",
    "for config in conv_configs:\n",
    "    conv = nn.Conv2d(in_channels=3, out_channels=64, **config)\n",
    "    out = conv(x)\n",
    "    print(f\"  kernel={config['kernel_size']}, stride={config['stride']}, \"\n",
    "          f\"padding={config['padding']} -> {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 輸出尺寸公式\n",
    "# output_size = (input_size - kernel_size + 2*padding) / stride + 1\n",
    "\n",
    "def calc_output_size(input_size, kernel_size, stride, padding):\n",
    "    return (input_size - kernel_size + 2*padding) // stride + 1\n",
    "\n",
    "print(\"輸出尺寸公式: output = (input - kernel + 2*padding) / stride + 1\")\n",
    "print(\"\\n範例 (input=32):\")\n",
    "\n",
    "for config in conv_configs:\n",
    "    out_size = calc_output_size(32, config['kernel_size'], \n",
    "                                config['stride'], config['padding'])\n",
    "    print(f\"  k={config['kernel_size']}, s={config['stride']}, \"\n",
    "          f\"p={config['padding']} -> output={out_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 多通道卷積\n",
    "\n",
    "**RGB 圖片的卷積：**\n",
    "- 輸入：3 個 channel（R, G, B）\n",
    "- 每個 filter 有 3 個對應的 kernel\n",
    "- 3 個 kernel 分別和 3 個 channel 做卷積，結果相加\n",
    "- 一個 filter 產生一個 output channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多通道卷積視覺化\n",
    "\n",
    "print(\"多通道卷積的維度變化：\")\n",
    "print(\"\")\n",
    "print(\"輸入: (batch, in_channels, H, W)\")\n",
    "print(\"       (1,     3,          32, 32)\")\n",
    "print(\"\")\n",
    "print(\"Conv2d(in_channels=3, out_channels=64, kernel_size=3)\")\n",
    "print(\"       ↓\")\n",
    "print(\"Weight shape: (out_channels, in_channels, kH, kW)\")\n",
    "print(\"              (64,          3,           3,  3)\")\n",
    "print(\"       ↓\")\n",
    "print(\"輸出: (batch, out_channels, H', W')\")\n",
    "print(\"       (1,     64,          30, 30)\")\n",
    "\n",
    "# 驗證\n",
    "conv = nn.Conv2d(3, 64, kernel_size=3)\n",
    "print(f\"\\n實際 weight shape: {conv.weight.shape}\")\n",
    "print(f\"總參數量: {conv.weight.numel() + conv.bias.numel()}\")\n",
    "print(f\"  = 64 × 3 × 3 × 3 + 64 = {64*3*3*3 + 64}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3：池化層 (Pooling)\n",
    "\n",
    "### 3.1 為什麼需要池化？\n",
    "\n",
    "1. **降低維度**：減少計算量和參數\n",
    "2. **增加感受野**：讓每個神經元「看到」更大的範圍\n",
    "3. **平移不變性**：小範圍的位移不影響結果\n",
    "\n",
    "### 3.2 Max Pooling vs Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 池化示範\n",
    "\n",
    "# 建立範例輸入\n",
    "x = torch.tensor([[[[1., 2., 3., 4.],\n",
    "                    [5., 6., 7., 8.],\n",
    "                    [9., 10., 11., 12.],\n",
    "                    [4., 3., 2., 1.]]]])\n",
    "\n",
    "print(f\"輸入 (4×4):\\n{x.squeeze()}\")\n",
    "\n",
    "# Max Pooling\n",
    "max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "max_out = max_pool(x)\n",
    "print(f\"\\nMax Pooling 2×2 (取最大值):\\n{max_out.squeeze()}\")\n",
    "\n",
    "# Average Pooling\n",
    "avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "avg_out = avg_pool(x)\n",
    "print(f\"\\nAverage Pooling 2×2 (取平均):\\n{avg_out.squeeze()}\")\n",
    "\n",
    "# 視覺化\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(x.squeeze(), cmap='viridis')\n",
    "axes[0].set_title('Input (4×4)')\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        axes[0].text(j, i, f'{x[0,0,i,j]:.0f}', ha='center', va='center', color='white')\n",
    "\n",
    "axes[1].imshow(max_out.squeeze(), cmap='viridis')\n",
    "axes[1].set_title('Max Pool (2×2)')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[1].text(j, i, f'{max_out[0,0,i,j]:.0f}', ha='center', va='center', color='white')\n",
    "\n",
    "axes[2].imshow(avg_out.squeeze(), cmap='viridis')\n",
    "axes[2].set_title('Avg Pool (2×2)')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axes[2].text(j, i, f'{avg_out[0,0,i,j]:.1f}', ha='center', va='center', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Global Average Pooling\n",
    "\n",
    "**概念：** 對整個 feature map 取平均，常用在網路最後取代全連接層。\n",
    "\n",
    "**優點：**\n",
    "- 大幅減少參數\n",
    "- 對輸入尺寸更彈性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Average Pooling\n",
    "\n",
    "x = torch.randn(1, 512, 7, 7)  # 典型的 CNN 輸出\n",
    "print(f\"輸入 shape: {x.shape}\")\n",
    "\n",
    "# 傳統做法：Flatten + Linear\n",
    "flatten = nn.Flatten()\n",
    "fc = nn.Linear(512 * 7 * 7, 10)\n",
    "out_fc = fc(flatten(x))\n",
    "print(f\"Flatten + FC: {512*7*7} -> 10, 參數量 = {512*7*7*10 + 10:,}\")\n",
    "\n",
    "# Global Average Pooling + Linear\n",
    "gap = nn.AdaptiveAvgPool2d(1)  # 輸出 1×1\n",
    "fc_small = nn.Linear(512, 10)\n",
    "out_gap = fc_small(gap(x).view(1, -1))\n",
    "print(f\"GAP + FC: 512 -> 10, 參數量 = {512*10 + 10:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4：經典 CNN 架構\n",
    "\n",
    "### 4.1 基本 CNN 結構\n",
    "\n",
    "```\n",
    "Input → [Conv → ReLU → Pool] × N → Flatten → FC → Output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡單的 CNN\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 卷積層\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        # 池化層\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # 全連接層\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv block 1: 32x32 -> 16x16\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        \n",
    "        # Conv block 2: 16x16 -> 8x8\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # Conv block 3: 8x8 -> 4x4\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 測試\n",
    "model = SimpleCNN()\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "print(f\"輸入: {x.shape}\")\n",
    "print(f\"輸出: {model(x).shape}\")\n",
    "\n",
    "# 參數量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n總參數量: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 追蹤每一層的輸出 shape\n",
    "\n",
    "def trace_shapes(model, input_shape=(1, 3, 32, 32)):\n",
    "    \"\"\"追蹤模型每一層的輸出 shape\"\"\"\n",
    "    x = torch.randn(input_shape)\n",
    "    print(f\"Input: {x.shape}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Conv blocks\n",
    "    x = model.pool(F.relu(model.conv1(x)))\n",
    "    print(f\"After conv1 + pool: {x.shape}\")\n",
    "    \n",
    "    x = model.pool(F.relu(model.conv2(x)))\n",
    "    print(f\"After conv2 + pool: {x.shape}\")\n",
    "    \n",
    "    x = model.pool(F.relu(model.conv3(x)))\n",
    "    print(f\"After conv3 + pool: {x.shape}\")\n",
    "    \n",
    "    x = x.view(x.size(0), -1)\n",
    "    print(f\"After flatten: {x.shape}\")\n",
    "    \n",
    "    x = F.relu(model.fc1(x))\n",
    "    print(f\"After fc1: {x.shape}\")\n",
    "    \n",
    "    x = model.fc2(x)\n",
    "    print(f\"After fc2 (output): {x.shape}\")\n",
    "\n",
    "trace_shapes(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 現代 CNN：使用 BatchNorm\n",
    "\n",
    "**Conv → BatchNorm → ReLU** 是現代 CNN 的標準組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 現代 CNN 設計\n",
    "\n",
    "class ModernCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Conv block helper\n",
    "        def conv_block(in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2, 2)\n",
    "            )\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            conv_block(3, 64),      # 32 -> 16\n",
    "            conv_block(64, 128),    # 16 -> 8\n",
    "            conv_block(128, 256),   # 8 -> 4\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = ModernCNN()\n",
    "print(model)\n",
    "print(f\"\\n總參數量: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5：殘差連接 (Residual Connection)\n",
    "\n",
    "### 5.1 為什麼需要殘差連接？\n",
    "\n",
    "**問題：** 網路越深，訓練越困難（梯度消失/爆炸、退化問題）\n",
    "\n",
    "**ResNet 的解決方案：** 讓網路學習「殘差」而不是完整的映射\n",
    "\n",
    "```\n",
    "原本要學：H(x)\n",
    "改成學：F(x) = H(x) - x （殘差）\n",
    "輸出：H(x) = F(x) + x （加回 x）\n",
    "```\n",
    "\n",
    "**直覺：** 如果什麼都不學，至少可以傳遞 x（identity），不會比淺網路差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 殘差連接視覺化\n",
    "\n",
    "print(\"\"\"\n",
    "傳統網路：\n",
    "    x → [Conv → BN → ReLU → Conv → BN] → ReLU → y\n",
    "    \n",
    "    y = F(x)\n",
    "\n",
    "殘差網路 (ResNet)：\n",
    "    x → [Conv → BN → ReLU → Conv → BN] → (+) → ReLU → y\n",
    "    └──────────────────────────────────────┘\n",
    "                    Skip Connection\n",
    "    \n",
    "    y = F(x) + x\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實作 Residual Block\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"基本的殘差區塊\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 主要路徑\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, \n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Skip connection（如果維度不同需要調整）\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, \n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 主要路徑\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # 加上 skip connection\n",
    "        out += self.shortcut(x)\n",
    "        \n",
    "        # 最後的 ReLU\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 測試\n",
    "block = ResidualBlock(64, 64)\n",
    "x = torch.randn(1, 64, 32, 32)\n",
    "print(f\"Same dimensions: {x.shape} -> {block(x).shape}\")\n",
    "\n",
    "block_down = ResidualBlock(64, 128, stride=2)\n",
    "print(f\"Downsampling: {x.shape} -> {block_down(x).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小型 ResNet\n",
    "\n",
    "class SmallResNet(nn.Module):\n",
    "    \"\"\"簡化版 ResNet for CIFAR-10\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 初始卷積\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # 殘差層\n",
    "        self.layer1 = self._make_layer(64, 64, 2, stride=1)   # 32x32\n",
    "        self.layer2 = self._make_layer(64, 128, 2, stride=2)  # 16x16\n",
    "        self.layer3 = self._make_layer(128, 256, 2, stride=2) # 8x8\n",
    "        \n",
    "        # 分類器\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def _make_layer(self, in_ch, out_ch, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(in_ch, out_ch, stride))\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(out_ch, out_ch))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "resnet = SmallResNet()\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "print(f\"輸入: {x.shape}\")\n",
    "print(f\"輸出: {resnet(x).shape}\")\n",
    "print(f\"\\n總參數量: {sum(p.numel() for p in resnet.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6：資料增強 (Data Augmentation)\n",
    "\n",
    "### 6.1 為什麼需要資料增強？\n",
    "\n",
    "- 增加訓練資料的多樣性\n",
    "- 減少過擬合\n",
    "- 讓模型學到更穩健的特徵\n",
    "\n",
    "### 6.2 常用的影像增強方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料增強示範\n",
    "\n",
    "# 載入一張範例圖片\n",
    "transform_basic = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 載入 CIFAR-10 的一張圖\n",
    "cifar_temp = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                           download=True, transform=transform_basic)\n",
    "sample_img, label = cifar_temp[0]\n",
    "sample_pil = transforms.ToPILImage()(sample_img)\n",
    "\n",
    "# 各種增強方法\n",
    "augmentations = {\n",
    "    'Original': transforms.Compose([]),\n",
    "    'HorizontalFlip': transforms.RandomHorizontalFlip(p=1.0),\n",
    "    'Rotation': transforms.RandomRotation(30),\n",
    "    'ColorJitter': transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
    "    'RandomCrop': transforms.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
    "    'GaussianBlur': transforms.GaussianBlur(3),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, aug) in enumerate(augmentations.items()):\n",
    "    if name == 'Original':\n",
    "        img = sample_pil\n",
    "    else:\n",
    "        torch.manual_seed(42)  # 固定隨機種子方便展示\n",
    "        img = aug(sample_pil)\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(name)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Data Augmentation Examples', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 實際使用的增強組合\n",
    "\n",
    "# 訓練時的增強（較強）\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),      # 隨機裁剪\n",
    "    transforms.RandomHorizontalFlip(),          # 隨機水平翻轉\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # 顏色抖動\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),  # CIFAR-10 的均值\n",
    "                         (0.2023, 0.1994, 0.2010))   # CIFAR-10 的標準差\n",
    "])\n",
    "\n",
    "# 測試時不增強\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# 展示同一張圖的多次增強結果\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    # 每次隨機增強\n",
    "    aug_img = train_transform(sample_pil)\n",
    "    # 反正規化以便顯示\n",
    "    aug_img = aug_img * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "    aug_img = aug_img + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "    aug_img = aug_img.clamp(0, 1)\n",
    "    \n",
    "    axes[i].imshow(aug_img.permute(1, 2, 0))\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Same Image with Random Augmentations', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7：完整實作 - CIFAR-10 分類\n",
    "\n",
    "把所有概念結合，訓練一個完整的 CNN 來分類 CIFAR-10。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 CIFAR-10\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=test_transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, \n",
    "                          num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False,\n",
    "                         num_workers=2, pin_memory=True)\n",
    "\n",
    "# CIFAR-10 類別\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(f\"訓練集: {len(train_dataset)} 張\")\n",
    "print(f\"測試集: {len(test_dataset)} 張\")\n",
    "print(f\"類別: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顯示一些範例圖片\n",
    "\n",
    "def imshow(img, title=None):\n",
    "    \"\"\"顯示圖片（反正規化）\"\"\"\n",
    "    img = img * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "    img = img + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "    img = img.clamp(0, 1)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "\n",
    "# 取一批資料\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# 顯示前 16 張\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    img = images[i]\n",
    "    img = img * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "    img = img + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "    img = img.clamp(0, 1)\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    ax.set_title(classes[labels[i]])\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立模型\n",
    "model = SmallResNet(num_classes=10).to(device)\n",
    "\n",
    "# 損失函數和優化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# 學習率調度器\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "print(f\"模型參數量: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練函數\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / total, 100. * correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / total, 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練！\n",
    "\n",
    "num_epochs = 20\n",
    "history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [], 'lr': []}\n",
    "\n",
    "print(\"開始訓練...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), 'best_cifar10_model.pth')\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1:2d}/{num_epochs}] \"\n",
    "          f\"LR: {current_lr:.6f} | \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"最佳測試準確率: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化訓練過程\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "ax = axes[0]\n",
    "ax.plot(history['train_loss'], label='Train')\n",
    "ax.plot(history['test_loss'], label='Test')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss Curve')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Accuracy\n",
    "ax = axes[1]\n",
    "ax.plot(history['train_acc'], label='Train')\n",
    "ax.plot(history['test_acc'], label='Test')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Accuracy Curve')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Learning Rate\n",
    "ax = axes[2]\n",
    "ax.plot(history['lr'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate Schedule (Cosine)')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看預測結果\n",
    "\n",
    "model.eval()\n",
    "images, labels = next(iter(test_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    probs = F.softmax(outputs, dim=1)\n",
    "    _, predicted = outputs.max(1)\n",
    "\n",
    "# 顯示前 16 個預測\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 5))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    img = images[i].cpu()\n",
    "    img = img * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "    img = img + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "    img = img.clamp(0, 1)\n",
    "    \n",
    "    pred_class = classes[predicted[i]]\n",
    "    true_class = classes[labels[i]]\n",
    "    prob = probs[i, predicted[i]].item()\n",
    "    \n",
    "    color = 'green' if predicted[i] == labels[i] else 'red'\n",
    "    \n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    ax.set_title(f'P: {pred_class}\\nT: {true_class}\\n{prob:.1%}', \n",
    "                 color=color, fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每個類別的準確率\n",
    "\n",
    "class_correct = [0] * 10\n",
    "class_total = [0] * 10\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i].item()\n",
    "            class_total[label] += 1\n",
    "            if predicted[i] == labels[i]:\n",
    "                class_correct[label] += 1\n",
    "\n",
    "print(\"每個類別的準確率：\")\n",
    "print(\"-\" * 30)\n",
    "for i in range(10):\n",
    "    acc = 100 * class_correct[i] / class_total[i]\n",
    "    print(f\"{classes[i]:>10}: {acc:.1f}%\")\n",
    "\n",
    "# 視覺化\n",
    "plt.figure(figsize=(10, 5))\n",
    "accs = [100 * class_correct[i] / class_total[i] for i in range(10)]\n",
    "bars = plt.bar(classes, accs)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Per-Class Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 標記最低和最高\n",
    "min_idx = np.argmin(accs)\n",
    "max_idx = np.argmax(accs)\n",
    "bars[min_idx].set_color('red')\n",
    "bars[max_idx].set_color('green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 練習題（已完成，請閱讀理解）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 1：視覺化 CNN 學到的 Filter\n",
    "\n",
    "**目標：** 看看 CNN 第一層學到了什麼樣的 filter\n",
    "\n",
    "**Hint：** 第一層的 filter 通常學到邊緣、顏色等低階特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1：視覺化 filters\n",
    "\n",
    "# 取得第一層卷積的權重\n",
    "first_conv = model.conv1\n",
    "weights = first_conv.weight.data.cpu()\n",
    "\n",
    "print(f\"第一層 filter shape: {weights.shape}\")\n",
    "print(f\"  = (out_channels, in_channels, kH, kW)\")\n",
    "\n",
    "# 視覺化前 32 個 filters\n",
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(32):\n",
    "    # 取一個 filter，shape = (3, 3, 3)\n",
    "    filt = weights[i]\n",
    "    # 正規化到 0-1\n",
    "    filt = (filt - filt.min()) / (filt.max() - filt.min())\n",
    "    # 轉成 (H, W, C) 格式\n",
    "    filt = filt.permute(1, 2, 0)\n",
    "    \n",
    "    axes[i].imshow(filt)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'Filter {i}')\n",
    "\n",
    "plt.suptitle('First Conv Layer Filters (learned edge/color detectors)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：視覺化 Feature Maps\n",
    "\n",
    "**目標：** 看看圖片經過卷積後變成什麼樣子\n",
    "\n",
    "**Hint：** Feature map 顯示了「哪些位置觸發了這個 filter」"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2：視覺化 feature maps\n",
    "\n",
    "# 取一張測試圖片\n",
    "img, label = test_dataset[0]\n",
    "img_input = img.unsqueeze(0).to(device)\n",
    "\n",
    "# 獲取中間層的輸出\n",
    "model.eval()\n",
    "\n",
    "# 第一層 conv 後的 feature maps\n",
    "with torch.no_grad():\n",
    "    x = model.conv1(img_input)\n",
    "    x = model.bn1(x)\n",
    "    x = F.relu(x)\n",
    "\n",
    "feature_maps = x.cpu().squeeze()\n",
    "print(f\"Feature maps shape: {feature_maps.shape}\")\n",
    "\n",
    "# 顯示原圖和前 15 個 feature maps\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 原圖\n",
    "orig = img.cpu()\n",
    "orig = orig * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "orig = orig + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "orig = orig.clamp(0, 1)\n",
    "axes[0].imshow(orig.permute(1, 2, 0))\n",
    "axes[0].set_title(f'Original\\n({classes[label]})')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Feature maps\n",
    "for i in range(15):\n",
    "    axes[i+1].imshow(feature_maps[i], cmap='viridis')\n",
    "    axes[i+1].set_title(f'FM {i}')\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.suptitle('Feature Maps after First Conv Layer', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3：比較有無資料增強的效果\n",
    "\n",
    "**目標：** 證明資料增強可以減少過擬合\n",
    "\n",
    "**Hint：** 無增強時，train acc 會很高但 test acc 較低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 3：比較有無資料增強\n",
    "\n",
    "# 無增強的 transform\n",
    "no_aug_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# 載入無增強的資料\n",
    "train_no_aug = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=no_aug_transform\n",
    ")\n",
    "train_loader_no_aug = DataLoader(train_no_aug, batch_size=128, shuffle=True)\n",
    "\n",
    "# 訓練函數（快速版）\n",
    "def quick_train(model, train_loader, test_loader, epochs=10):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    history = {'train_acc': [], 'test_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        history['train_acc'].append(100. * correct / total)\n",
    "        \n",
    "        # Test\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        history['test_acc'].append(100. * correct / total)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train {history['train_acc'][-1]:.1f}%, Test {history['test_acc'][-1]:.1f}%\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# 訓練兩個模型\n",
    "print(\"=\" * 50)\n",
    "print(\"訓練無資料增強的模型：\")\n",
    "print(\"=\" * 50)\n",
    "torch.manual_seed(42)\n",
    "model_no_aug = SmallResNet()\n",
    "history_no_aug = quick_train(model_no_aug, train_loader_no_aug, test_loader, epochs=10)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"訓練有資料增強的模型：\")\n",
    "print(\"=\" * 50)\n",
    "torch.manual_seed(42)\n",
    "model_with_aug = SmallResNet()\n",
    "history_with_aug = quick_train(model_with_aug, train_loader, test_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較結果\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 無增強\n",
    "ax = axes[0]\n",
    "ax.plot(history_no_aug['train_acc'], 'b-', label='Train')\n",
    "ax.plot(history_no_aug['test_acc'], 'r-', label='Test')\n",
    "ax.fill_between(range(10), history_no_aug['train_acc'], history_no_aug['test_acc'], \n",
    "                alpha=0.3, color='red', label='Overfitting Gap')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Without Data Augmentation')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# 有增強\n",
    "ax = axes[1]\n",
    "ax.plot(history_with_aug['train_acc'], 'b-', label='Train')\n",
    "ax.plot(history_with_aug['test_acc'], 'r-', label='Test')\n",
    "ax.fill_between(range(10), history_with_aug['train_acc'], history_with_aug['test_acc'], \n",
    "                alpha=0.3, color='green', label='Smaller Gap')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('With Data Augmentation')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n無增強 - 最終 Train: {history_no_aug['train_acc'][-1]:.1f}%, Test: {history_no_aug['test_acc'][-1]:.1f}%\")\n",
    "print(f\"有增強 - 最終 Train: {history_with_aug['train_acc'][-1]:.1f}%, Test: {history_with_aug['test_acc'][-1]:.1f}%\")\n",
    "print(f\"\\n觀察：無增強時 Train-Test 差距更大（過擬合），有增強時差距較小且 Test 準確率更高\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 4：使用預訓練模型 (Transfer Learning)\n",
    "\n",
    "**目標：** 使用 torchvision 提供的預訓練 ResNet\n",
    "\n",
    "**Hint：** 預訓練模型已經學到了通用的視覺特徵，微調會更快更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 4：Transfer Learning\n",
    "\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# 載入預訓練的 ResNet18\n",
    "pretrained_model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# 修改最後一層以適應 CIFAR-10（10 類）\n",
    "# 原本是 1000 類（ImageNet）\n",
    "pretrained_model.fc = nn.Linear(pretrained_model.fc.in_features, 10)\n",
    "\n",
    "# 因為 CIFAR-10 是 32x32，需要調整第一層\n",
    "# 原本 ResNet 假設輸入是 224x224\n",
    "pretrained_model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "pretrained_model.maxpool = nn.Identity()  # 移除 maxpool（圖片太小）\n",
    "\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "\n",
    "print(f\"預訓練 ResNet18 參數量: {sum(p.numel() for p in pretrained_model.parameters()):,}\")\n",
    "\n",
    "# 快速訓練\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"使用預訓練 ResNet18：\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(pretrained_model.parameters(), lr=0.0001)  # 較小的學習率\n",
    "\n",
    "history_pretrained = {'train_acc': [], 'test_acc': []}\n",
    "\n",
    "for epoch in range(5):  # 只訓練 5 個 epoch\n",
    "    pretrained_model.train()\n",
    "    correct, total = 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = pretrained_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    history_pretrained['train_acc'].append(100. * correct / total)\n",
    "    \n",
    "    pretrained_model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = pretrained_model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    history_pretrained['test_acc'].append(100. * correct / total)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train {history_pretrained['train_acc'][-1]:.1f}%, \"\n",
    "          f\"Test {history_pretrained['test_acc'][-1]:.1f}%\")\n",
    "\n",
    "print(f\"\\n預訓練模型只用 5 個 epoch 就達到 {history_pretrained['test_acc'][-1]:.1f}% 準確率！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Module 3 中場總結\n\n### 核心概念\n\n1. **為什麼用 CNN**：\n   - 局部連接 + 權重共享 = 參數少\n   - 平移不變性\n   - 利用影像的空間結構\n\n2. **卷積運算**：\n   - kernel/filter 在圖片上滑動，做內積\n   - 重要參數：kernel_size, stride, padding\n   - 輸出尺寸公式：`(input - kernel + 2*padding) / stride + 1`\n\n3. **池化層**：\n   - 降低維度，增加感受野\n   - Max Pooling vs Average Pooling\n   - Global Average Pooling 取代全連接\n\n4. **殘差連接（ResNet）**：\n   - `output = F(x) + x`\n   - 解決深網路訓練困難的問題\n\n5. **資料增強**：\n   - 增加資料多樣性，減少過擬合\n   - 常用：RandomCrop, HorizontalFlip, ColorJitter"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 8：進階 CNN 概念\n\n### 8.1 感受野（Receptive Field）\n\n**概念：** 每個輸出神經元能「看到」的輸入區域大小\n\n**重要性：** \n- 感受野太小 → 無法捕捉大範圍的模式\n- 感受野太大 → 可能失去局部細節\n\n**計算公式：** \n- 對於連續的 3×3 卷積：RF = 1 + 2 × (層數)\n- stride 會快速增加感受野",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 感受野計算\n\ndef calculate_receptive_field(layers):\n    \"\"\"\n    計算一系列卷積層後的感受野\n    layers: list of (kernel_size, stride, padding)\n    \"\"\"\n    rf = 1  # 起始感受野\n    stride_product = 1  # 累積 stride\n    \n    print(\"感受野計算：\")\n    print(\"-\" * 50)\n    \n    for i, (k, s, p) in enumerate(layers):\n        rf = rf + (k - 1) * stride_product\n        stride_product *= s\n        print(f\"Layer {i+1}: kernel={k}, stride={s} -> RF = {rf}\")\n    \n    return rf\n\n# 例子：VGG-style 網路\nprint(\"VGG-style (多個 3x3 卷積 + maxpool):\")\nvgg_layers = [\n    (3, 1, 1), (3, 1, 1), (2, 2, 0),  # Conv, Conv, Pool\n    (3, 1, 1), (3, 1, 1), (2, 2, 0),  # Conv, Conv, Pool\n    (3, 1, 1), (3, 1, 1), (2, 2, 0),  # Conv, Conv, Pool\n]\nrf_vgg = calculate_receptive_field(vgg_layers)\nprint(f\"\\n最終感受野: {rf_vgg}×{rf_vgg}\")\n\nprint(\"\\n\" + \"=\" * 50)\nprint(\"\\nResNet-style (有 stride=2 的卷積):\")\nresnet_layers = [\n    (7, 2, 3),  # Initial conv\n    (3, 2, 1),  # MaxPool\n    (3, 1, 1), (3, 1, 1),  # Block 1\n    (3, 2, 1), (3, 1, 1),  # Block 2 (downsample)\n    (3, 2, 1), (3, 1, 1),  # Block 3 (downsample)\n]\nrf_resnet = calculate_receptive_field(resnet_layers)\nprint(f\"\\n最終感受野: {rf_resnet}×{rf_resnet}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 8.2 Grad-CAM：CNN 可解釋性\n\n**問題：** CNN 是「黑盒子」，我們想知道它「看哪裡」做決策\n\n**Grad-CAM：** 利用梯度找出對預測最重要的區域\n1. 對目標類別計算梯度\n2. 找出哪些 feature map 對該類別最重要\n3. 用這些權重組合 feature maps，得到熱力圖",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 簡化版 Grad-CAM 實現\n\nclass GradCAM:\n    \"\"\"簡化版 Grad-CAM\"\"\"\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n        \n        # 註冊 hooks\n        target_layer.register_forward_hook(self.save_activation)\n        target_layer.register_full_backward_hook(self.save_gradient)\n    \n    def save_activation(self, module, input, output):\n        self.activations = output.detach()\n    \n    def save_gradient(self, module, grad_input, grad_output):\n        self.gradients = grad_output[0].detach()\n    \n    def generate(self, input_image, target_class=None):\n        # Forward pass\n        output = self.model(input_image)\n        \n        if target_class is None:\n            target_class = output.argmax(dim=1)\n        \n        # Backward pass\n        self.model.zero_grad()\n        one_hot = torch.zeros_like(output)\n        one_hot[0, target_class] = 1\n        output.backward(gradient=one_hot)\n        \n        # 計算權重：對梯度做 global average pooling\n        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n        \n        # 加權組合 feature maps\n        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n        cam = F.relu(cam)  # 只保留正值\n        \n        # 正規化到 0-1\n        cam = cam - cam.min()\n        cam = cam / (cam.max() + 1e-8)\n        \n        # Resize 到原圖大小\n        cam = F.interpolate(cam, size=input_image.shape[2:], mode='bilinear', align_corners=False)\n        \n        return cam.squeeze().cpu().numpy()\n\n# 使用 Grad-CAM\nmodel.eval()\n\n# 獲取最後一個卷積層\ntarget_layer = model.layer3[-1].conv2  # SmallResNet 的最後一個卷積\n\ngrad_cam = GradCAM(model, target_layer)\n\n# 對幾張測試圖片生成 CAM\nfig, axes = plt.subplots(3, 4, figsize=(16, 12))\n\nfor row in range(3):\n    img, label = test_dataset[row * 10]\n    img_input = img.unsqueeze(0).to(device)\n    img_input.requires_grad = True\n    \n    # 生成 CAM\n    cam = grad_cam.generate(img_input)\n    \n    # 取得預測\n    with torch.no_grad():\n        pred = model(img_input.detach()).argmax(dim=1).item()\n    \n    # 原圖\n    orig = img.cpu()\n    orig = orig * torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n    orig = orig + torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n    orig = orig.clamp(0, 1).permute(1, 2, 0).numpy()\n    \n    # 顯示\n    axes[row, 0].imshow(orig)\n    axes[row, 0].set_title(f'Original\\nTrue: {classes[label]}')\n    axes[row, 0].axis('off')\n    \n    axes[row, 1].imshow(cam, cmap='jet')\n    axes[row, 1].set_title('Grad-CAM')\n    axes[row, 1].axis('off')\n    \n    # 疊加\n    axes[row, 2].imshow(orig)\n    axes[row, 2].imshow(cam, cmap='jet', alpha=0.5)\n    axes[row, 2].set_title(f'Overlay\\nPred: {classes[pred]}')\n    axes[row, 2].axis('off')\n    \n    # 只顯示高激活區域\n    mask = cam > 0.5\n    masked = orig * mask[:, :, np.newaxis]\n    axes[row, 3].imshow(masked)\n    axes[row, 3].set_title('High Activation Region')\n    axes[row, 3].axis('off')\n\nplt.suptitle('Grad-CAM Visualization: Where does the model look?', fontsize=14)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 8.3 深度可分離卷積（Depthwise Separable Convolution）\n\n**問題：** 標準卷積參數量太大\n\n**解決方案：** 把卷積分成兩步\n1. **Depthwise Conv**：每個通道單獨做卷積\n2. **Pointwise Conv**：用 1×1 卷積混合通道\n\n**用途：** MobileNet, EfficientNet 等輕量級網路",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 深度可分離卷積 vs 標準卷積\n\nclass DepthwiseSeparableConv(nn.Module):\n    \"\"\"深度可分離卷積\"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n        super().__init__()\n        # Depthwise: 每個通道單獨卷積\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size,\n                                   stride=stride, padding=padding, \n                                   groups=in_channels)  # groups=in_channels!\n        # Pointwise: 1x1 卷積混合通道\n        self.pointwise = nn.Conv2d(in_channels, out_channels, 1)\n    \n    def forward(self, x):\n        x = self.depthwise(x)\n        x = self.pointwise(x)\n        return x\n\n# 比較參數量\nin_ch, out_ch = 64, 128\nkernel_size = 3\n\n# 標準卷積\nstandard_conv = nn.Conv2d(in_ch, out_ch, kernel_size, padding=1)\nstandard_params = sum(p.numel() for p in standard_conv.parameters())\n\n# 深度可分離卷積\ndw_sep_conv = DepthwiseSeparableConv(in_ch, out_ch, kernel_size)\ndw_sep_params = sum(p.numel() for p in dw_sep_conv.parameters())\n\nprint(\"參數量比較：\")\nprint(f\"標準卷積 (in={in_ch}, out={out_ch}, k={kernel_size}):\")\nprint(f\"  參數量 = {in_ch} × {out_ch} × {kernel_size}² + {out_ch} = {standard_params:,}\")\nprint(f\"\\n深度可分離卷積:\")\nprint(f\"  Depthwise: {in_ch} × 1 × {kernel_size}² + {in_ch} = {in_ch * kernel_size**2 + in_ch}\")\nprint(f\"  Pointwise: {in_ch} × {out_ch} × 1 + {out_ch} = {in_ch * out_ch + out_ch}\")\nprint(f\"  總參數量 = {dw_sep_params:,}\")\nprint(f\"\\n深度可分離卷積只需要 {dw_sep_params/standard_params*100:.1f}% 的參數！\")\n\n# 測試輸出\nx = torch.randn(1, in_ch, 32, 32)\nprint(f\"\\n輸入 shape: {x.shape}\")\nprint(f\"標準卷積輸出: {standard_conv(x).shape}\")\nprint(f\"深度可分離卷積輸出: {dw_sep_conv(x).shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Module 3 完整總結\n\n### 🎯 核心概念\n\n| 概念 | 說明 | 重要程度 |\n|------|------|----------|\n| **卷積運算** | Filter 滑動做內積，提取局部特徵 | ⭐⭐⭐⭐⭐ |\n| **權重共享** | 同一 filter 用於整張圖，大幅減少參數 | ⭐⭐⭐⭐⭐ |\n| **池化層** | 降維、增加感受野、平移不變性 | ⭐⭐⭐⭐ |\n| **殘差連接** | output = F(x) + x，解決深網路訓練問題 | ⭐⭐⭐⭐⭐ |\n| **資料增強** | 增加多樣性，減少過擬合 | ⭐⭐⭐⭐⭐ |\n| **BatchNorm** | 穩定訓練，加速收斂 | ⭐⭐⭐⭐ |\n\n### 🔑 關鍵公式\n\n```\n卷積輸出尺寸：\noutput_size = (input - kernel + 2×padding) / stride + 1\n\n感受野計算：\nRF_new = RF_old + (kernel - 1) × stride_product\n\n參數量（標準卷積）：\nparams = in_channels × out_channels × kernel_h × kernel_w + out_channels\n\n參數量（深度可分離卷積）：\nparams = in_ch × kernel² + in_ch × out_ch   # 約 1/kernel² 的壓縮\n```\n\n### 🏗️ 經典架構演進\n\n```\nLeNet (1998)    → 首個成功的 CNN\nAlexNet (2012)  → ImageNet 突破，使用 ReLU、Dropout、GPU\nVGG (2014)      → 堆疊 3×3 卷積，證明深度重要\nGoogLeNet (2014)→ Inception module，多尺度特徵\nResNet (2015)   → 殘差連接，深度可達 152+ 層\nDenseNet (2017) → 密集連接，特徵重用\nMobileNet (2017)→ 深度可分離卷積，輕量化\nEfficientNet (2019)→ 複合縮放，效率與準確率平衡\n```\n\n### 💡 實務技巧\n\n1. **資料增強必備三件套**：\n   - RandomCrop + padding\n   - RandomHorizontalFlip\n   - Normalize（使用資料集的均值/標準差）\n\n2. **現代 CNN 標準模組**：\n   ```\n   Conv → BatchNorm → ReLU → Dropout(optional)\n   ```\n\n3. **常見超參數**：\n   - kernel_size: 3×3（小但有效）\n   - stride: 1（保持尺寸）或 2（降維）\n   - padding: 'same' 或 (kernel-1)//2\n\n4. **遷移學習優先**：\n   - 優先使用預訓練模型\n   - 小資料集：凍結前面層，只訓練分類器\n   - 大資料集：全部微調，但用較小學習率\n\n5. **調試技巧**：\n   - 用 Grad-CAM 檢查模型「看哪裡」\n   - 視覺化 filter 和 feature maps\n   - 監控每類準確率，找出困難類別\n\n### 🚀 下一步\n\n- Module 4：RNN 與序列資料（文字、時間序列）\n- 後續可以探索：Vision Transformer (ViT)、自監督學習 (SimCLR)\n\n---\n\n**恭喜完成 CNN 模組！** 🎉\n\nCNN 是深度學習最重要的基礎架構之一，掌握了卷積、池化、殘差連接這些概念，你就能理解大部分影像相關的模型。",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}