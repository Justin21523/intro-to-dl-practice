{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6: LLM å¯¦å‹™æ‡‰ç”¨\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "- ç†è§£å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¯¦å‹™è€ƒé‡\n",
    "- å­¸æœƒä½¿ç”¨é‡åŒ–ï¼ˆQuantizationï¼‰è¼‰å…¥å¤§å‹æ¨¡å‹\n",
    "- æŒæ¡ LoRA/PEFT é«˜æ•ˆå¾®èª¿æŠ€è¡“\n",
    "- ç›£æ§å’Œå„ªåŒ– VRAM ä½¿ç”¨\n",
    "\n",
    "## ç‚ºä»€éº¼éœ€è¦é€™äº›æŠ€è¡“ï¼Ÿ\n",
    "\n",
    "ç¾ä»£ LLM å‹•è¼’æ•¸åå„„åƒæ•¸ï¼š\n",
    "- **7B æ¨¡å‹**ï¼šå…¨ç²¾åº¦éœ€è¦ ~28GB VRAMï¼ˆfp32ï¼‰æˆ– ~14GBï¼ˆfp16ï¼‰\n",
    "- **13B æ¨¡å‹**ï¼šå…¨ç²¾åº¦éœ€è¦ ~52GB VRAM\n",
    "- **70B æ¨¡å‹**ï¼šéœ€è¦å¤šå¼µ A100 80GB\n",
    "\n",
    "ä½ çš„ RTX 5080 æœ‰ 16GB VRAMï¼Œé€éä»¥ä¸‹æŠ€è¡“å¯ä»¥ï¼š\n",
    "1. **é‡åŒ–**ï¼šå°‡ 7B æ¨¡å‹å£“ç¸®åˆ° 4-8GB\n",
    "2. **LoRA**ï¼šåªè¨“ç·´æ¥µå°‘é‡åƒæ•¸ï¼Œç¯€çœ VRAM\n",
    "3. **æ¢¯åº¦ç´¯ç©**ï¼šæ¨¡æ“¬å¤§ batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: ç’°å¢ƒæº–å‚™èˆ‡ VRAM ç›£æ§\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘å€‘éœ€è¦å®‰è£ä¸€äº›é¡å¤–çš„å¥—ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£å¿…è¦å¥—ä»¶ï¼ˆåªéœ€åŸ·è¡Œä¸€æ¬¡ï¼‰\n",
    "# !pip install transformers accelerate bitsandbytes peft datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from typing import Optional\n",
    "\n",
    "def get_gpu_memory_info():\n",
    "    \"\"\"ç²å–è©³ç´°çš„ GPU è¨˜æ†¶é«”è³‡è¨Š\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return \"CUDA ä¸å¯ç”¨\"\n",
    "    \n",
    "    # ç²å–è¨˜æ†¶é«”è³‡è¨Šï¼ˆä»¥ MB ç‚ºå–®ä½ï¼‰\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1024**2\n",
    "    free = total - reserved\n",
    "    \n",
    "    info = f\"\"\"\n",
    "    GPU: {torch.cuda.get_device_name(0)}\n",
    "    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    å·²åˆ†é… (Allocated): {allocated:,.0f} MB\n",
    "    å·²ä¿ç•™ (Reserved):  {reserved:,.0f} MB\n",
    "    å¯ç”¨ (Free):        {free:,.0f} MB\n",
    "    ç¸½è¨ˆ (Total):       {total:,.0f} MB\n",
    "    ä½¿ç”¨ç‡:             {(reserved/total)*100:.1f}%\n",
    "    \"\"\"\n",
    "    return info\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"æ¸…ç† GPU è¨˜æ†¶é«”\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"GPU è¨˜æ†¶é«”å·²æ¸…ç†\")\n",
    "\n",
    "# é¡¯ç¤ºåˆå§‹ç‹€æ…‹\n",
    "print(get_gpu_memory_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VRAM ä½¿ç”¨è¿½è¹¤å™¨\n",
    "\n",
    "å»ºç«‹ä¸€å€‹ context manager ä¾†è¿½è¹¤æ“ä½œçš„ VRAM æ¶ˆè€—ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRAMTracker:\n",
    "    \"\"\"è¿½è¹¤ç¨‹å¼ç¢¼å€å¡Šçš„ VRAM ä½¿ç”¨é‡\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str = \"Operation\"):\n",
    "        self.name = name\n",
    "        self.start_allocated = 0\n",
    "        self.start_reserved = 0\n",
    "        \n",
    "    def __enter__(self):\n",
    "        torch.cuda.synchronize()\n",
    "        self.start_allocated = torch.cuda.memory_allocated()\n",
    "        self.start_reserved = torch.cuda.memory_reserved()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        torch.cuda.synchronize()\n",
    "        end_allocated = torch.cuda.memory_allocated()\n",
    "        end_reserved = torch.cuda.memory_reserved()\n",
    "        \n",
    "        delta_allocated = (end_allocated - self.start_allocated) / 1024**2\n",
    "        delta_reserved = (end_reserved - self.start_reserved) / 1024**2\n",
    "        \n",
    "        print(f\"\\n[{self.name}] VRAM è®ŠåŒ–:\")\n",
    "        print(f\"  åˆ†é…è®ŠåŒ–: {delta_allocated:+,.1f} MB\")\n",
    "        print(f\"  ä¿ç•™è®ŠåŒ–: {delta_reserved:+,.1f} MB\")\n",
    "\n",
    "# æ¸¬è©¦è¿½è¹¤å™¨\n",
    "with VRAMTracker(\"å»ºç«‹æ¸¬è©¦å¼µé‡\"):\n",
    "    # å»ºç«‹ä¸€å€‹ 1GB çš„å¼µé‡\n",
    "    test_tensor = torch.randn(256, 1024, 1024, device='cuda')\n",
    "\n",
    "# æ¸…ç†\n",
    "del test_tensor\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: æ¨¡å‹é‡åŒ– (Quantization)\n",
    "\n",
    "### é‡åŒ–çš„ç›´è§€ç†è§£\n",
    "\n",
    "é‡åŒ–å°±åƒæ˜¯ã€Œå£“ç¸®åœ–ç‰‡ã€ï¼š\n",
    "- **FP32**ï¼šæ¯å€‹åƒæ•¸ç”¨ 32 ä½å…ƒå„²å­˜ï¼ˆæœ€ç²¾ç¢ºï¼‰\n",
    "- **FP16/BF16**ï¼šæ¯å€‹åƒæ•¸ç”¨ 16 ä½å…ƒï¼ˆç²¾åº¦ç¨é™ï¼Œé€Ÿåº¦æ›´å¿«ï¼‰\n",
    "- **INT8**ï¼šæ¯å€‹åƒæ•¸ç”¨ 8 ä½å…ƒï¼ˆè¨˜æ†¶é«”æ¸›åŠï¼‰\n",
    "- **INT4**ï¼šæ¯å€‹åƒæ•¸ç”¨ 4 ä½å…ƒï¼ˆè¨˜æ†¶é«”å†æ¸›åŠï¼‰\n",
    "\n",
    "```\n",
    "7B æ¨¡å‹è¨˜æ†¶é«”éœ€æ±‚ï¼š\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ ç²¾åº¦        â”‚ VRAM éœ€æ±‚  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ FP32        â”‚ ~28 GB     â”‚\n",
    "â”‚ FP16/BF16   â”‚ ~14 GB     â”‚\n",
    "â”‚ INT8        â”‚ ~7 GB      â”‚\n",
    "â”‚ INT4        â”‚ ~3.5 GB    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ bitsandbytes é€²è¡Œ 4-bit é‡åŒ–\n",
    "\n",
    "bitsandbytes æ˜¯ç›®å‰æœ€æµè¡Œçš„é‡åŒ–å·¥å…·ï¼Œæ”¯æ´ 4-bit å’Œ 8-bit é‡åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# 4-bit é‡åŒ–é…ç½®\n",
    "bnb_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # ä½¿ç”¨ 4-bit é‡åŒ–\n",
    "    bnb_4bit_quant_type=\"nf4\",             # NormalFloat4 é‡åŒ–é¡å‹ï¼ˆæ¨è–¦ï¼‰\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # è¨ˆç®—æ™‚ä½¿ç”¨ bfloat16\n",
    "    bnb_4bit_use_double_quant=True,        # é›™é‡é‡åŒ–ï¼Œé€²ä¸€æ­¥ç¯€çœè¨˜æ†¶é«”\n",
    ")\n",
    "\n",
    "# 8-bit é‡åŒ–é…ç½®\n",
    "bnb_config_8bit = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,                      # ä½¿ç”¨ 8-bit é‡åŒ–\n",
    "    llm_int8_threshold=6.0,                # é›¢ç¾¤å€¼è™•ç†é–¾å€¼\n",
    ")\n",
    "\n",
    "print(\"é‡åŒ–é…ç½®å·²æº–å‚™\")\n",
    "print(f\"\\n4-bit é…ç½®: {bnb_config_4bit.to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¼‰å…¥é‡åŒ–æ¨¡å‹ç¤ºä¾‹\n",
    "\n",
    "æˆ‘å€‘ä½¿ç”¨ä¸€å€‹è¼ƒå°çš„æ¨¡å‹ä¾†ç¤ºç¯„ï¼ˆå› ç‚ºä¸‹è¼‰ 7B æ¨¡å‹éœ€è¦è¼ƒé•·æ™‚é–“ï¼‰ã€‚\n",
    "åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œç›¸åŒçš„ç¨‹å¼ç¢¼å¯ä»¥ç”¨æ–¼ä»»ä½•å¤§å°çš„æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹é€²è¡Œç¤ºç¯„\n",
    "# å¯¦éš›ä½¿ç”¨æ™‚å¯ä»¥æ›æˆ \"meta-llama/Llama-2-7b-hf\" ç­‰æ›´å¤§çš„æ¨¡å‹\n",
    "MODEL_NAME = \"microsoft/DialoGPT-medium\"  # ç´„ 355M åƒæ•¸ï¼Œé©åˆç¤ºç¯„\n",
    "\n",
    "def load_model_comparison():\n",
    "    \"\"\"æ¯”è¼ƒä¸åŒç²¾åº¦è¼‰å…¥æ¨¡å‹çš„ VRAM ä½¿ç”¨\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. è¼‰å…¥ FP16 æ¨¡å‹\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"è¼‰å…¥ FP16 æ¨¡å‹...\")\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    with VRAMTracker(\"FP16 è¼‰å…¥\"):\n",
    "        model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"cuda\"\n",
    "        )\n",
    "    results['fp16'] = torch.cuda.memory_allocated() / 1024**2\n",
    "    \n",
    "    del model_fp16\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # 2. è¼‰å…¥ 8-bit é‡åŒ–æ¨¡å‹\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"è¼‰å…¥ 8-bit é‡åŒ–æ¨¡å‹...\")\n",
    "    \n",
    "    with VRAMTracker(\"8-bit è¼‰å…¥\"):\n",
    "        model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            quantization_config=bnb_config_8bit,\n",
    "            device_map=\"cuda\"\n",
    "        )\n",
    "    results['int8'] = torch.cuda.memory_allocated() / 1024**2\n",
    "    \n",
    "    del model_8bit\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # 3. è¼‰å…¥ 4-bit é‡åŒ–æ¨¡å‹\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"è¼‰å…¥ 4-bit é‡åŒ–æ¨¡å‹...\")\n",
    "    \n",
    "    with VRAMTracker(\"4-bit è¼‰å…¥\"):\n",
    "        model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            quantization_config=bnb_config_4bit,\n",
    "            device_map=\"cuda\"\n",
    "        )\n",
    "    results['int4'] = torch.cuda.memory_allocated() / 1024**2\n",
    "    \n",
    "    del model_4bit\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # é¡¯ç¤ºæ¯”è¼ƒçµæœ\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"VRAM ä½¿ç”¨æ¯”è¼ƒï¼š\")\n",
    "    print(f\"  FP16:  {results['fp16']:,.0f} MB\")\n",
    "    print(f\"  INT8:  {results['int8']:,.0f} MB ({results['int8']/results['fp16']*100:.0f}% of FP16)\")\n",
    "    print(f\"  INT4:  {results['int4']:,.0f} MB ({results['int4']/results['fp16']*100:.0f}% of FP16)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# åŸ·è¡Œæ¯”è¼ƒï¼ˆå¯èƒ½éœ€è¦å¹¾åˆ†é˜ä¸‹è¼‰æ¨¡å‹ï¼‰\n",
    "# memory_comparison = load_model_comparison()\n",
    "print(\"å–æ¶ˆè¨»è§£ä¸Šé¢é€™è¡Œä¾†åŸ·è¡Œ VRAM æ¯”è¼ƒæ¸¬è©¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¼‰å…¥çœŸæ­£çš„ 7B æ¨¡å‹\n",
    "\n",
    "ä»¥ä¸‹æ˜¯è¼‰å…¥ 7B æ¨¡å‹çš„å®Œæ•´ç¯„ä¾‹ï¼ˆéœ€è¦ Hugging Face å¸³è™Ÿå’Œæ¨¡å‹å­˜å–æ¬Šé™ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_7b_model_4bit(model_name: str = \"meta-llama/Llama-2-7b-hf\"):\n",
    "    \"\"\"\n",
    "    è¼‰å…¥ 7B æ¨¡å‹çš„æ¨™æº–æµç¨‹\n",
    "    \n",
    "    å¸¸ç”¨çš„ 7B æ¨¡å‹ï¼š\n",
    "    - \"meta-llama/Llama-2-7b-hf\" (éœ€è¦ç”³è«‹å­˜å–)\n",
    "    - \"mistralai/Mistral-7B-v0.1\"\n",
    "    - \"Qwen/Qwen2-7B\"\n",
    "    - \"tiiuae/falcon-7b\"\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"æ­£åœ¨è¼‰å…¥ {model_name}...\")\n",
    "    print(f\"é è¨ˆ VRAM ä½¿ç”¨: ~4-5 GB (4-bit é‡åŒ–)\")\n",
    "    \n",
    "    # é‡åŒ–é…ç½®\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    # è¼‰å…¥ tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # è¼‰å…¥æ¨¡å‹\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",  # è‡ªå‹•åˆ†é…åˆ°å¯ç”¨è¨­å‚™\n",
    "        trust_remote_code=True,  # æŸäº›æ¨¡å‹éœ€è¦\n",
    "    )\n",
    "    \n",
    "    print(f\"\\næ¨¡å‹è¼‰å…¥å®Œæˆï¼\")\n",
    "    print(get_gpu_memory_info())\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# ç¯„ä¾‹ç”¨æ³•ï¼ˆå–æ¶ˆè¨»è§£åŸ·è¡Œï¼‰ï¼š\n",
    "# model, tokenizer = load_7b_model_4bit(\"mistralai/Mistral-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: LoRA - ä½ç§©é©æ‡‰ (Low-Rank Adaptation)\n",
    "\n",
    "### LoRA çš„ç›´è§€ç†è§£\n",
    "\n",
    "æƒ³åƒä½ æœ‰ä¸€æœ¬ç™¾ç§‘å…¨æ›¸ï¼ˆé è¨“ç·´æ¨¡å‹ï¼‰ï¼Œä½ æƒ³è®“å®ƒå­¸æœƒæ–°çš„å°ˆæ¥­çŸ¥è­˜ã€‚\n",
    "\n",
    "**å‚³çµ±å¾®èª¿**ï¼šé‡æ–°å°åˆ·æ•´æœ¬ç™¾ç§‘å…¨æ›¸ï¼ˆä¿®æ”¹æ‰€æœ‰åƒæ•¸ï¼‰\n",
    "- æˆæœ¬é«˜ã€éœ€è¦å¤§é‡è³‡æº\n",
    "\n",
    "**LoRA å¾®èª¿**ï¼šåœ¨æ›¸é é‚Šç·£åŠ ä¸Šä¾¿åˆ©è²¼ï¼ˆåªæ·»åŠ å°‘é‡åƒæ•¸ï¼‰\n",
    "- åŸæ›¸ä¸è®Šï¼Œä¾¿åˆ©è²¼è¨˜éŒ„ä¿®æ”¹\n",
    "- æˆæœ¬ä½ã€å¯ä»¥æœ‰å¤šå¥—ä¾¿åˆ©è²¼ï¼ˆå¤šå€‹ä»»å‹™ï¼‰\n",
    "\n",
    "```\n",
    "åŸå§‹æ¬Šé‡çŸ©é™£ W (d Ã— k)ï¼š\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     â”‚\n",
    "â”‚    å‡çµï¼Œä¸è¨“ç·´      â”‚  ~7B åƒæ•¸\n",
    "â”‚                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "LoRA åˆ†è§£ï¼šW' = W + A Ã— B\n",
    "\n",
    "çŸ©é™£ A (d Ã— r)    çŸ©é™£ B (r Ã— k)\n",
    "â”Œâ”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   â”‚             â”‚         â”‚\n",
    "â”‚   â”‚      Ã—      â”‚         â”‚  åªè¨“ç·´ A å’Œ B\n",
    "â”‚   â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  r << d, k\n",
    "â”‚   â”‚                          ä¾‹å¦‚ r=8\n",
    "â””â”€â”€â”€â”˜\n",
    "\n",
    "åƒæ•¸é‡æ¯”è¼ƒï¼ˆä»¥ 7B æ¨¡å‹ç‚ºä¾‹ï¼‰ï¼š\n",
    "- å…¨é‡å¾®èª¿: 7,000,000,000 åƒæ•¸\n",
    "- LoRA (r=8): ~4,000,000 åƒæ•¸ (0.06%)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA æ•¸å­¸åŸç†\n",
    "\n",
    "å°æ–¼ä¸€å€‹é è¨“ç·´æ¬Šé‡çŸ©é™£ $W_0 \\in \\mathbb{R}^{d \\times k}$ï¼š\n",
    "\n",
    "$$W = W_0 + \\Delta W = W_0 + BA$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$\n",
    "- $A \\in \\mathbb{R}^{r \\times k}$\n",
    "- $r \\ll \\min(d, k)$ï¼ˆç§©é å°æ–¼ç¶­åº¦ï¼‰\n",
    "\n",
    "å‰å‘å‚³æ’­ï¼š\n",
    "$$h = W_0 x + \\frac{\\alpha}{r} BA x$$\n",
    "\n",
    "$\\alpha$ æ˜¯ç¸®æ”¾å› å­ï¼Œç”¨æ–¼æ§åˆ¶ LoRA çš„å½±éŸ¿ç¨‹åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA å±¤çš„ç°¡åŒ–å¯¦ç¾\n",
    "    \n",
    "    é€™å€‹å¯¦ç¾å±•ç¤ºäº† LoRA çš„æ ¸å¿ƒæ¦‚å¿µï¼š\n",
    "    ç”¨å…©å€‹å°çŸ©é™£çš„ä¹˜ç©ä¾†è¿‘ä¼¼æ¬Šé‡æ›´æ–°\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer: nn.Linear,\n",
    "        r: int = 8,           # LoRA ç§©\n",
    "        alpha: float = 16,    # ç¸®æ”¾å› å­\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.original_layer = original_layer\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "        \n",
    "        in_features = original_layer.in_features\n",
    "        out_features = original_layer.out_features\n",
    "        \n",
    "        # å‡çµåŸå§‹æ¬Šé‡\n",
    "        for param in self.original_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # LoRA çŸ©é™£ A å’Œ B\n",
    "        # A: ç”¨é«˜æ–¯åˆå§‹åŒ–\n",
    "        # B: ç”¨é›¶åˆå§‹åŒ–ï¼ˆç¢ºä¿è¨“ç·´é–‹å§‹æ™‚ LoRA æ²’æœ‰å½±éŸ¿ï¼‰\n",
    "        self.lora_A = nn.Parameter(torch.randn(r, in_features) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, r))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # åŸå§‹è¼¸å‡º\n",
    "        original_output = self.original_layer(x)\n",
    "        \n",
    "        # LoRA è¼¸å‡º: (B @ A) @ x\n",
    "        lora_output = self.dropout(x) @ self.lora_A.T @ self.lora_B.T\n",
    "        \n",
    "        return original_output + lora_output * self.scaling\n",
    "    \n",
    "    def get_trainable_params(self):\n",
    "        \"\"\"è¿”å›å¯è¨“ç·´åƒæ•¸æ•¸é‡\"\"\"\n",
    "        return self.lora_A.numel() + self.lora_B.numel()\n",
    "\n",
    "# ç¤ºç¯„\n",
    "print(\"LoRA å±¤ç¤ºç¯„ï¼š\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# æ¨¡æ“¬ä¸€å€‹ Transformer ä¸­çš„å¤§å‹ç·šæ€§å±¤\n",
    "d_model = 4096\n",
    "original_linear = nn.Linear(d_model, d_model)\n",
    "original_params = sum(p.numel() for p in original_linear.parameters())\n",
    "\n",
    "# åŒ…è£æˆ LoRA å±¤\n",
    "lora_layer = LoRALayer(original_linear, r=8, alpha=16)\n",
    "lora_params = lora_layer.get_trainable_params()\n",
    "\n",
    "print(f\"åŸå§‹ç·šæ€§å±¤åƒæ•¸: {original_params:,}\")\n",
    "print(f\"LoRA å¯è¨“ç·´åƒæ•¸: {lora_params:,}\")\n",
    "print(f\"åƒæ•¸æ¯”ä¾‹: {lora_params/original_params*100:.4f}%\")\n",
    "\n",
    "# æ¸¬è©¦å‰å‘å‚³æ’­\n",
    "x = torch.randn(1, 32, d_model)  # [batch, seq_len, d_model]\n",
    "output = lora_layer(x)\n",
    "print(f\"\\nè¼¸å…¥å½¢ç‹€: {x.shape}\")\n",
    "print(f\"è¼¸å‡ºå½¢ç‹€: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ PEFT åº«é€²è¡Œ LoRA å¾®èª¿\n",
    "\n",
    "PEFT (Parameter-Efficient Fine-Tuning) æ˜¯ Hugging Face çš„å®˜æ–¹åº«ï¼Œ\n",
    "æä¾›äº† LoRA çš„æ¨™æº–åŒ–å¯¦ç¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "def create_lora_config(\n",
    "    r: int = 8,\n",
    "    alpha: int = 16,\n",
    "    dropout: float = 0.05,\n",
    "    target_modules: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    å»ºç«‹ LoRA é…ç½®\n",
    "    \n",
    "    åƒæ•¸èªªæ˜ï¼š\n",
    "    - r: LoRA ç§©ï¼Œè¶Šå¤§è¡¨é”èƒ½åŠ›è¶Šå¼·ä½†åƒæ•¸æ›´å¤š\n",
    "    - alpha: ç¸®æ”¾å› å­ï¼Œé€šå¸¸è¨­ç‚º 2*r æˆ–æ›´é«˜\n",
    "    - dropout: æ­£å‰‡åŒ–\n",
    "    - target_modules: è¦æ‡‰ç”¨ LoRA çš„å±¤åç¨±\n",
    "    \n",
    "    å¸¸è¦‹ target_modulesï¼š\n",
    "    - LLaMA: [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    - GPT-2: [\"c_attn\", \"c_proj\"]\n",
    "    - BERT: [\"query\", \"value\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    if target_modules is None:\n",
    "        # é è¨­é‡å°æ³¨æ„åŠ›å±¤\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    \n",
    "    config = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=dropout,\n",
    "        bias=\"none\",  # é€šå¸¸ä¸è¨“ç·´ bias\n",
    "        task_type=TaskType.CAUSAL_LM,  # å› æœèªè¨€æ¨¡å‹\n",
    "    )\n",
    "    \n",
    "    return config\n",
    "\n",
    "# é¡¯ç¤ºä¸åŒé…ç½®çš„åƒæ•¸é‡ä¼°ç®—\n",
    "print(\"LoRA åƒæ•¸é‡ä¼°ç®— (7B æ¨¡å‹)ï¼š\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# å‡è¨­ 7B æ¨¡å‹æœ‰ 32 å±¤ï¼Œæ¯å±¤ 4 å€‹æ³¨æ„åŠ›çŸ©é™£\n",
    "d_model = 4096\n",
    "n_layers = 32\n",
    "n_matrices_per_layer = 4  # Q, K, V, O\n",
    "\n",
    "for r in [4, 8, 16, 32, 64]:\n",
    "    # æ¯å€‹ LoRA é©é…å™¨: 2 * d_model * r åƒæ•¸\n",
    "    params_per_matrix = 2 * d_model * r\n",
    "    total_params = params_per_matrix * n_matrices_per_layer * n_layers\n",
    "    \n",
    "    print(f\"r={r:2d}: {total_params/1e6:,.1f}M åƒæ•¸ ({total_params/7e9*100:.3f}% of 7B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®Œæ•´çš„ LoRA å¾®èª¿æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "def setup_lora_training(\n",
    "    model_name: str,\n",
    "    lora_r: int = 8,\n",
    "    lora_alpha: int = 16,\n",
    "    target_modules: list = None\n",
    "):\n",
    "    \"\"\"\n",
    "    è¨­ç½® LoRA å¾®èª¿çš„å®Œæ•´æµç¨‹\n",
    "    \n",
    "    é€™å€‹å‡½æ•¸å±•ç¤ºäº†å¦‚ä½•ï¼š\n",
    "    1. è¼‰å…¥é‡åŒ–æ¨¡å‹\n",
    "    2. æº–å‚™æ¨¡å‹é€²è¡Œè¨“ç·´\n",
    "    3. æ‡‰ç”¨ LoRA é…ç½®\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"æ­¥é©Ÿ 1: è¼‰å…¥é‡åŒ–æ¨¡å‹...\")\n",
    "    \n",
    "    # é‡åŒ–é…ç½®\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    # è¼‰å…¥æ¨¡å‹\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(\"æ­¥é©Ÿ 2: æº–å‚™æ¨¡å‹é€²è¡Œ k-bit è¨“ç·´...\")\n",
    "    \n",
    "    # æº–å‚™é‡åŒ–æ¨¡å‹é€²è¡Œè¨“ç·´ï¼ˆè™•ç†æ¢¯åº¦ checkpointing ç­‰ï¼‰\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    print(\"æ­¥é©Ÿ 3: æ‡‰ç”¨ LoRA é…ç½®...\")\n",
    "    \n",
    "    # LoRA é…ç½®\n",
    "    if target_modules is None:\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]  # æœ€å°é…ç½®\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    \n",
    "    # æ‡‰ç”¨ LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # é¡¯ç¤ºå¯è¨“ç·´åƒæ•¸\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    print(f\"\\nç•¶å‰ VRAM ä½¿ç”¨:\")\n",
    "    print(get_gpu_memory_info())\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# ç¯„ä¾‹ç”¨æ³•ï¼ˆå–æ¶ˆè¨»è§£åŸ·è¡Œï¼‰ï¼š\n",
    "# model, tokenizer = setup_lora_training(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: è¨“ç·´å„ªåŒ–æŠ€å·§\n",
    "\n",
    "### æ¢¯åº¦ç´¯ç© (Gradient Accumulation)\n",
    "\n",
    "ç•¶ VRAM ä¸è¶³ä»¥å®¹ç´å¤§ batch size æ™‚ï¼Œå¯ä»¥ç”¨æ¢¯åº¦ç´¯ç©ä¾†æ¨¡æ“¬ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_accumulation_demo():\n",
    "    \"\"\"\n",
    "    æ¢¯åº¦ç´¯ç©çš„åŸç†ç¤ºç¯„\n",
    "    \n",
    "    å‡è¨­ä½ æƒ³è¦ batch_size=32ï¼Œä½† GPU åªèƒ½å®¹ç´ batch_size=4ï¼š\n",
    "    - è¨­å®š accumulation_steps=8\n",
    "    - æ¯æ¬¡è™•ç† 4 å€‹æ¨£æœ¬ï¼Œç´¯ç© 8 æ¬¡æ¢¯åº¦\n",
    "    - ç­‰æ•ˆæ–¼ batch_size=32\n",
    "    \"\"\"\n",
    "    \n",
    "    # å‰µå»ºä¸€å€‹ç°¡å–®æ¨¡å‹\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(100, 50),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50, 10)\n",
    "    ).cuda()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # åƒæ•¸è¨­å®š\n",
    "    effective_batch_size = 32\n",
    "    micro_batch_size = 4\n",
    "    accumulation_steps = effective_batch_size // micro_batch_size\n",
    "    \n",
    "    print(f\"æœ‰æ•ˆ batch size: {effective_batch_size}\")\n",
    "    print(f\"å¾® batch size: {micro_batch_size}\")\n",
    "    print(f\"ç´¯ç©æ­¥æ•¸: {accumulation_steps}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # æ¨¡æ“¬ä¸€å€‹ epoch\n",
    "    total_samples = 128\n",
    "    n_micro_batches = total_samples // micro_batch_size\n",
    "    \n",
    "    optimizer.zero_grad()  # é–‹å§‹æ™‚æ¸…é›¶æ¢¯åº¦\n",
    "    \n",
    "    for step in range(n_micro_batches):\n",
    "        # ç”Ÿæˆå‡è³‡æ–™\n",
    "        x = torch.randn(micro_batch_size, 100).cuda()\n",
    "        y = torch.randint(0, 10, (micro_batch_size,)).cuda()\n",
    "        \n",
    "        # å‰å‘å‚³æ’­\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        \n",
    "        # é‡è¦ï¼šæå¤±é™¤ä»¥ç´¯ç©æ­¥æ•¸ï¼\n",
    "        # é€™ç¢ºä¿äº†ç´¯ç©å¾Œçš„å¹³å‡æ¢¯åº¦èˆ‡å¤§ batch ä¸€è‡´\n",
    "        loss = loss / accumulation_steps\n",
    "        \n",
    "        # åå‘å‚³æ’­ï¼ˆæ¢¯åº¦æœƒç´¯ç©ï¼‰\n",
    "        loss.backward()\n",
    "        \n",
    "        # æ¯ accumulation_steps æ­¥æ›´æ–°ä¸€æ¬¡åƒæ•¸\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()  # æ›´æ–°åƒæ•¸\n",
    "            optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦\n",
    "            \n",
    "            update_step = (step + 1) // accumulation_steps\n",
    "            print(f\"æ›´æ–°æ­¥ {update_step}: ç´¯ç©äº† {accumulation_steps} å€‹å¾® batch\")\n",
    "    \n",
    "    print(\"\\nè¨“ç·´å®Œæˆï¼\")\n",
    "\n",
    "gradient_accumulation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ··åˆç²¾åº¦è¨“ç·´ (Mixed Precision Training)\n",
    "\n",
    "ä½¿ç”¨ FP16/BF16 é€²è¡Œè¨ˆç®—å¯ä»¥ï¼š\n",
    "1. æ¸›å°‘ VRAM ä½¿ç”¨\n",
    "2. åŠ é€Ÿè¨“ç·´ï¼ˆç¾ä»£ GPU å°ä½ç²¾åº¦è¨ˆç®—æœ‰å„ªåŒ–ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def mixed_precision_training_demo():\n",
    "    \"\"\"\n",
    "    æ··åˆç²¾åº¦è¨“ç·´ç¤ºç¯„\n",
    "    \n",
    "    PyTorch çš„ autocast å’Œ GradScaler è®“é€™è®Šå¾—å¾ˆç°¡å–®ï¼š\n",
    "    - autocast: è‡ªå‹•å°‡é©ç•¶çš„æ“ä½œè½‰æ›ç‚º FP16\n",
    "    - GradScaler: é˜²æ­¢æ¢¯åº¦ä¸‹æº¢\n",
    "    \"\"\"\n",
    "    \n",
    "    # å‰µå»ºæ¨¡å‹\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256, 10)\n",
    "    ).cuda()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = GradScaler()  # æ¢¯åº¦ç¸®æ”¾å™¨\n",
    "    \n",
    "    # æ¯”è¼ƒ FP32 vs FP16 çš„è¨˜æ†¶é«”ä½¿ç”¨\n",
    "    print(\"æ··åˆç²¾åº¦è¨“ç·´ç¤ºç¯„\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # FP32 è¨“ç·´\n",
    "    x = torch.randn(64, 1024).cuda()\n",
    "    y = torch.randint(0, 10, (64,)).cuda()\n",
    "    \n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    with VRAMTracker(\"FP32 å‰å‘+åå‘\"):\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # FP16 æ··åˆç²¾åº¦è¨“ç·´\n",
    "    with VRAMTracker(\"FP16 æ··åˆç²¾åº¦å‰å‘+åå‘\"):\n",
    "        with autocast(dtype=torch.float16):  # è‡ªå‹•æ··åˆç²¾åº¦\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "        \n",
    "        # ä½¿ç”¨ scaler é€²è¡Œåå‘å‚³æ’­\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "    print(\"\\næ··åˆç²¾åº¦è¨“ç·´å¯ä»¥ç¯€çœé¡¯è‘—çš„ VRAMï¼\")\n",
    "\n",
    "mixed_precision_training_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¢¯åº¦æª¢æŸ¥é» (Gradient Checkpointing)\n",
    "\n",
    "çŠ§ç‰²ä¸€äº›è¨ˆç®—æ™‚é–“ä¾†æ›å– VRAMï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class LargeModel(nn.Module):\n",
    "    \"\"\"ä¸€å€‹æœ‰å¤šå±¤çš„å¤§æ¨¡å‹\"\"\"\n",
    "    \n",
    "    def __init__(self, n_layers=8, d_model=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, d_model * 4),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(d_model * 4, d_model),\n",
    "            )\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.output = nn.Linear(d_model, 10)\n",
    "        \n",
    "    def forward(self, x, use_checkpointing=False):\n",
    "        for layer in self.layers:\n",
    "            if use_checkpointing:\n",
    "                # ä½¿ç”¨ checkpointingï¼šä¸ä¿å­˜ä¸­é–“æ¿€æ´»å€¼\n",
    "                # åå‘å‚³æ’­æ™‚æœƒé‡æ–°è¨ˆç®—\n",
    "                x = checkpoint(layer, x, use_reentrant=False)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return self.output(x)\n",
    "\n",
    "def checkpointing_comparison():\n",
    "    \"\"\"æ¯”è¼ƒæœ‰ç„¡ checkpointing çš„ VRAM ä½¿ç”¨\"\"\"\n",
    "    \n",
    "    print(\"æ¢¯åº¦æª¢æŸ¥é»æ¯”è¼ƒ\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # ä½¿ç”¨è¼ƒå¤§çš„æ¨¡å‹å’Œæ‰¹æ¬¡ä¾†çœ‹å·®ç•°\n",
    "    model = LargeModel(n_layers=16, d_model=1024).cuda()\n",
    "    x = torch.randn(32, 128, 1024).cuda()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # ç„¡ checkpointing\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    with VRAMTracker(\"ç„¡ Checkpointing\"):\n",
    "        output = model(x, use_checkpointing=False)\n",
    "        loss = criterion(output.mean(dim=1), torch.zeros(32, dtype=torch.long).cuda())\n",
    "        loss.backward()\n",
    "    \n",
    "    model.zero_grad()\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    # æœ‰ checkpointing\n",
    "    with VRAMTracker(\"æœ‰ Checkpointing\"):\n",
    "        output = model(x, use_checkpointing=True)\n",
    "        loss = criterion(output.mean(dim=1), torch.zeros(32, dtype=torch.long).cuda())\n",
    "        loss.backward()\n",
    "    \n",
    "    print(\"\\nCheckpointing ç”¨è¨ˆç®—æ›è¨˜æ†¶é«”ï¼š\")\n",
    "    print(\"- è¨“ç·´é€Ÿåº¦ç•¥æ…¢ï¼ˆç´„ 20-30%ï¼‰\")\n",
    "    print(\"- ä½† VRAM ä½¿ç”¨å¤§å¹…æ¸›å°‘ï¼\")\n",
    "\n",
    "checkpointing_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: å®Œæ•´çš„ LLM å¾®èª¿ç¯„ä¾‹\n",
    "\n",
    "ä»¥ä¸‹æ˜¯ä¸€å€‹å®Œæ•´çš„ã€å¯åœ¨ 16GB VRAM ä¸Šé‹è¡Œçš„ LLM å¾®èª¿ç¯„ä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´çš„ LoRA å¾®èª¿è¨­å®š\n",
    "def get_training_config(output_dir: str = \"./lora_output\"):\n",
    "    \"\"\"\n",
    "    ç‚º 16GB VRAM å„ªåŒ–çš„è¨“ç·´é…ç½®\n",
    "    \n",
    "    é€™äº›åƒæ•¸ç¶“éèª¿æ•´ï¼Œå¯ä»¥åœ¨ RTX 5080 16GB ä¸Šç©©å®šé‹è¡Œ\n",
    "    \"\"\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        \n",
    "        # Batch size è¨­å®š\n",
    "        per_device_train_batch_size=1,      # å° batch é©æ‡‰ VRAM\n",
    "        gradient_accumulation_steps=16,      # ç´¯ç©åˆ°æœ‰æ•ˆ batch=16\n",
    "        \n",
    "        # è¨“ç·´æ­¥æ•¸\n",
    "        num_train_epochs=1,\n",
    "        max_steps=100,  # ç¤ºç¯„ç”¨ï¼Œå¯¦éš›å¯èƒ½éœ€è¦æ›´å¤š\n",
    "        \n",
    "        # å„ªåŒ–å™¨è¨­å®š\n",
    "        learning_rate=2e-4,\n",
    "        optim=\"paged_adamw_8bit\",  # 8-bit Adam ç¯€çœè¨˜æ†¶é«”\n",
    "        weight_decay=0.01,\n",
    "        \n",
    "        # å­¸ç¿’ç‡èª¿åº¦\n",
    "        warmup_steps=10,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        \n",
    "        # ç²¾åº¦è¨­å®š\n",
    "        fp16=False,  # RTX 30/40/50 ç³»åˆ—å»ºè­°ç”¨ bf16\n",
    "        bf16=True,\n",
    "        \n",
    "        # è¨˜æ†¶é«”å„ªåŒ–\n",
    "        gradient_checkpointing=True,\n",
    "        \n",
    "        # æ—¥èªŒè¨­å®š\n",
    "        logging_steps=10,\n",
    "        save_steps=50,\n",
    "        \n",
    "        # å…¶ä»–\n",
    "        report_to=\"none\",  # ä¸ä½¿ç”¨ wandb ç­‰\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    \n",
    "    return training_args\n",
    "\n",
    "# é¡¯ç¤ºé…ç½®\n",
    "config = get_training_config()\n",
    "print(\"è¨“ç·´é…ç½®æ‘˜è¦ï¼š\")\n",
    "print(\"=\"*50)\n",
    "print(f\"æœ‰æ•ˆ batch size: {config.per_device_train_batch_size * config.gradient_accumulation_steps}\")\n",
    "print(f\"ç²¾åº¦: {'bf16' if config.bf16 else 'fp16' if config.fp16 else 'fp32'}\")\n",
    "print(f\"å­¸ç¿’ç‡: {config.learning_rate}\")\n",
    "print(f\"æ¢¯åº¦æª¢æŸ¥é»: {config.gradient_checkpointing}\")\n",
    "print(f\"å„ªåŒ–å™¨: {config.optim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_for_finetuning(tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    æº–å‚™ç”¨æ–¼å¾®èª¿çš„è³‡æ–™é›†\n",
    "    \n",
    "    é€™è£¡ä½¿ç”¨ä¸€å€‹ç°¡å–®çš„æŒ‡ä»¤è³‡æ–™é›†æ ¼å¼\n",
    "    \"\"\"\n",
    "    \n",
    "    # ç¤ºç¯„è³‡æ–™ï¼ˆå¯¦éš›æ‡‰ç”¨ä¸­æœƒä½¿ç”¨çœŸå¯¦è³‡æ–™é›†ï¼‰\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"instruction\": \"What is machine learning?\",\n",
    "            \"response\": \"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Explain what a neural network is.\",\n",
    "            \"response\": \"A neural network is a computational model inspired by the human brain. It consists of interconnected nodes (neurons) organized in layers that process and transform data to learn patterns.\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"What is deep learning?\",\n",
    "            \"response\": \"Deep learning is a type of machine learning that uses neural networks with many layers (deep networks) to learn hierarchical representations of data.\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # æ ¼å¼åŒ–æ–‡æœ¬\n",
    "    def format_prompt(example):\n",
    "        return f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['response']}\"\n",
    "    \n",
    "    # å»ºç«‹è³‡æ–™é›†\n",
    "    texts = [format_prompt(ex) for ex in sample_data]\n",
    "    \n",
    "    # Tokenize\n",
    "    def tokenize_function(examples):\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # å°æ–¼å› æœèªè¨€æ¨¡å‹ï¼Œlabels = input_ids\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "        return tokenized\n",
    "    \n",
    "    dataset = Dataset.from_dict({\"text\": texts})\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"è³‡æ–™é›†å¤§å°: {len(tokenized_dataset)} ç­†\")\n",
    "    print(f\"ç¯„ä¾‹æ–‡æœ¬:\\n{texts[0][:200]}...\")\n",
    "    \n",
    "    return tokenized_dataset\n",
    "\n",
    "print(\"è³‡æ–™é›†æº–å‚™å‡½æ•¸å·²å®šç¾©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_finetuning(\n",
    "    model_name: str = \"microsoft/DialoGPT-medium\",\n",
    "    output_dir: str = \"./lora_finetuned\",\n",
    "    max_steps: int = 50\n",
    "):\n",
    "    \"\"\"\n",
    "    åŸ·è¡Œå®Œæ•´çš„ LoRA å¾®èª¿æµç¨‹\n",
    "    \n",
    "    é€™å€‹å‡½æ•¸æ•´åˆäº†æ‰€æœ‰æ­¥é©Ÿï¼š\n",
    "    1. è¼‰å…¥é‡åŒ–æ¨¡å‹\n",
    "    2. è¨­ç½® LoRA\n",
    "    3. æº–å‚™è³‡æ–™\n",
    "    4. è¨“ç·´\n",
    "    5. ä¿å­˜\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"é–‹å§‹ LoRA å¾®èª¿\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # æ­¥é©Ÿ 1: è¼‰å…¥æ¨¡å‹\n",
    "    print(\"\\n[1/5] è¼‰å…¥é‡åŒ–æ¨¡å‹...\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"æ¨¡å‹è¼‰å…¥å®Œæˆ\")\n",
    "    print(get_gpu_memory_info())\n",
    "    \n",
    "    # æ­¥é©Ÿ 2: æº–å‚™ LoRA\n",
    "    print(\"\\n[2/5] è¨­ç½® LoRA...\")\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"c_attn\"],  # GPT-2 é¢¨æ ¼çš„æ³¨æ„åŠ›å±¤\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # æ­¥é©Ÿ 3: æº–å‚™è³‡æ–™\n",
    "    print(\"\\n[3/5] æº–å‚™è¨“ç·´è³‡æ–™...\")\n",
    "    \n",
    "    train_dataset = prepare_dataset_for_finetuning(tokenizer)\n",
    "    \n",
    "    # æ­¥é©Ÿ 4: è¨“ç·´\n",
    "    print(\"\\n[4/5] é–‹å§‹è¨“ç·´...\")\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,\n",
    "        max_steps=max_steps,\n",
    "        learning_rate=2e-4,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        logging_steps=10,\n",
    "        save_steps=max_steps,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "    )\n",
    "    \n",
    "    # è¨“ç·´\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"\\nè¨“ç·´å®Œæˆï¼\")\n",
    "    print(get_gpu_memory_info())\n",
    "    \n",
    "    # æ­¥é©Ÿ 5: ä¿å­˜\n",
    "    print(\"\\n[5/5] ä¿å­˜ LoRA æ¬Šé‡...\")\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"\\næ¨¡å‹å·²ä¿å­˜è‡³: {output_dir}\")\n",
    "    print(\"åªä¿å­˜äº† LoRA æ¬Šé‡ï¼Œæª”æ¡ˆéå¸¸å°ï¼\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# åŸ·è¡Œå¾®èª¿ï¼ˆå–æ¶ˆè¨»è§£åŸ·è¡Œï¼‰\n",
    "# model, tokenizer = run_complete_finetuning(max_steps=30)\n",
    "print(\"å–æ¶ˆè¨»è§£ä¸Šé¢é€™è¡Œä¾†åŸ·è¡Œå®Œæ•´çš„å¾®èª¿æµç¨‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¼‰å…¥å’Œä½¿ç”¨å¾®èª¿å¾Œçš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "def load_finetuned_model(\n",
    "    base_model_name: str,\n",
    "    lora_path: str\n",
    "):\n",
    "    \"\"\"\n",
    "    è¼‰å…¥å¾®èª¿å¾Œçš„æ¨¡å‹é€²è¡Œæ¨ç†\n",
    "    \n",
    "    LoRA çš„å„ªé»ä¹‹ä¸€ï¼šå¯ä»¥å¿«é€Ÿåˆ‡æ›ä¸åŒçš„ adapter\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"è¼‰å…¥åŸºç¤æ¨¡å‹...\")\n",
    "    \n",
    "    # è¼‰å…¥é‡åŒ–çš„åŸºç¤æ¨¡å‹\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(\"è¼‰å…¥ LoRA æ¬Šé‡...\")\n",
    "    \n",
    "    # è¼‰å…¥ LoRA adapter\n",
    "    model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "    \n",
    "    # å¯é¸ï¼šåˆä½µ LoRA æ¬Šé‡åˆ°åŸºç¤æ¨¡å‹ï¼ˆæ¨ç†æ›´å¿«ï¼Œä½†å¤±å»åˆ‡æ›èƒ½åŠ›ï¼‰\n",
    "    # model = model.merge_and_unload()\n",
    "    \n",
    "    print(\"æ¨¡å‹è¼‰å…¥å®Œæˆï¼\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_text(model, tokenizer, prompt: str, max_new_tokens: int = 100):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "print(\"æ¨ç†å‡½æ•¸å·²å®šç¾©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: VRAM å„ªåŒ–ç­–ç•¥ç¸½çµ\n",
    "\n",
    "### 16GB VRAM å„ªåŒ–æ¸…å–®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vram_optimization_guide = \"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    16GB VRAM å„ªåŒ–ç­–ç•¥                                â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  ğŸ“Š æ¨¡å‹è¼‰å…¥                                                         â•‘\n",
    "â•‘  â”œâ”€ 4-bit é‡åŒ– (QLoRA)           ç¯€çœ ~75% VRAM                     â•‘\n",
    "â•‘  â”œâ”€ 8-bit é‡åŒ–                   ç¯€çœ ~50% VRAM                     â•‘\n",
    "â•‘  â””â”€ BF16/FP16                    ç¯€çœ ~50% VRAM                     â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  ğŸ”§ è¨“ç·´å„ªåŒ–                                                         â•‘\n",
    "â•‘  â”œâ”€ LoRA (r=8)                   åªè¨“ç·´ ~0.1% åƒæ•¸                  â•‘\n",
    "â•‘  â”œâ”€ æ¢¯åº¦ç´¯ç©                     æ¨¡æ“¬å¤§ batch size                  â•‘\n",
    "â•‘  â”œâ”€ æ¢¯åº¦æª¢æŸ¥é»                   ç”¨è¨ˆç®—æ›è¨˜æ†¶é«”                     â•‘\n",
    "â•‘  â””â”€ 8-bit å„ªåŒ–å™¨                 å„ªåŒ–å™¨ç‹€æ…‹è¨˜æ†¶é«”æ¸›åŠ               â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  ğŸ’¡ æœ€ä½³å¯¦è¸                                                         â•‘\n",
    "â•‘  â”œâ”€ batch_size=1 + gradient_accumulation                            â•‘\n",
    "â•‘  â”œâ”€ ä½¿ç”¨ bf16ï¼ˆRTX 30/40/50 ç³»åˆ—ï¼‰                                  â•‘\n",
    "â•‘  â”œâ”€ å®šæœŸæ¸…ç† GPU è¨˜æ†¶é«”                                             â•‘\n",
    "â•‘  â””â”€ ç›£æ§ VRAM ä½¿ç”¨ï¼Œé¿å… OOM                                        â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  ğŸ“ˆ å¯è™•ç†çš„æ¨¡å‹å¤§å° (16GB VRAM)                                    â•‘\n",
    "â•‘  â”œâ”€ æ¨ç†: 7B-13B (4-bit é‡åŒ–)                                       â•‘\n",
    "â•‘  â”œâ”€ LoRA å¾®èª¿: 7B (4-bit + LoRA)                                    â•‘\n",
    "â•‘  â””â”€ å…¨é‡å¾®èª¿: ~1-2B                                                 â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\"\n",
    "\n",
    "print(vram_optimization_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ç·´ç¿’é¡Œ\n",
    "\n",
    "### ç·´ç¿’ 1: VRAM ç›£æ§å·¥å…·æ”¹é€²\n",
    "\n",
    "**ç›®æ¨™**: å»ºç«‹ä¸€å€‹å¯ä»¥æŒçºŒç›£æ§ VRAM ä¸¦åœ¨è¶…éé–¾å€¼æ™‚ç™¼å‡ºè­¦å‘Šçš„å·¥å…·\n",
    "\n",
    "**æç¤º**: ä½¿ç”¨ threading åœ¨èƒŒæ™¯ç›£æ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·´ç¿’ 1: VRAM ç›£æ§å™¨\n",
    "# Hint: ä½¿ç”¨ threading.Thread åœ¨èƒŒæ™¯åŸ·è¡Œç›£æ§\n",
    "\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class VRAMMonitor:\n",
    "    \"\"\"\n",
    "    æŒçºŒç›£æ§ VRAM ä½¿ç”¨çš„å·¥å…·\n",
    "    \n",
    "    åŠŸèƒ½ï¼š\n",
    "    - åœ¨èƒŒæ™¯æŒçºŒç›£æ§\n",
    "    - è¶…éé–¾å€¼æ™‚ç™¼å‡ºè­¦å‘Š\n",
    "    - è¨˜éŒ„å³°å€¼ä½¿ç”¨é‡\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, warning_threshold_gb: float = 14.0, check_interval: float = 1.0):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–ç›£æ§å™¨\n",
    "        \n",
    "        Args:\n",
    "            warning_threshold_gb: è­¦å‘Šé–¾å€¼ï¼ˆGBï¼‰\n",
    "            check_interval: æª¢æŸ¥é–“éš”ï¼ˆç§’ï¼‰\n",
    "        \"\"\"\n",
    "        self.warning_threshold = warning_threshold_gb * 1024  # è½‰æ›ç‚º MB\n",
    "        self.check_interval = check_interval\n",
    "        self.running = False\n",
    "        self.peak_usage = 0\n",
    "        self.thread = None\n",
    "        self.history = []\n",
    "        \n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"èƒŒæ™¯ç›£æ§å¾ªç’°\"\"\"\n",
    "        while self.running:\n",
    "            if torch.cuda.is_available():\n",
    "                # ç²å–ç•¶å‰ä½¿ç”¨é‡\n",
    "                current_usage = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "                reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "                \n",
    "                # æ›´æ–°å³°å€¼\n",
    "                if current_usage > self.peak_usage:\n",
    "                    self.peak_usage = current_usage\n",
    "                \n",
    "                # è¨˜éŒ„æ­·å²\n",
    "                self.history.append({\n",
    "                    'time': time.time(),\n",
    "                    'allocated': current_usage,\n",
    "                    'reserved': reserved\n",
    "                })\n",
    "                \n",
    "                # æª¢æŸ¥è­¦å‘Š\n",
    "                if reserved > self.warning_threshold:\n",
    "                    print(f\"âš ï¸ VRAM è­¦å‘Š! ä½¿ç”¨: {reserved:.0f} MB / é–¾å€¼: {self.warning_threshold:.0f} MB\")\n",
    "            \n",
    "            time.sleep(self.check_interval)\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"é–‹å§‹ç›£æ§\"\"\"\n",
    "        if self.running:\n",
    "            print(\"ç›£æ§å™¨å·²åœ¨é‹è¡Œ\")\n",
    "            return\n",
    "        \n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._monitor_loop, daemon=True)\n",
    "        self.thread.start()\n",
    "        print(f\"VRAM ç›£æ§å·²å•Ÿå‹• (è­¦å‘Šé–¾å€¼: {self.warning_threshold/1024:.1f} GB)\")\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"åœæ­¢ç›£æ§\"\"\"\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=2)\n",
    "        print(f\"ç›£æ§å·²åœæ­¢ã€‚å³°å€¼ä½¿ç”¨: {self.peak_usage:.0f} MB\")\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"ç²å–çµ±è¨ˆè³‡è¨Š\"\"\"\n",
    "        if not self.history:\n",
    "            return \"ç„¡æ­·å²è³‡æ–™\"\n",
    "        \n",
    "        allocated_values = [h['allocated'] for h in self.history]\n",
    "        reserved_values = [h['reserved'] for h in self.history]\n",
    "        \n",
    "        stats = f\"\"\"\n",
    "        VRAM çµ±è¨ˆ:\n",
    "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        åˆ†é…è¨˜æ†¶é«”:\n",
    "          å¹³å‡: {sum(allocated_values)/len(allocated_values):.0f} MB\n",
    "          æœ€å¤§: {max(allocated_values):.0f} MB\n",
    "          æœ€å°: {min(allocated_values):.0f} MB\n",
    "        \n",
    "        ä¿ç•™è¨˜æ†¶é«”:\n",
    "          å¹³å‡: {sum(reserved_values)/len(reserved_values):.0f} MB\n",
    "          æœ€å¤§: {max(reserved_values):.0f} MB\n",
    "          æœ€å°: {min(reserved_values):.0f} MB\n",
    "        \n",
    "        è¨˜éŒ„é»æ•¸: {len(self.history)}\n",
    "        \"\"\"\n",
    "        return stats\n",
    "\n",
    "# æ¸¬è©¦ç›£æ§å™¨\n",
    "monitor = VRAMMonitor(warning_threshold_gb=12.0, check_interval=0.5)\n",
    "monitor.start()\n",
    "\n",
    "# æ¨¡æ“¬ä¸€äº› GPU æ“ä½œ\n",
    "tensors = []\n",
    "for i in range(5):\n",
    "    t = torch.randn(512, 512, 512, device='cuda')\n",
    "    tensors.append(t)\n",
    "    time.sleep(0.3)\n",
    "\n",
    "# æ¸…ç†\n",
    "del tensors\n",
    "clear_gpu_memory()\n",
    "time.sleep(0.5)\n",
    "\n",
    "# åœæ­¢ä¸¦æŸ¥çœ‹çµ±è¨ˆ\n",
    "monitor.stop()\n",
    "print(monitor.get_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 2: è‡ªå®šç¾© LoRA å±¤çš„æ”¹é€²\n",
    "\n",
    "**ç›®æ¨™**: ç‚º LoRA å±¤æ·»åŠ  dropout å’Œå¯é¸çš„æ¬Šé‡åˆä½µåŠŸèƒ½\n",
    "\n",
    "**æç¤º**: åˆä½µæ™‚è¨ˆç®— `W + scaling * B @ A`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·´ç¿’ 2: æ”¹é€²çš„ LoRA å±¤\n",
    "\n",
    "class ImprovedLoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    æ”¹é€²çš„ LoRA å±¤ï¼Œå¢åŠ ï¼š\n",
    "    1. Dropout æ­£å‰‡åŒ–\n",
    "    2. æ¬Šé‡åˆä½µåŠŸèƒ½\n",
    "    3. å‹•æ…‹ç§©èª¿æ•´\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer: nn.Linear,\n",
    "        r: int = 8,\n",
    "        alpha: float = 16,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.original_layer = original_layer\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / r\n",
    "        self.merged = False  # è¿½è¹¤æ˜¯å¦å·²åˆä½µ\n",
    "        \n",
    "        in_features = original_layer.in_features\n",
    "        out_features = original_layer.out_features\n",
    "        \n",
    "        # å‡çµåŸå§‹æ¬Šé‡\n",
    "        for param in self.original_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # LoRA çŸ©é™£\n",
    "        self.lora_A = nn.Parameter(torch.randn(r, in_features) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, r))\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # å¦‚æœå·²åˆä½µï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å±¤\n",
    "        if self.merged:\n",
    "            return self.original_layer(x)\n",
    "        \n",
    "        # åŸå§‹è¼¸å‡º\n",
    "        original_output = self.original_layer(x)\n",
    "        \n",
    "        # LoRA è¼¸å‡ºï¼ˆå¸¶ dropoutï¼‰\n",
    "        dropped_x = self.dropout(x)\n",
    "        lora_output = dropped_x @ self.lora_A.T @ self.lora_B.T\n",
    "        \n",
    "        return original_output + lora_output * self.scaling\n",
    "    \n",
    "    def merge_weights(self):\n",
    "        \"\"\"\n",
    "        å°‡ LoRA æ¬Šé‡åˆä½µåˆ°åŸå§‹å±¤\n",
    "        åˆä½µå¾Œæ¨ç†æ›´å¿«ï¼Œä½†ç„¡æ³•ç¹¼çºŒè¨“ç·´\n",
    "        \"\"\"\n",
    "        if self.merged:\n",
    "            print(\"æ¬Šé‡å·²ç¶“åˆä½µéäº†\")\n",
    "            return\n",
    "        \n",
    "        # è¨ˆç®— LoRA å¢é‡: B @ A\n",
    "        delta_w = self.lora_B @ self.lora_A  # [out_features, in_features]\n",
    "        \n",
    "        # åˆä½µåˆ°åŸå§‹æ¬Šé‡\n",
    "        with torch.no_grad():\n",
    "            self.original_layer.weight.add_(delta_w * self.scaling)\n",
    "        \n",
    "        self.merged = True\n",
    "        print(\"LoRA æ¬Šé‡å·²åˆä½µåˆ°åŸå§‹å±¤\")\n",
    "    \n",
    "    def unmerge_weights(self):\n",
    "        \"\"\"\n",
    "        å–æ¶ˆåˆä½µï¼ˆæ¢å¾©åŸå§‹æ¬Šé‡ï¼‰\n",
    "        \"\"\"\n",
    "        if not self.merged:\n",
    "            print(\"æ¬Šé‡å°šæœªåˆä½µ\")\n",
    "            return\n",
    "        \n",
    "        # è¨ˆç®— LoRA å¢é‡\n",
    "        delta_w = self.lora_B @ self.lora_A\n",
    "        \n",
    "        # å¾åŸå§‹æ¬Šé‡ä¸­æ¸›å»\n",
    "        with torch.no_grad():\n",
    "            self.original_layer.weight.sub_(delta_w * self.scaling)\n",
    "        \n",
    "        self.merged = False\n",
    "        print(\"LoRA æ¬Šé‡å·²å–æ¶ˆåˆä½µ\")\n",
    "    \n",
    "    def get_lora_state_dict(self):\n",
    "        \"\"\"\n",
    "        åªç²å– LoRA æ¬Šé‡ï¼ˆç”¨æ–¼ä¿å­˜ï¼‰\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'lora_A': self.lora_A.data.clone(),\n",
    "            'lora_B': self.lora_B.data.clone(),\n",
    "            'r': self.r,\n",
    "            'alpha': self.alpha,\n",
    "        }\n",
    "\n",
    "# æ¸¬è©¦æ”¹é€²çš„ LoRA å±¤\n",
    "print(\"æ¸¬è©¦æ”¹é€²çš„ LoRA å±¤\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# å‰µå»ºå±¤\n",
    "d = 256\n",
    "original = nn.Linear(d, d)\n",
    "lora = ImprovedLoRALayer(original, r=8, alpha=16)\n",
    "\n",
    "# æ¸¬è©¦è¼¸å…¥\n",
    "x = torch.randn(4, 32, d)\n",
    "\n",
    "# æœªåˆä½µæ™‚çš„è¼¸å‡º\n",
    "output_unmerged = lora(x)\n",
    "print(f\"æœªåˆä½µè¼¸å‡ºå½¢ç‹€: {output_unmerged.shape}\")\n",
    "\n",
    "# åˆä½µæ¬Šé‡\n",
    "lora.merge_weights()\n",
    "output_merged = lora(x)\n",
    "print(f\"åˆä½µå¾Œè¼¸å‡ºå½¢ç‹€: {output_merged.shape}\")\n",
    "\n",
    "# é©—è­‰è¼¸å‡ºä¸€è‡´æ€§ï¼ˆè¨“ç·´æ¨¡å¼ä¸‹å›  dropout æœƒç•¥æœ‰ä¸åŒï¼‰\n",
    "lora.eval()\n",
    "lora.unmerge_weights()\n",
    "output1 = lora(x)\n",
    "lora.merge_weights()\n",
    "output2 = lora(x)\n",
    "\n",
    "diff = (output1 - output2).abs().max().item()\n",
    "print(f\"\\nåˆä½µå‰å¾Œè¼¸å‡ºå·®ç•°ï¼ˆeval æ¨¡å¼ï¼‰: {diff:.10f}\")\n",
    "print(\"å·®ç•°æ‡‰è©²æ¥è¿‘ 0ï¼ˆæµ®é»èª¤å·®ç¯„åœå…§ï¼‰\")\n",
    "\n",
    "# ç²å– LoRA ç‹€æ…‹\n",
    "state = lora.get_lora_state_dict()\n",
    "print(f\"\\nLoRA ç‹€æ…‹: r={state['r']}, alpha={state['alpha']}\")\n",
    "print(f\"lora_A å½¢ç‹€: {state['lora_A'].shape}\")\n",
    "print(f\"lora_B å½¢ç‹€: {state['lora_B'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç·´ç¿’ 3: QLoRA è¨˜æ†¶é«”ä¼°ç®—å™¨\n",
    "\n",
    "**ç›®æ¨™**: å»ºç«‹ä¸€å€‹å·¥å…·ä¾†ä¼°ç®—ä¸åŒæ¨¡å‹å¤§å°å’Œé…ç½®ä¸‹çš„ VRAM éœ€æ±‚\n",
    "\n",
    "**æç¤º**: è€ƒæ…®æ¨¡å‹æ¬Šé‡ã€å„ªåŒ–å™¨ç‹€æ…‹ã€æ¢¯åº¦ã€æ¿€æ´»å€¼ç­‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç·´ç¿’ 3: VRAM éœ€æ±‚ä¼°ç®—å™¨\n",
    "\n",
    "class VRAMEstimator:\n",
    "    \"\"\"\n",
    "    ä¼°ç®—ä¸åŒé…ç½®ä¸‹çš„ VRAM éœ€æ±‚\n",
    "    \n",
    "    è€ƒæ…®å› ç´ ï¼š\n",
    "    1. æ¨¡å‹æ¬Šé‡\n",
    "    2. å„ªåŒ–å™¨ç‹€æ…‹\n",
    "    3. æ¢¯åº¦\n",
    "    4. æ¿€æ´»å€¼ï¼ˆå‰å‘å‚³æ’­ï¼‰\n",
    "    5. è‡¨æ™‚ç·©è¡å€\n",
    "    \"\"\"\n",
    "    \n",
    "    # ä¸åŒç²¾åº¦çš„æ¯åƒæ•¸ä½å…ƒæ•¸\n",
    "    BITS_PER_PARAM = {\n",
    "        'fp32': 32,\n",
    "        'fp16': 16,\n",
    "        'bf16': 16,\n",
    "        'int8': 8,\n",
    "        'int4': 4,\n",
    "    }\n",
    "    \n",
    "    # å„ªåŒ–å™¨ç‹€æ…‹ä¹˜æ•¸ï¼ˆç›¸å°æ–¼æ¨¡å‹å¤§å°ï¼‰\n",
    "    OPTIMIZER_OVERHEAD = {\n",
    "        'sgd': 0,           # ç„¡ç‹€æ…‹\n",
    "        'adam': 2,          # m å’Œ v\n",
    "        'adamw': 2,\n",
    "        'adam_8bit': 1,     # 8-bit é‡åŒ–ç‹€æ…‹\n",
    "        'paged_adamw_8bit': 1,\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def estimate_model_memory(\n",
    "        cls,\n",
    "        num_params_billions: float,\n",
    "        precision: str = 'fp16'\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        ä¼°ç®—æ¨¡å‹æ¬Šé‡çš„è¨˜æ†¶é«”éœ€æ±‚ï¼ˆGBï¼‰\n",
    "        \"\"\"\n",
    "        bits = cls.BITS_PER_PARAM.get(precision, 16)\n",
    "        bytes_per_param = bits / 8\n",
    "        memory_gb = num_params_billions * 1e9 * bytes_per_param / 1024**3\n",
    "        return memory_gb\n",
    "    \n",
    "    @classmethod\n",
    "    def estimate_training_memory(\n",
    "        cls,\n",
    "        num_params_billions: float,\n",
    "        precision: str = 'bf16',\n",
    "        optimizer: str = 'adamw',\n",
    "        batch_size: int = 1,\n",
    "        seq_length: int = 512,\n",
    "        use_gradient_checkpointing: bool = True,\n",
    "        lora_r: int = None,\n",
    "        num_layers: int = 32,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        ä¼°ç®—è¨“ç·´æ™‚çš„ç¸½ VRAM éœ€æ±‚\n",
    "        \n",
    "        Returns:\n",
    "            dict: åŒ…å«å„éƒ¨åˆ†è¨˜æ†¶é«”ä½¿ç”¨çš„è©³ç´°è³‡è¨Š\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. æ¨¡å‹æ¬Šé‡\n",
    "        model_memory = cls.estimate_model_memory(num_params_billions, precision)\n",
    "        \n",
    "        # 2. ç¢ºå®šå¯è¨“ç·´åƒæ•¸æ¯”ä¾‹\n",
    "        if lora_r is not None:\n",
    "            # LoRA: åªæœ‰å¾ˆå°æ¯”ä¾‹çš„åƒæ•¸å¯è¨“ç·´\n",
    "            # å‡è¨­æ¯å±¤æœ‰ 4 å€‹çŸ©é™£éœ€è¦ LoRA\n",
    "            d_model = int((num_params_billions * 1e9 / (12 * num_layers)) ** 0.5)\n",
    "            lora_params = 2 * d_model * lora_r * 4 * num_layers\n",
    "            trainable_ratio = lora_params / (num_params_billions * 1e9)\n",
    "        else:\n",
    "            trainable_ratio = 1.0\n",
    "        \n",
    "        # 3. æ¢¯åº¦ï¼ˆåªé‡å°å¯è¨“ç·´åƒæ•¸ï¼‰\n",
    "        gradient_memory = model_memory * trainable_ratio\n",
    "        \n",
    "        # 4. å„ªåŒ–å™¨ç‹€æ…‹\n",
    "        optimizer_multiplier = cls.OPTIMIZER_OVERHEAD.get(optimizer, 2)\n",
    "        optimizer_memory = gradient_memory * optimizer_multiplier\n",
    "        \n",
    "        # 5. æ¿€æ´»å€¼ï¼ˆèˆ‡ batch size å’Œ seq length ç›¸é—œï¼‰\n",
    "        # ä½¿ç”¨å•Ÿç™¼å¼ä¼°ç®—\n",
    "        if use_gradient_checkpointing:\n",
    "            # Checkpointing å¤§å¹…æ¸›å°‘æ¿€æ´»å€¼è¨˜æ†¶é«”\n",
    "            activation_factor = 0.3\n",
    "        else:\n",
    "            activation_factor = 1.0\n",
    "        \n",
    "        # æ¿€æ´»å€¼ä¼°ç®—ï¼šç´„ç‚ºæ¨¡å‹å¤§å°çš„ batch_size * seq_length / 1000 å€\n",
    "        activation_memory = model_memory * batch_size * seq_length / 2000 * activation_factor\n",
    "        \n",
    "        # 6. è‡¨æ™‚ç·©è¡å€å’Œç¢ç‰‡åŒ–é–‹éŠ·ï¼ˆç´„ 10-20%ï¼‰\n",
    "        overhead = 0.15\n",
    "        \n",
    "        # ç¸½è¨ˆ\n",
    "        subtotal = model_memory + gradient_memory + optimizer_memory + activation_memory\n",
    "        total = subtotal * (1 + overhead)\n",
    "        \n",
    "        return {\n",
    "            'model_weights': model_memory,\n",
    "            'gradients': gradient_memory,\n",
    "            'optimizer_states': optimizer_memory,\n",
    "            'activations': activation_memory,\n",
    "            'overhead': subtotal * overhead,\n",
    "            'total': total,\n",
    "            'trainable_params_ratio': trainable_ratio,\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def print_comparison(\n",
    "        cls,\n",
    "        num_params_billions: float = 7,\n",
    "        batch_size: int = 1,\n",
    "        seq_length: int = 512,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        æ¯”è¼ƒä¸åŒé…ç½®çš„è¨˜æ†¶é«”éœ€æ±‚\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"VRAM éœ€æ±‚ä¼°ç®—: {num_params_billions}B æ¨¡å‹\")\n",
    "        print(f\"Batch size: {batch_size}, Sequence length: {seq_length}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        configs = [\n",
    "            ('å…¨é‡å¾®èª¿ FP16', {'precision': 'fp16', 'optimizer': 'adamw', 'lora_r': None}),\n",
    "            ('å…¨é‡å¾®èª¿ BF16 + Checkpointing', {'precision': 'bf16', 'optimizer': 'adamw', 'lora_r': None, 'use_gradient_checkpointing': True}),\n",
    "            ('8-bit é‡åŒ– + LoRA r=8', {'precision': 'int8', 'optimizer': 'paged_adamw_8bit', 'lora_r': 8}),\n",
    "            ('4-bit é‡åŒ– + LoRA r=8 (QLoRA)', {'precision': 'int4', 'optimizer': 'paged_adamw_8bit', 'lora_r': 8}),\n",
    "            ('4-bit é‡åŒ– + LoRA r=16', {'precision': 'int4', 'optimizer': 'paged_adamw_8bit', 'lora_r': 16}),\n",
    "        ]\n",
    "        \n",
    "        for name, config in configs:\n",
    "            result = cls.estimate_training_memory(\n",
    "                num_params_billions=num_params_billions,\n",
    "                batch_size=batch_size,\n",
    "                seq_length=seq_length,\n",
    "                **config\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  æ¨¡å‹æ¬Šé‡:   {result['model_weights']:6.1f} GB\")\n",
    "            print(f\"  æ¢¯åº¦:       {result['gradients']:6.1f} GB\")\n",
    "            print(f\"  å„ªåŒ–å™¨:     {result['optimizer_states']:6.1f} GB\")\n",
    "            print(f\"  æ¿€æ´»å€¼:     {result['activations']:6.1f} GB\")\n",
    "            print(f\"  é–‹éŠ·:       {result['overhead']:6.1f} GB\")\n",
    "            print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "            print(f\"  ç¸½è¨ˆ:       {result['total']:6.1f} GB\")\n",
    "            \n",
    "            if config.get('lora_r'):\n",
    "                print(f\"  å¯è¨“ç·´æ¯”ä¾‹: {result['trainable_params_ratio']*100:.4f}%\")\n",
    "            \n",
    "            # 16GB VRAM å¯è¡Œæ€§\n",
    "            if result['total'] <= 16:\n",
    "                print(f\"  âœ… é©åˆ 16GB VRAM\")\n",
    "            else:\n",
    "                print(f\"  âŒ éœ€è¦ > 16GB VRAM\")\n",
    "\n",
    "# åŸ·è¡Œæ¯”è¼ƒ\n",
    "VRAMEstimator.print_comparison(num_params_billions=7, batch_size=1, seq_length=512)\n",
    "\n",
    "# ä¹Ÿå¯ä»¥çœ‹çœ‹æ›´å¤§çš„æ¨¡å‹\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ä¸åŒæ¨¡å‹å¤§å°çš„ 4-bit QLoRA è¨˜æ†¶é«”éœ€æ±‚:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for size in [3, 7, 13, 34, 70]:\n",
    "    result = VRAMEstimator.estimate_training_memory(\n",
    "        num_params_billions=size,\n",
    "        precision='int4',\n",
    "        optimizer='paged_adamw_8bit',\n",
    "        lora_r=8,\n",
    "        batch_size=1,\n",
    "        seq_length=512,\n",
    "    )\n",
    "    status = \"âœ…\" if result['total'] <= 16 else \"âŒ\"\n",
    "    print(f\"{size}B æ¨¡å‹: {result['total']:5.1f} GB {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Part 7: é€²éš LLM æŠ€å·§\n\n### 7.1 æ¨ç†å„ªåŒ–ï¼šKV Cache\n\n**å•é¡Œï¼š** è‡ªå›æ­¸ç”Ÿæˆæ™‚ï¼Œæ¯ç”Ÿæˆä¸€å€‹ token éƒ½è¦é‡æ–°è¨ˆç®—æ‰€æœ‰ä¹‹å‰ token çš„ Kã€V\n\n**è§£æ±ºæ–¹æ¡ˆï¼š** ç·©å­˜å·²è¨ˆç®—çš„ Kã€V å€¼ï¼ˆKV Cacheï¼‰",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ç¸½çµ\n",
    "\n",
    "### æœ¬æ¨¡çµ„é‡é»\n",
    "\n",
    "1. **VRAM ç›£æ§**\n",
    "   - ä½¿ç”¨ `torch.cuda.memory_allocated()` å’Œ `memory_reserved()`\n",
    "   - å®šæœŸæ¸…ç†ï¼š`gc.collect()` + `torch.cuda.empty_cache()`\n",
    "\n",
    "2. **æ¨¡å‹é‡åŒ–**\n",
    "   - 4-bit NF4 é‡åŒ–ç¯€çœ ~75% è¨˜æ†¶é«”\n",
    "   - ä½¿ç”¨ bitsandbytes çš„ `BitsAndBytesConfig`\n",
    "   - é›™é‡é‡åŒ–é€²ä¸€æ­¥ç¯€çœç©ºé–“\n",
    "\n",
    "3. **LoRA å¾®èª¿**\n",
    "   - åªè¨“ç·´æ¥µå°‘é‡åƒæ•¸ï¼ˆ~0.1%ï¼‰\n",
    "   - ä½¿ç”¨ PEFT åº«çš„ `LoraConfig` å’Œ `get_peft_model`\n",
    "   - ç§© r è¶Šå¤§è¡¨é”èƒ½åŠ›è¶Šå¼·\n",
    "\n",
    "4. **è¨“ç·´å„ªåŒ–**\n",
    "   - æ¢¯åº¦ç´¯ç©æ¨¡æ“¬å¤§ batch size\n",
    "   - æ··åˆç²¾åº¦ï¼ˆBF16ï¼‰åŠ é€Ÿè¨“ç·´\n",
    "   - æ¢¯åº¦æª¢æŸ¥é»ç”¨è¨ˆç®—æ›è¨˜æ†¶é«”\n",
    "   - 8-bit å„ªåŒ–å™¨æ¸›å°‘ç‹€æ…‹è¨˜æ†¶é«”\n",
    "\n",
    "### 16GB VRAM çš„å¯¦ç”¨é…ç½®\n",
    "\n",
    "```python\n",
    "# æ¨è–¦çš„ 7B æ¨¡å‹ QLoRA é…ç½®\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    ")\n",
    "```\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "åœ¨ä¸‹ä¸€å€‹æ¨¡çµ„ä¸­ï¼Œæˆ‘å€‘å°‡å­¸ç¿’ **Autoencoder å’Œ VAE**ï¼Œæ¢ç´¢å¦‚ä½•å­¸ç¿’è³‡æ–™çš„æ½›åœ¨è¡¨ç¤ºä¸¦ç”Ÿæˆæ–°æ¨£æœ¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "source": "# KV Cache æ¦‚å¿µèªªæ˜\n\nprint(\"\"\"\nKV Cache åŸç†ï¼š\n\nè‡ªå›æ­¸ç”Ÿæˆï¼šæ¯æ¬¡åªç”Ÿæˆä¸€å€‹ token\n  Step 1: \"Hello\" â†’ è¨ˆç®— Kâ‚, Vâ‚ â†’ ç”Ÿæˆ \"world\"\n  Step 2: \"Hello world\" â†’ é‡æ–°è¨ˆç®— Kâ‚, Vâ‚, Kâ‚‚, Vâ‚‚ â†’ ç”Ÿæˆ \"!\"  â† æµªè²»ï¼\n  Step 3: \"Hello world!\" â†’ é‡æ–°è¨ˆç®— Kâ‚, Vâ‚, Kâ‚‚, Vâ‚‚, Kâ‚ƒ, Vâ‚ƒ â†’ ...\n\nä½¿ç”¨ KV Cacheï¼š\n  Step 1: \"Hello\" â†’ è¨ˆç®—ä¸¦ç·©å­˜ Kâ‚, Vâ‚ â†’ ç”Ÿæˆ \"world\"\n  Step 2: åªè¨ˆç®— Kâ‚‚, Vâ‚‚ï¼ˆKâ‚, Vâ‚ å¾ç·©å­˜è®€å–ï¼‰â†’ ç”Ÿæˆ \"!\"\n  Step 3: åªè¨ˆç®— Kâ‚ƒ, Vâ‚ƒ â†’ ...\n\næ•ˆæœï¼š\n  - ç”Ÿæˆé€Ÿåº¦æå‡æ•¸å€\n  - ä»£åƒ¹ï¼šéœ€è¦é¡å¤–çš„è¨˜æ†¶é«”å­˜å„² KV Cache\n\nKV Cache è¨˜æ†¶é«”ä¼°ç®—ï¼š\n  memory = 2 Ã— batch_size Ã— num_layers Ã— seq_len Ã— num_heads Ã— head_dim Ã— bytes_per_element\n  \n  7B æ¨¡å‹ (å‡è¨­ 32 å±¤, 32 heads, head_dim=128):\n    FP16: 2 Ã— 1 Ã— 32 Ã— 2048 Ã— 32 Ã— 128 Ã— 2 bytes = ~1 GB per 2K tokens\n\"\"\")\n\n# ä½¿ç”¨ Transformers æ™‚ï¼ŒKV Cache æ˜¯è‡ªå‹•è™•ç†çš„\n# åªéœ€è¦åœ¨ç”Ÿæˆæ™‚è¨­ç½® use_cache=Trueï¼ˆé è¨­å°±æ˜¯ Trueï¼‰\n\ndef generate_with_kv_cache_demo():\n    \"\"\"å±•ç¤º KV Cache çš„ä½¿ç”¨\"\"\"\n    print(\"\\nä½¿ç”¨ HuggingFace æ™‚ï¼ŒKV Cache è‡ªå‹•å•Ÿç”¨ï¼š\")\n    print(\"-\" * 50)\n    \n    # ç”Ÿæˆé…ç½®ç¤ºä¾‹\n    generation_config = {\n        \"max_new_tokens\": 100,\n        \"do_sample\": True,\n        \"temperature\": 0.7,\n        \"use_cache\": True,  # é è¨­å°±æ˜¯ Trueï¼Œå•Ÿç”¨ KV Cache\n        # \"past_key_values\": None,  # å¯ä»¥å‚³å…¥ä¹‹å‰çš„ KV Cache ç¹¼çºŒç”Ÿæˆ\n    }\n    \n    print(\"generation_config:\")\n    for k, v in generation_config.items():\n        print(f\"  {k}: {v}\")\n    \n    print(\"\\né€²éšç”¨æ³•ï¼šæ‰‹å‹•ç®¡ç† KV Cache\")\n    print(\"  outputs = model(input_ids, past_key_values=past_kv, use_cache=True)\")\n    print(\"  past_kv = outputs.past_key_values  # ä¿å­˜ç”¨æ–¼ä¸‹ä¸€æ¬¡ç”Ÿæˆ\")\n\ngenerate_with_kv_cache_demo()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.2 Prompt Engineering åŸºç¤\n\næœ‰æ•ˆçš„ Prompt è¨­è¨ˆå° LLM è¼¸å‡ºå“è³ªè‡³é—œé‡è¦ï¼š",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Prompt Engineering æŠ€å·§\n\nprint(\"\"\"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘                      Prompt Engineering æŠ€å·§                         â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘                                                                      â•‘\nâ•‘  1. æŒ‡ä»¤æ ¼å¼ (Instruction Format)                                    â•‘\nâ•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â•‘\nâ•‘  ### Instruction:                                                    â•‘\nâ•‘  {ä»»å‹™æè¿°}                                                          â•‘\nâ•‘                                                                      â•‘\nâ•‘  ### Input:                                                          â•‘\nâ•‘  {è¼¸å…¥è³‡æ–™}                                                          â•‘\nâ•‘                                                                      â•‘\nâ•‘  ### Response:                                                       â•‘\nâ•‘                                                                      â•‘\nâ•‘  2. Few-shot Learning                                                â•‘\nâ•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â•‘\nâ•‘  æä¾›å¹¾å€‹ç¯„ä¾‹è®“æ¨¡å‹å­¸ç¿’æ ¼å¼ï¼š                                         â•‘\nâ•‘  Q: What is 2+2?                                                     â•‘\nâ•‘  A: 4                                                                â•‘\nâ•‘                                                                      â•‘\nâ•‘  Q: What is 3+5?                                                     â•‘\nâ•‘  A: 8                                                                â•‘\nâ•‘                                                                      â•‘\nâ•‘  Q: What is 7+9?                                                     â•‘\nâ•‘  A:                                                                  â•‘\nâ•‘                                                                      â•‘\nâ•‘  3. Chain-of-Thought (CoT)                                           â•‘\nâ•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â•‘\nâ•‘  è®“æ¨¡å‹å±•ç¤ºæ¨ç†éç¨‹ï¼š                                                 â•‘\nâ•‘  \"Let's think step by step...\"                                       â•‘\nâ•‘                                                                      â•‘\nâ•‘  4. System Prompt (å°è©±æ¨¡å‹)                                         â•‘\nâ•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â•‘\nâ•‘  è¨­å®šæ¨¡å‹çš„è§’è‰²å’Œè¡Œç‚ºï¼š                                               â•‘\nâ•‘  \"You are a helpful assistant that...\"                               â•‘\nâ•‘                                                                      â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\"\"\")\n\n# ä¸åŒä»»å‹™çš„ Prompt æ¨¡æ¿\nprompt_templates = {\n    \"classification\": '''Classify the following text into one of the categories: {categories}\n\nText: {text}\n\nCategory:''',\n\n    \"summarization\": '''Summarize the following text in {num_sentences} sentences:\n\nText: {text}\n\nSummary:''',\n\n    \"translation\": '''Translate the following text from {source_lang} to {target_lang}:\n\n{text}\n\nTranslation:''',\n\n    \"code_generation\": '''Write a Python function that {task_description}.\n\nRequirements:\n{requirements}\n\n```python''',\n\n    \"qa\": '''Answer the question based on the context below.\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer:''',\n}\n\nprint(\"\\nPrompt æ¨¡æ¿ç¯„ä¾‹ï¼š\")\nprint(\"=\" * 50)\nfor name, template in prompt_templates.items():\n    print(f\"\\n[{name}]\")\n    print(template[:100] + \"...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Module 6 å®Œæ•´ç¸½çµ\n\n### ğŸ¯ æ ¸å¿ƒæŠ€è¡“\n\n| æŠ€è¡“ | ç”¨é€” | VRAM ç¯€çœ |\n|------|------|-----------|\n| **4-bit é‡åŒ–** | è¼‰å…¥å¤§æ¨¡å‹ | ~75% |\n| **8-bit é‡åŒ–** | è¼‰å…¥å¤§æ¨¡å‹ | ~50% |\n| **LoRA** | é«˜æ•ˆå¾®èª¿ | åªè¨“ç·´ ~0.1% åƒæ•¸ |\n| **QLoRA** | é‡åŒ– + LoRA | çµåˆå…©è€…å„ªå‹¢ |\n| **æ¢¯åº¦ç´¯ç©** | æ¨¡æ“¬å¤§ batch | ä¸ç›´æ¥çœ VRAM |\n| **æ¢¯åº¦æª¢æŸ¥é»** | è¨“ç·´å¤§æ¨¡å‹ | ~50% æ¿€æ´»å€¼ |\n\n### ğŸ”‘ é—œéµé…ç½®\n\n```python\n# 4-bit é‡åŒ–ï¼ˆæ¨è–¦é…ç½®ï¼‰\nBitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# LoRA é…ç½®\nLoraConfig(\n    r=8,                          # ç§©ï¼ˆ8-64ï¼‰\n    lora_alpha=16,                # ç¸®æ”¾ï¼ˆé€šå¸¸ 2*rï¼‰\n    target_modules=[\"q_proj\", \"v_proj\"],  # ç›®æ¨™å±¤\n    lora_dropout=0.05,\n)\n\n# è¨“ç·´é…ç½®ï¼ˆ16GB VRAMï¼‰\nTrainingArguments(\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=16,\n    bf16=True,\n    gradient_checkpointing=True,\n    optim=\"paged_adamw_8bit\",\n)\n```\n\n### ğŸ“Š 16GB VRAM èƒ½åŠ›ç¯„åœ\n\n| ä»»å‹™ | å¯ç”¨æ¨¡å‹å¤§å° |\n|------|-------------|\n| **æ¨ç†** | 7B-13B (4-bit) |\n| **QLoRA å¾®èª¿** | 7B (4-bit + LoRA) |\n| **å…¨é‡å¾®èª¿** | ~1-2B |\n\n### ğŸ’¡ å¯¦å‹™å»ºè­°\n\n1. **æ¨¡å‹é¸æ“‡**ï¼š\n   - 7B æ¨¡å‹æ˜¯ 16GB VRAM çš„ç”œèœœé»\n   - å„ªå…ˆè€ƒæ…® Mistralã€LLaMA 2ã€Qwen2\n\n2. **å¾®èª¿ç­–ç•¥**ï¼š\n   - å…ˆå˜—è©¦ r=8ï¼Œæ•ˆæœä¸å¥½å†å¢åŠ \n   - target_modules è¶Šå¤šæ•ˆæœè¶Šå¥½ä½† VRAM è¶Šé«˜\n   - å¾ q_proj, v_proj é–‹å§‹\n\n3. **æ¨ç†å„ªåŒ–**ï¼š\n   - ä½¿ç”¨ KV Cacheï¼ˆé è¨­å•Ÿç”¨ï¼‰\n   - é•·æ–‡æœ¬è€ƒæ…® vLLM æˆ– TGI\n   - æ‰¹æ¬¡æ¨ç†å¯æé«˜ååé‡\n\n4. **å¸¸è¦‹å•é¡Œ**ï¼š\n   - OOM â†’ æ¸›å°‘ batch_size æˆ–å•Ÿç”¨ gradient_checkpointing\n   - è¨“ç·´ä¸ç©©å®š â†’ é™ä½å­¸ç¿’ç‡æˆ–å¢åŠ  warmup\n   - æ•ˆæœä¸å¥½ â†’ å¢åŠ  r æˆ– target_modules\n\n### ğŸš€ ä¸‹ä¸€æ­¥\n\n- Module 7ï¼šAutoencoder èˆ‡ VAEï¼ˆå­¸ç¿’æ½›åœ¨è¡¨ç¤ºï¼‰\n- é€²ä¸€æ­¥å­¸ç¿’ï¼švLLMã€TGI ç­‰æ¨ç†æ¡†æ¶\n\n---\n\n**æ­å–œå®Œæˆ LLM å¯¦å‹™æ¨¡çµ„ï¼** ğŸ‰\n\nä½ ç¾åœ¨å·²ç¶“æŒæ¡äº†åœ¨æ¶ˆè²»ç´š GPU ä¸Šä½¿ç”¨å’Œå¾®èª¿å¤§å‹èªè¨€æ¨¡å‹çš„æ ¸å¿ƒæŠ€è¡“ã€‚QLoRA æ˜¯ç›®å‰æœ€å¯¦ç”¨çš„å¾®èª¿æ–¹æ³•ï¼Œè®“ 7B æ¨¡å‹çš„å¾®èª¿è®Šå¾—è§¸æ‰‹å¯åŠã€‚",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}