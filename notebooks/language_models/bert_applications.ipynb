{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT æ‡‰ç”¨å¯¦æˆ°\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "- ç†è§£ BERT çš„é è¨“ç·´-å¾®èª¿ç¯„å¼\n",
    "- å­¸ç¿’ä½¿ç”¨ HuggingFace Transformers\n",
    "- å¯¦ä½œå•ç­”ç³»çµ±ï¼ˆæå®æ¯… HW7ï¼‰\n",
    "- æŒæ¡ BERT å¾®èª¿æŠ€å·§\n",
    "\n",
    "## å°æ‡‰èª²ç¨‹\n",
    "- [æå®æ¯… ML 2021 - BERT and its family](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php)\n",
    "- [HW7: Question Answering (Extractive)](https://github.com/ga642381/ML2021-Spring/tree/main/HW07)\n",
    "\n",
    "## BERT æ¶æ§‹æ¦‚è¿°\n",
    "\n",
    "```\n",
    "BERT é è¨“ç·´ä»»å‹™\n",
    "â”œâ”€â”€ Masked Language Model (MLM)\n",
    "â”‚   éš¨æ©Ÿé®è“‹ 15% çš„ tokenï¼Œé æ¸¬è¢«é®è“‹çš„è©\n",
    "â”‚\n",
    "â””â”€â”€ Next Sentence Prediction (NSP)\n",
    "    é æ¸¬å…©å€‹å¥å­æ˜¯å¦é€£çºŒ\n",
    "\n",
    "BERT ä¸‹æ¸¸ä»»å‹™\n",
    "â”œâ”€â”€ æ–‡æœ¬åˆ†é¡: [CLS] token çš„è¼¸å‡º\n",
    "â”œâ”€â”€ åºåˆ—æ¨™è¨»: æ¯å€‹ token çš„è¼¸å‡º\n",
    "â”œâ”€â”€ å•ç­”: é æ¸¬ç­”æ¡ˆçš„èµ·å§‹å’ŒçµæŸä½ç½®\n",
    "â””â”€â”€ æ–‡æœ¬ç”Ÿæˆ: éœ€è¦é¡å¤–çš„è§£ç¢¼å™¨\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "# å˜—è©¦å°å…¥ transformers\n",
    "try:\n",
    "    from transformers import (\n",
    "        BertTokenizer, BertModel, BertForQuestionAnswering,\n",
    "        AutoTokenizer, AutoModelForQuestionAnswering,\n",
    "        AdamW, get_linear_schedule_with_warmup\n",
    "    )\n",
    "    HAS_TRANSFORMERS = True\n",
    "    print(\"HuggingFace Transformers å·²è¼‰å…¥\")\n",
    "except ImportError:\n",
    "    HAS_TRANSFORMERS = False\n",
    "    print(\"è«‹å®‰è£: pip install transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: BERT Tokenizer åŸºç¤\n",
    "\n",
    "BERT ä½¿ç”¨ WordPiece tokenizationï¼Œå°‡è©åˆ†å‰²æˆå­è©å–®å…ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== BERT Tokenizer ==========\n",
    "\n",
    "if HAS_TRANSFORMERS:\n",
    "    # è¼‰å…¥é è¨“ç·´ tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "    \n",
    "    # ç¯„ä¾‹æ–‡æœ¬\n",
    "    text = \"æ·±åº¦å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„æ ¸å¿ƒæŠ€è¡“\"\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"åŸæ–‡: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    \n",
    "    # å®Œæ•´ç·¨ç¢¼\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        padding='max_length',\n",
    "        max_length=20,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nInput IDs: {encoded['input_ids']}\")\n",
    "    print(f\"Attention Mask: {encoded['attention_mask']}\")\n",
    "    print(f\"Token Type IDs: {encoded['token_type_ids']}\")\n",
    "    \n",
    "    # è§£ç¢¼\n",
    "    decoded = tokenizer.decode(encoded['input_ids'][0], skip_special_tokens=True)\n",
    "    print(f\"\\nè§£ç¢¼: {decoded}\")\n",
    "else:\n",
    "    print(\"éœ€è¦å®‰è£ transformers æ‰èƒ½åŸ·è¡Œæ­¤ç¯„ä¾‹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== å•ç­”ä»»å‹™çš„ Tokenization ==========\n",
    "\n",
    "if HAS_TRANSFORMERS:\n",
    "    question = \"æ·±åº¦å­¸ç¿’æ˜¯ä»€éº¼ï¼Ÿ\"\n",
    "    context = \"æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹åˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†å­¸ç¿’è³‡æ–™çš„è¡¨ç¤ºã€‚\"\n",
    "    \n",
    "    # å•ç­”ä»»å‹™éœ€è¦åŒæ™‚ç·¨ç¢¼å•é¡Œå’Œä¸Šä¸‹æ–‡\n",
    "    encoded = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        padding='max_length',\n",
    "        max_length=64,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    print(\"å•ç­”ä»»å‹™ç·¨ç¢¼:\")\n",
    "    print(f\"  Input IDs shape: {encoded['input_ids'].shape}\")\n",
    "    \n",
    "    # å¯è¦–åŒ– token çµæ§‹\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
    "    token_type_ids = encoded['token_type_ids'][0].tolist()\n",
    "    \n",
    "    print(\"\\nçµæ§‹:\")\n",
    "    print(\"[CLS] + å•é¡Œ tokens + [SEP] + ä¸Šä¸‹æ–‡ tokens + [SEP] + [PAD]...\")\n",
    "    print(f\"\\nå‰ 20 å€‹ tokens: {tokens[:20]}\")\n",
    "    print(f\"Token Type IDs: {token_type_ids[:20]}\")\n",
    "    print(\"(0 = å•é¡Œ, 1 = ä¸Šä¸‹æ–‡)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: BERT æ¨¡å‹çµæ§‹\n",
    "\n",
    "BERT æ˜¯ä¸€å€‹å¤šå±¤é›™å‘ Transformer Encoderã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== BERT æ¨¡å‹ ==========\n",
    "\n",
    "if HAS_TRANSFORMERS:\n",
    "    # è¼‰å…¥é è¨“ç·´ BERT\n",
    "    model = BertModel.from_pretrained('bert-base-chinese')\n",
    "    \n",
    "    # æ¨¡å‹çµæ§‹\n",
    "    print(\"BERT æ¨¡å‹çµæ§‹:\")\n",
    "    print(f\"  å±¤æ•¸: {model.config.num_hidden_layers}\")\n",
    "    print(f\"  éš±è—ç¶­åº¦: {model.config.hidden_size}\")\n",
    "    print(f\"  æ³¨æ„åŠ›é ­æ•¸: {model.config.num_attention_heads}\")\n",
    "    print(f\"  è©å½™é‡: {model.config.vocab_size}\")\n",
    "    \n",
    "    # åƒæ•¸é‡\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\nç¸½åƒæ•¸é‡: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== BERT è¼¸å‡º ==========\n",
    "\n",
    "if HAS_TRANSFORMERS:\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded)\n",
    "    \n",
    "    print(\"BERT è¼¸å‡º:\")\n",
    "    print(f\"  last_hidden_state: {outputs.last_hidden_state.shape}\")\n",
    "    print(f\"  pooler_output: {outputs.pooler_output.shape}\")\n",
    "    \n",
    "    # last_hidden_state: æ¯å€‹ token çš„è¡¨ç¤º\n",
    "    # pooler_output: [CLS] token ç¶“é pooler å±¤çš„è¼¸å‡º\n",
    "    \n",
    "    print(\"\\nèªªæ˜:\")\n",
    "    print(\"  last_hidden_state: (batch, seq_len, hidden_size) - æ‰€æœ‰ token çš„è¡¨ç¤º\")\n",
    "    print(\"  pooler_output: (batch, hidden_size) - [CLS] çš„æ± åŒ–è¡¨ç¤º\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Extractive QAï¼ˆæå®æ¯… HW7ï¼‰\n",
    "\n",
    "### ä»»å‹™èªªæ˜\n",
    "\n",
    "çµ¦å®šå•é¡Œå’Œæ–‡ç« ï¼Œå¾æ–‡ç« ä¸­æ‰¾å‡ºç­”æ¡ˆçš„ä½ç½®ï¼ˆèµ·å§‹å’ŒçµæŸä½ç½®ï¼‰ã€‚\n",
    "\n",
    "**è¼¸å…¥**ï¼šå•é¡Œ + æ–‡ç« \n",
    "\n",
    "**è¼¸å‡º**ï¼šç­”æ¡ˆåœ¨æ–‡ç« ä¸­çš„èµ·å§‹å’ŒçµæŸä½ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ¨¡æ“¬ QA è³‡æ–™é›† ==========\n",
    "\n",
    "def generate_qa_data(n_samples=100):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæ¨¡æ“¬çš„ QA è³‡æ–™\n",
    "    \n",
    "    å¯¦éš› HW7 ä½¿ç”¨é¡ä¼¼ SQuAD æ ¼å¼çš„ä¸­æ–‡å•ç­”è³‡æ–™\n",
    "    \"\"\"\n",
    "    samples = [\n",
    "        {\n",
    "            \"question\": \"æ·±åº¦å­¸ç¿’æ˜¯ä»€éº¼ï¼Ÿ\",\n",
    "            \"context\": \"æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹åˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†å­¸ç¿’è³‡æ–™çš„è¡¨ç¤ºã€‚å®ƒåœ¨å½±åƒè¾¨è­˜ã€è‡ªç„¶èªè¨€è™•ç†ç­‰é ˜åŸŸå–å¾—äº†çªç ´æ€§é€²å±•ã€‚\",\n",
    "            \"answer\": \"æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹åˆ†æ”¯\",\n",
    "            \"answer_start\": 4\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Transformer æ˜¯åœ¨å“ªä¸€å¹´æå‡ºçš„ï¼Ÿ\",\n",
    "            \"context\": \"Transformer æ¶æ§‹æ˜¯ç”± Google åœ¨ 2017 å¹´çš„è«–æ–‡ã€ŠAttention is All You Needã€‹ä¸­æå‡ºçš„ï¼Œå®ƒå®Œå…¨åŸºæ–¼è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ã€‚\",\n",
    "            \"answer\": \"2017 å¹´\",\n",
    "            \"answer_start\": 18\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"BERT çš„å…¨ç¨±æ˜¯ä»€éº¼ï¼Ÿ\",\n",
    "            \"context\": \"BERT çš„å…¨ç¨±æ˜¯ Bidirectional Encoder Representations from Transformersï¼Œå®ƒæ˜¯ä¸€ç¨®é è¨“ç·´èªè¨€æ¨¡å‹ã€‚\",\n",
    "            \"answer\": \"Bidirectional Encoder Representations from Transformers\",\n",
    "            \"answer_start\": 12\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # è¤‡è£½ä»¥é”åˆ°æ‰€éœ€æ•¸é‡\n",
    "    while len(samples) < n_samples:\n",
    "        samples = samples + samples\n",
    "    \n",
    "    return samples[:n_samples]\n",
    "\n",
    "qa_data = generate_qa_data(100)\n",
    "print(f\"è³‡æ–™é›†å¤§å°: {len(qa_data)}\")\n",
    "print(f\"\\nç¯„ä¾‹:\")\n",
    "print(f\"  å•é¡Œ: {qa_data[0]['question']}\")\n",
    "print(f\"  ä¸Šä¸‹æ–‡: {qa_data[0]['context'][:50]}...\")\n",
    "print(f\"  ç­”æ¡ˆ: {qa_data[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== QA Dataset ==========\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Extractive QA Dataset\n",
    "    \n",
    "    è™•ç†å•ç­”è³‡æ–™ï¼Œç”¢ç”Ÿæ¨¡å‹è¼¸å…¥å’Œæ¨™ç±¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_len=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            item['question'],\n",
    "            item['context'],\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # æ‰¾åˆ°ç­”æ¡ˆçš„ token ä½ç½®\n",
    "        # é€™æ˜¯ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›éœ€è¦æ›´è¤‡é›œçš„è™•ç†\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "        token_type_ids = encoding['token_type_ids'].squeeze()\n",
    "        \n",
    "        # ç°¡åŒ–ï¼šå‡è¨­ç­”æ¡ˆåœ¨ç‰¹å®šä½ç½®\n",
    "        start_position = min(item['answer_start'], self.max_len - 1)\n",
    "        end_position = min(start_position + len(item['answer']), self.max_len - 1)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'start_position': torch.tensor(start_position),\n",
    "            'end_position': torch.tensor(end_position),\n",
    "        }\n",
    "\n",
    "if HAS_TRANSFORMERS:\n",
    "    dataset = QADataset(qa_data, tokenizer, max_len=128)\n",
    "    sample = dataset[0]\n",
    "    print(\"Dataset æ¨£æœ¬:\")\n",
    "    for k, v in sample.items():\n",
    "        print(f\"  {k}: {v.shape if hasattr(v, 'shape') else v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== QA æ¨¡å‹ ==========\n",
    "\n",
    "class BertQA(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT for Extractive QA\n",
    "    \n",
    "    åœ¨ BERT è¼¸å‡ºä¸ŠåŠ å…©å€‹ç·šæ€§å±¤ï¼š\n",
    "    - ä¸€å€‹é æ¸¬èµ·å§‹ä½ç½®\n",
    "    - ä¸€å€‹é æ¸¬çµæŸä½ç½®\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-chinese'):\n",
    "        super().__init__()\n",
    "        \n",
    "        if HAS_TRANSFORMERS:\n",
    "            self.bert = BertModel.from_pretrained(model_name)\n",
    "            hidden_size = self.bert.config.hidden_size\n",
    "        else:\n",
    "            # æ¨¡æ“¬\n",
    "            hidden_size = 768\n",
    "            self.bert = None\n",
    "        \n",
    "        # èµ·å§‹å’ŒçµæŸä½ç½®çš„åˆ†é¡é ­\n",
    "        self.qa_outputs = nn.Linear(hidden_size, 2)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        if self.bert is not None:\n",
    "            outputs = self.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            sequence_output = outputs.last_hidden_state\n",
    "        else:\n",
    "            # æ¨¡æ“¬è¼¸å‡º\n",
    "            batch_size, seq_len = input_ids.shape\n",
    "            sequence_output = torch.randn(batch_size, seq_len, 768)\n",
    "        \n",
    "        # é æ¸¬èµ·å§‹å’ŒçµæŸä½ç½®\n",
    "        logits = self.qa_outputs(sequence_output)  # (batch, seq_len, 2)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        \n",
    "        start_logits = start_logits.squeeze(-1)  # (batch, seq_len)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        \n",
    "        return start_logits, end_logits\n",
    "\n",
    "if HAS_TRANSFORMERS:\n",
    "    qa_model = BertQA()\n",
    "    print(f\"QA æ¨¡å‹åƒæ•¸é‡: {sum(p.numel() for p in qa_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è¨“ç·´ QA æ¨¡å‹ ==========\n",
    "\n",
    "def train_qa_model(model, train_loader, epochs=3, lr=2e-5):\n",
    "    \"\"\"\n",
    "    è¨“ç·´ QA æ¨¡å‹\n",
    "    \n",
    "    HW7 è¨“ç·´æŠ€å·§:\n",
    "    - è¼ƒå°çš„å­¸ç¿’ç‡ (2e-5)\n",
    "    - ç·šæ€§å­¸ç¿’ç‡ warmup\n",
    "    - Gradient clipping\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # å„ªåŒ–å™¨ï¼ˆå€åˆ† BERT å’Œåˆ†é¡é ­çš„å­¸ç¿’ç‡ï¼‰\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() \n",
    "                      if not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': 0.01\n",
    "        },\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() \n",
    "                      if any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': 0.0\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    \n",
    "    # å­¸ç¿’ç‡èª¿åº¦\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    warmup_steps = total_steps // 10\n",
    "    \n",
    "    # æå¤±å‡½æ•¸\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    history = {'loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            start_positions = batch['start_position'].to(device)\n",
    "            end_positions = batch['end_position'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            start_logits, end_logits = model(\n",
    "                input_ids, attention_mask, token_type_ids\n",
    "            )\n",
    "            \n",
    "            # è¨ˆç®—æå¤±\n",
    "            start_loss = criterion(start_logits, start_positions)\n",
    "            end_loss = criterion(end_logits, end_positions)\n",
    "            loss = (start_loss + end_loss) / 2\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        history['loss'].append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "if HAS_TRANSFORMERS:\n",
    "    train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "    # ç”±æ–¼ BERT è¼ƒå¤§ï¼Œé€™è£¡åªç¤ºç¯„è¨“ç·´æµç¨‹\n",
    "    print(\"è¨“ç·´æµç¨‹å·²å®šç¾©ã€‚å¯¦éš›è¨“ç·´éœ€è¦è¶³å¤ çš„ GPU è¨˜æ†¶é«”ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== QA æ¨ç† ==========\n",
    "\n",
    "def answer_question(model, tokenizer, question, context, device='cpu'):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹å›ç­”å•é¡Œ\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    encoding = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    token_type_ids = encoding['token_type_ids'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_logits, end_logits = model(\n",
    "            input_ids, attention_mask, token_type_ids\n",
    "        )\n",
    "    \n",
    "    # æ‰¾åˆ°æœ€å¯èƒ½çš„èµ·å§‹å’ŒçµæŸä½ç½®\n",
    "    start_idx = start_logits.argmax().item()\n",
    "    end_idx = end_logits.argmax().item()\n",
    "    \n",
    "    # ç¢ºä¿ end >= start\n",
    "    if end_idx < start_idx:\n",
    "        end_idx = start_idx\n",
    "    \n",
    "    # è§£ç¢¼ç­”æ¡ˆ\n",
    "    answer_ids = input_ids[0, start_idx:end_idx+1]\n",
    "    answer = tokenizer.decode(answer_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return answer, start_idx, end_idx\n",
    "\n",
    "if HAS_TRANSFORMERS:\n",
    "    print(\"QA æ¨ç†å‡½æ•¸å·²å®šç¾©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: BERT å¾®èª¿æŠ€å·§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== å‡çµéƒ¨åˆ†å±¤ ==========\n",
    "\n",
    "def freeze_bert_layers(model, num_layers_to_freeze=6):\n",
    "    \"\"\"\n",
    "    å‡çµ BERT çš„å‰å¹¾å±¤\n",
    "    \n",
    "    æŠ€å·§: å‡çµåº•å±¤ï¼Œåªè¨“ç·´é«˜å±¤å’Œåˆ†é¡é ­\n",
    "    å¯ä»¥åŠ é€Ÿè¨“ç·´ä¸¦æ¸›å°‘éæ“¬åˆ\n",
    "    \"\"\"\n",
    "    # å‡çµ embeddings\n",
    "    for param in model.bert.embeddings.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # å‡çµå‰ N å±¤\n",
    "    for i in range(num_layers_to_freeze):\n",
    "        for param in model.bert.encoder.layer[i].parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # è¨ˆç®—å¯è¨“ç·´åƒæ•¸\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"å¯è¨“ç·´åƒæ•¸: {trainable:,} / {total:,} ({100*trainable/total:.1f}%)\")\n",
    "\n",
    "if HAS_TRANSFORMERS:\n",
    "    print(\"å‡çµå‰ 6 å±¤:\")\n",
    "    test_model = BertQA()\n",
    "    freeze_bert_layers(test_model, num_layers_to_freeze=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== åˆ†å±¤å­¸ç¿’ç‡ ==========\n",
    "\n",
    "def get_layerwise_lr_params(model, base_lr=2e-5, lr_decay=0.95):\n",
    "    \"\"\"\n",
    "    ç‚ºä¸åŒå±¤è¨­ç½®ä¸åŒçš„å­¸ç¿’ç‡\n",
    "    \n",
    "    åº•å±¤å­¸ç¿’ç‡è¼ƒå°ï¼Œé«˜å±¤å­¸ç¿’ç‡è¼ƒå¤§\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    \n",
    "    if not HAS_TRANSFORMERS or model.bert is None:\n",
    "        return [{'params': model.parameters(), 'lr': base_lr}]\n",
    "    \n",
    "    # BERT å±¤ï¼ˆå¾é«˜åˆ°ä½ï¼‰\n",
    "    num_layers = model.bert.config.num_hidden_layers\n",
    "    \n",
    "    for i in range(num_layers - 1, -1, -1):\n",
    "        layer_lr = base_lr * (lr_decay ** (num_layers - 1 - i))\n",
    "        params.append({\n",
    "            'params': model.bert.encoder.layer[i].parameters(),\n",
    "            'lr': layer_lr\n",
    "        })\n",
    "    \n",
    "    # Embeddings ä½¿ç”¨æœ€å°å­¸ç¿’ç‡\n",
    "    params.append({\n",
    "        'params': model.bert.embeddings.parameters(),\n",
    "        'lr': base_lr * (lr_decay ** num_layers)\n",
    "    })\n",
    "    \n",
    "    # åˆ†é¡é ­ä½¿ç”¨æœ€å¤§å­¸ç¿’ç‡\n",
    "    params.append({\n",
    "        'params': model.qa_outputs.parameters(),\n",
    "        'lr': base_lr\n",
    "    })\n",
    "    \n",
    "    return params\n",
    "\n",
    "print(\"åˆ†å±¤å­¸ç¿’ç‡é…ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: HW7 å¯¦ç”¨æŠ€å·§\n",
    "\n",
    "### è™•ç†é•·æ–‡æœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Sliding Window è™•ç†é•·æ–‡æœ¬ ==========\n",
    "\n",
    "def process_long_context(question, context, tokenizer, max_len=512, doc_stride=128):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æ»‘å‹•çª—å£è™•ç†è¶…é•·ä¸Šä¸‹æ–‡\n",
    "    \n",
    "    ç•¶æ–‡ç« å¤ªé•·æ™‚ï¼Œå°‡å…¶åˆ†å‰²æˆå¤šå€‹é‡ç–Šçš„çª—å£\n",
    "    \"\"\"\n",
    "    # å…ˆ tokenize å•é¡Œ\n",
    "    question_tokens = tokenizer.tokenize(question)\n",
    "    \n",
    "    # å¯ç”¨æ–¼ä¸Šä¸‹æ–‡çš„æœ€å¤§é•·åº¦\n",
    "    # éœ€è¦æ‰£é™¤ [CLS], [SEP], [SEP] å’Œå•é¡Œé•·åº¦\n",
    "    max_context_len = max_len - len(question_tokens) - 3\n",
    "    \n",
    "    # Tokenize ä¸Šä¸‹æ–‡\n",
    "    context_tokens = tokenizer.tokenize(context)\n",
    "    \n",
    "    if len(context_tokens) <= max_context_len:\n",
    "        # ä¸éœ€è¦åˆ†å‰²\n",
    "        return [{'question': question, 'context': context, 'offset': 0}]\n",
    "    \n",
    "    # æ»‘å‹•çª—å£åˆ†å‰²\n",
    "    windows = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(context_tokens):\n",
    "        end = min(start + max_context_len, len(context_tokens))\n",
    "        \n",
    "        window_tokens = context_tokens[start:end]\n",
    "        window_context = tokenizer.convert_tokens_to_string(window_tokens)\n",
    "        \n",
    "        windows.append({\n",
    "            'question': question,\n",
    "            'context': window_context,\n",
    "            'offset': start\n",
    "        })\n",
    "        \n",
    "        if end >= len(context_tokens):\n",
    "            break\n",
    "        \n",
    "        start += doc_stride\n",
    "    \n",
    "    return windows\n",
    "\n",
    "if HAS_TRANSFORMERS:\n",
    "    # æ¸¬è©¦\n",
    "    long_context = \"æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹åˆ†æ”¯ã€‚\" * 100\n",
    "    windows = process_long_context(\n",
    "        \"ä»€éº¼æ˜¯æ·±åº¦å­¸ç¿’ï¼Ÿ\",\n",
    "        long_context,\n",
    "        tokenizer,\n",
    "        max_len=128,\n",
    "        doc_stride=64\n",
    "    )\n",
    "    print(f\"åŸå§‹ä¸Šä¸‹æ–‡é•·åº¦: {len(tokenizer.tokenize(long_context))} tokens\")\n",
    "    print(f\"åˆ†å‰²æˆ {len(windows)} å€‹çª—å£\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ç­”æ¡ˆå¾Œè™•ç† ==========\n",
    "\n",
    "def post_process_predictions(start_logits, end_logits, n_best=5, max_answer_len=30):\n",
    "    \"\"\"\n",
    "    å¾Œè™•ç†é æ¸¬çµæœ\n",
    "    \n",
    "    1. æ‰¾åˆ° n_best å€‹æœ€å¯èƒ½çš„ (start, end) çµ„åˆ\n",
    "    2. éæ¿¾ä¸åˆæ³•çš„çµ„åˆï¼ˆend < start, é•·åº¦è¶…é™ï¼‰\n",
    "    3. è¿”å›å¾—åˆ†æœ€é«˜çš„ç­”æ¡ˆ\n",
    "    \"\"\"\n",
    "    # ç²å– top k çš„èµ·å§‹å’ŒçµæŸä½ç½®\n",
    "    start_indices = torch.topk(start_logits, n_best).indices.tolist()\n",
    "    end_indices = torch.topk(end_logits, n_best).indices.tolist()\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for start_idx in start_indices:\n",
    "        for end_idx in end_indices:\n",
    "            # éæ¿¾æ¢ä»¶\n",
    "            if end_idx < start_idx:\n",
    "                continue\n",
    "            if end_idx - start_idx + 1 > max_answer_len:\n",
    "                continue\n",
    "            \n",
    "            score = start_logits[start_idx] + end_logits[end_idx]\n",
    "            candidates.append({\n",
    "                'start': start_idx,\n",
    "                'end': end_idx,\n",
    "                'score': score.item()\n",
    "            })\n",
    "    \n",
    "    # æŒ‰åˆ†æ•¸æ’åº\n",
    "    candidates = sorted(candidates, key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    return candidates[:n_best]\n",
    "\n",
    "# æ¸¬è©¦\n",
    "test_start = torch.randn(128)\n",
    "test_end = torch.randn(128)\n",
    "candidates = post_process_predictions(test_start, test_end)\n",
    "print(f\"Top {len(candidates)} å€™é¸ç­”æ¡ˆ:\")\n",
    "for i, c in enumerate(candidates[:3]):\n",
    "    print(f\"  {i+1}. start={c['start']}, end={c['end']}, score={c['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¯ ç¸½çµ\n",
    "\n",
    "### BERT å¾®èª¿è¦é»\n",
    "\n",
    "| æŠ€å·§ | èªªæ˜ |\n",
    "|------|------|\n",
    "| **å°å­¸ç¿’ç‡** | 2e-5 ~ 5e-5ï¼Œé¿å…ç ´å£é è¨“ç·´æ¬Šé‡ |\n",
    "| **Warmup** | å‰ 10% æ­¥æ•¸ç·šæ€§å¢åŠ å­¸ç¿’ç‡ |\n",
    "| **Gradient Clipping** | é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ |\n",
    "| **åˆ†å±¤å­¸ç¿’ç‡** | åº•å±¤å°ï¼Œé«˜å±¤å¤§ |\n",
    "| **å‡çµå±¤** | å‡çµåº•å±¤åŠ é€Ÿè¨“ç·´ |\n",
    "\n",
    "### Extractive QA æµç¨‹\n",
    "\n",
    "```\n",
    "è¨“ç·´:\n",
    "1. Tokenize å•é¡Œ + ä¸Šä¸‹æ–‡\n",
    "2. æ¨™è¨˜ç­”æ¡ˆèµ·å§‹å’ŒçµæŸä½ç½®\n",
    "3. ç”¨ CrossEntropy è¨“ç·´å…©å€‹åˆ†é¡é ­\n",
    "\n",
    "æ¨ç†:\n",
    "1. é•·æ–‡æœ¬ä½¿ç”¨æ»‘å‹•çª—å£\n",
    "2. é æ¸¬æ¯å€‹çª—å£çš„ (start, end)\n",
    "3. å¾Œè™•ç†é¸æ“‡æœ€ä½³ç­”æ¡ˆ\n",
    "```\n",
    "\n",
    "### æå®æ¯… HW7 æŠ€å·§\n",
    "\n",
    "```\n",
    "Strong Baseline:\n",
    "1. ä½¿ç”¨ chinese-bert-wwm ç­‰ä¸­æ–‡é è¨“ç·´æ¨¡å‹\n",
    "2. æ­£ç¢ºè™•ç†æ»‘å‹•çª—å£\n",
    "3. Learning rate warmup\n",
    "4. Gradient accumulationï¼ˆå¦‚æœ batch size å¤ªå°ï¼‰\n",
    "5. ç­”æ¡ˆå¾Œè™•ç†ï¼ˆn-best, é•·åº¦é™åˆ¶ï¼‰\n",
    "```\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å‰å¾€ `robustness_security/explainable_ai.ipynb` å­¸ç¿’å¯è§£é‡‹ AIï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## ğŸ‹ï¸ ç·´ç¿’\n\n### ç·´ç¿’ 1: è¦–è¦ºåŒ– BERT çš„æ³¨æ„åŠ›æ¬Šé‡\n\nè§€å¯Ÿ BERT å¦‚ä½•é—œæ³¨æ–‡æœ¬çš„ä¸åŒéƒ¨åˆ†æ˜¯ç†è§£æ¨¡å‹çš„é‡è¦æ–¹å¼ã€‚å¯¦ä½œä¸€å€‹å‡½æ•¸ä¾†æå–å’Œè¦–è¦ºåŒ–æ³¨æ„åŠ›ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== ç·´ç¿’ 1: è¦–è¦ºåŒ– BERT æ³¨æ„åŠ›æ¬Šé‡ ==========\n\ndef visualize_bert_attention(model, tokenizer, text, layer_idx=-1, head_idx=0):\n    \"\"\"\n    è¦–è¦ºåŒ– BERT ç‰¹å®šå±¤å’Œé ­çš„æ³¨æ„åŠ›æ¬Šé‡\n    \n    Args:\n        model: BERT æ¨¡å‹\n        tokenizer: BERT tokenizer\n        text: è¼¸å…¥æ–‡æœ¬\n        layer_idx: è¦è¦–è¦ºåŒ–çš„å±¤ç´¢å¼•ï¼ˆ-1 è¡¨ç¤ºæœ€å¾Œä¸€å±¤ï¼‰\n        head_idx: è¦è¦–è¦ºåŒ–çš„æ³¨æ„åŠ›é ­ç´¢å¼•\n    \"\"\"\n    model.eval()\n    \n    # Tokenize\n    encoding = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n    \n    # ç²å–æ³¨æ„åŠ›æ¬Šé‡\n    with torch.no_grad():\n        outputs = model(**encoding, output_attentions=True)\n    \n    # attentions: tuple of (batch, num_heads, seq_len, seq_len) for each layer\n    attention = outputs.attentions[layer_idx][0, head_idx].cpu().numpy()\n    \n    # ç¹ªè£½ç†±åŠ›åœ–\n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    im = ax.imshow(attention, cmap='Blues')\n    \n    # è¨­ç½®æ¨™ç±¤\n    ax.set_xticks(range(len(tokens)))\n    ax.set_yticks(range(len(tokens)))\n    ax.set_xticklabels(tokens, rotation=90, fontsize=8)\n    ax.set_yticklabels(tokens, fontsize=8)\n    \n    ax.set_xlabel('Key Tokens')\n    ax.set_ylabel('Query Tokens')\n    ax.set_title(f'BERT Attention (Layer {layer_idx}, Head {head_idx})')\n    \n    plt.colorbar(im, ax=ax, label='Attention Weight')\n    plt.tight_layout()\n    plt.show()\n    \n    return attention\n\nif HAS_TRANSFORMERS:\n    # æ¸¬è©¦è¦–è¦ºåŒ–\n    print(\"ç·´ç¿’ 1: è¦–è¦ºåŒ– BERT æ³¨æ„åŠ›æ¬Šé‡\")\n    print(\"=\" * 50)\n    \n    # è¼‰å…¥æ¨¡å‹ä¸¦å•Ÿç”¨æ³¨æ„åŠ›è¼¸å‡º\n    bert_model = BertModel.from_pretrained('bert-base-chinese', output_attentions=True)\n    \n    # æ¸¬è©¦æ–‡æœ¬\n    test_text = \"æ·±åº¦å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„æ ¸å¿ƒæŠ€è¡“\"\n    \n    # è¦–è¦ºåŒ–æœ€å¾Œä¸€å±¤çš„ç¬¬ä¸€å€‹æ³¨æ„åŠ›é ­\n    attention = visualize_bert_attention(bert_model, tokenizer, test_text, layer_idx=-1, head_idx=0)\n    \n    # æ¯”è¼ƒä¸åŒé ­çš„æ³¨æ„åŠ›æ¨¡å¼\n    print(\"\\nä¸åŒæ³¨æ„åŠ›é ­å¯èƒ½å­¸ç¿’ä¸åŒçš„æ¨¡å¼ï¼š\")\n    print(\"- æœ‰äº›é ­é—œæ³¨å‰ä¸€å€‹/å¾Œä¸€å€‹è©ï¼ˆä½ç½®æ¨¡å¼ï¼‰\")\n    print(\"- æœ‰äº›é ­é—œæ³¨ç‰¹å®šè©æ€§ï¼ˆèªæ³•æ¨¡å¼ï¼‰\")\n    print(\"- æœ‰äº›é ­é—œæ³¨ [SEP] æˆ– [CLS]ï¼ˆå…¨å±€æ¨¡å¼ï¼‰\")\nelse:\n    print(\"éœ€è¦å®‰è£ transformers æ‰èƒ½åŸ·è¡Œæ­¤ç·´ç¿’\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### ç·´ç¿’ 2: æ¯”è¼ƒä¸åŒ doc_stride å°é•·æ–‡æœ¬ QA çš„å½±éŸ¿\n\nåœ¨è™•ç†é•·æ–‡æœ¬æ™‚ï¼Œæ»‘å‹•çª—å£çš„æ­¥é•·ï¼ˆdoc_strideï¼‰æœƒå½±éŸ¿è¦†è“‹ç‡å’Œè¨ˆç®—æˆæœ¬ã€‚å¯¦é©—ä¸åŒ stride çš„æ•ˆæœã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== ç·´ç¿’ 2: æ¯”è¼ƒä¸åŒ doc_stride çš„å½±éŸ¿ ==========\n\ndef experiment_doc_stride(question, context, tokenizer, max_len=128, strides=[32, 64, 96, 128]):\n    \"\"\"\n    æ¯”è¼ƒä¸åŒ doc_stride å°é•·æ–‡æœ¬è™•ç†çš„å½±éŸ¿\n    \n    Args:\n        question: å•é¡Œ\n        context: é•·æ–‡æœ¬ä¸Šä¸‹æ–‡\n        tokenizer: BERT tokenizer\n        max_len: æœ€å¤§åºåˆ—é•·åº¦\n        strides: è¦æ¸¬è©¦çš„ stride å€¼åˆ—è¡¨\n    \"\"\"\n    results = []\n    \n    context_tokens = tokenizer.tokenize(context)\n    question_tokens = tokenizer.tokenize(question)\n    max_context_len = max_len - len(question_tokens) - 3\n    \n    print(f\"å•é¡Œ: {question}\")\n    print(f\"ä¸Šä¸‹æ–‡é•·åº¦: {len(context_tokens)} tokens\")\n    print(f\"æ¯å€‹çª—å£æœ€å¤§ä¸Šä¸‹æ–‡é•·åº¦: {max_context_len} tokens\")\n    print(\"=\" * 60)\n    \n    for stride in strides:\n        windows = process_long_context(question, context, tokenizer, max_len=max_len, doc_stride=stride)\n        \n        # è¨ˆç®—è¦†è“‹çµ±è¨ˆ\n        num_windows = len(windows)\n        \n        # è¨ˆç®—é‡ç–Šç‡\n        if num_windows > 1:\n            overlap = max_context_len - stride\n            overlap_ratio = overlap / max_context_len\n        else:\n            overlap_ratio = 0.0\n        \n        # è¨ˆç®—è¨ˆç®—æˆæœ¬ï¼ˆç›¸å°æ–¼å–®çª—å£ï¼‰\n        compute_cost = num_windows\n        \n        results.append({\n            'stride': stride,\n            'num_windows': num_windows,\n            'overlap_ratio': overlap_ratio,\n            'compute_cost': compute_cost\n        })\n        \n        print(f\"doc_stride={stride:3d}: {num_windows:3d} çª—å£, \"\n              f\"é‡ç–Šç‡={overlap_ratio*100:5.1f}%, è¨ˆç®—æˆæœ¬={compute_cost}x\")\n    \n    # è¦–è¦ºåŒ–\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    \n    stride_vals = [r['stride'] for r in results]\n    \n    axes[0].bar(range(len(stride_vals)), [r['num_windows'] for r in results], color='steelblue')\n    axes[0].set_xticks(range(len(stride_vals)))\n    axes[0].set_xticklabels(stride_vals)\n    axes[0].set_xlabel('doc_stride')\n    axes[0].set_ylabel('çª—å£æ•¸é‡')\n    axes[0].set_title('çª—å£æ•¸é‡ vs doc_stride')\n    \n    axes[1].bar(range(len(stride_vals)), [r['overlap_ratio']*100 for r in results], color='coral')\n    axes[1].set_xticks(range(len(stride_vals)))\n    axes[1].set_xticklabels(stride_vals)\n    axes[1].set_xlabel('doc_stride')\n    axes[1].set_ylabel('é‡ç–Šç‡ (%)')\n    axes[1].set_title('é‡ç–Šç‡ vs doc_stride')\n    \n    # æ¬Šè¡¡åˆ†æï¼šæ›´å¤šé‡ç–Š = æ›´é«˜è¦†è“‹ä½†æ›´å¤šè¨ˆç®—\n    axes[2].scatter([r['num_windows'] for r in results], \n                   [r['overlap_ratio']*100 for r in results], \n                   c=stride_vals, cmap='viridis', s=100)\n    for r in results:\n        axes[2].annotate(f\"s={r['stride']}\", \n                        (r['num_windows'], r['overlap_ratio']*100 + 2))\n    axes[2].set_xlabel('è¨ˆç®—æˆæœ¬ï¼ˆçª—å£æ•¸é‡ï¼‰')\n    axes[2].set_ylabel('é‡ç–Šç‡ (%)')\n    axes[2].set_title('æˆæœ¬ vs è¦†è“‹ç‡æ¬Šè¡¡')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nçµè«–ï¼š\")\n    print(\"- è¼ƒå°çš„ stride â†’ æ›´å¤šé‡ç–Š â†’ æ›´å¥½çš„è¦†è“‹ä½†æ›´æ…¢\")\n    print(\"- è¼ƒå¤§çš„ stride â†’ æ›´å°‘é‡ç–Š â†’ å¯èƒ½æ¼æ‰è·¨çª—å£ç­”æ¡ˆ\")\n    print(\"- å»ºè­°ï¼šstride = max_len // 2 æ˜¯å¸¸è¦‹çš„å¹³è¡¡é¸æ“‡\")\n    \n    return results\n\nif HAS_TRANSFORMERS:\n    print(\"ç·´ç¿’ 2: æ¯”è¼ƒä¸åŒ doc_stride çš„å½±éŸ¿\")\n    print(\"=\" * 50)\n    \n    # ç”Ÿæˆé•·æ–‡æœ¬\n    long_context = \"\"\"\n    æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹åˆ†æ”¯ï¼Œå®ƒä½¿ç”¨å¤šå±¤ç¥ç¶“ç¶²è·¯ä¾†å­¸ç¿’è³‡æ–™çš„è¡¨ç¤ºã€‚\n    é€™ç¨®æ–¹æ³•åœ¨å½±åƒè¾¨è­˜ã€è‡ªç„¶èªè¨€è™•ç†ã€èªéŸ³è­˜åˆ¥ç­‰é ˜åŸŸå–å¾—äº†çªç ´æ€§é€²å±•ã€‚\n    BERT æ˜¯ç”± Google åœ¨ 2018 å¹´æå‡ºçš„é è¨“ç·´èªè¨€æ¨¡å‹ï¼Œå…¨ç¨±æ˜¯ Bidirectional Encoder Representations from Transformersã€‚\n    å®ƒçš„æ ¸å¿ƒå‰µæ–°æ˜¯ä½¿ç”¨é›™å‘ Transformer ç·¨ç¢¼å™¨å’Œå…©å€‹é è¨“ç·´ä»»å‹™ï¼šé®è“‹èªè¨€æ¨¡å‹å’Œä¸‹ä¸€å¥é æ¸¬ã€‚\n    GPT ç³»åˆ—å‰‡æ˜¯ç”± OpenAI é–‹ç™¼çš„è‡ªå›æ­¸èªè¨€æ¨¡å‹ï¼Œä½¿ç”¨å–®å‘ Transformer è§£ç¢¼å™¨ã€‚\n    Transformer æ¶æ§‹æœ€åˆåœ¨ 2017 å¹´çš„è«–æ–‡ã€ŠAttention is All You Needã€‹ä¸­æå‡ºã€‚\n    è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶æ˜¯å…¶æ ¸å¿ƒçµ„ä»¶ï¼Œå¯ä»¥æ•æ‰åºåˆ—ä¸­ä»»æ„ä½ç½®ä¹‹é–“çš„ä¾è³´é—œä¿‚ã€‚\n    \"\"\" * 5  # é‡è¤‡ä»¥å‰µå»ºé•·æ–‡æœ¬\n    \n    results = experiment_doc_stride(\n        question=\"BERT æ˜¯ä»€éº¼æ™‚å€™æå‡ºçš„ï¼Ÿ\",\n        context=long_context,\n        tokenizer=tokenizer,\n        max_len=128,\n        strides=[32, 64, 96, 128]\n    )\nelse:\n    print(\"éœ€è¦å®‰è£ transformers æ‰èƒ½åŸ·è¡Œæ­¤ç·´ç¿’\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}