{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成式 AI 入門 (Introduction to Generative AI)\n",
    "\n",
    "**對應課程**: 李宏毅 2025 Fall GenAI-ML HW1 & HW3\n",
    "\n",
    "本 notebook 涵蓋生成式 AI 的基礎概念，從傳統機器學習到現代大型語言模型的演進。\n",
    "\n",
    "## 學習目標\n",
    "1. 理解生成式 AI 與判別式 AI 的區別\n",
    "2. 掌握 LLM 的核心架構（GPT vs BERT）\n",
    "3. 學會 Tokenization 的原理與實作\n",
    "4. 理解推理參數對生成的影響\n",
    "5. 實踐基礎 Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: 生成式 AI 概覽\n",
    "\n",
    "### 1.1 判別式 vs 生成式模型\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     AI 模型分類                              │\n",
    "├────────────────────────┬────────────────────────────────────┤\n",
    "│     判別式模型          │          生成式模型                │\n",
    "│   (Discriminative)     │        (Generative)               │\n",
    "├────────────────────────┼────────────────────────────────────┤\n",
    "│ 學習 P(Y|X)            │ 學習 P(X) 或 P(X|Y)               │\n",
    "│ 輸入 → 類別/數值        │ 條件 → 新資料                     │\n",
    "├────────────────────────┼────────────────────────────────────┤\n",
    "│ 範例:                   │ 範例:                             │\n",
    "│ - 圖像分類              │ - 文本生成 (GPT)                  │\n",
    "│ - 情感分析              │ - 圖像生成 (Stable Diffusion)     │\n",
    "│ - 物件偵測              │ - 語音合成 (TTS)                  │\n",
    "│ - 回歸預測              │ - 程式碼生成 (Codex)              │\n",
    "└────────────────────────┴────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境設置\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# 檢查設備\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用設備: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 生成式 AI 的發展歷程\n",
    "\n",
    "```\n",
    "時間軸: 生成式 AI 演進\n",
    "═══════════════════════════════════════════════════════════════\n",
    "\n",
    "2014    2017      2018       2020      2022      2023     2024+\n",
    "  │       │         │          │         │         │        │\n",
    "  ▼       ▼         ▼          ▼         ▼         ▼        ▼\n",
    "┌───┐  ┌─────┐  ┌──────┐  ┌──────┐  ┌──────┐  ┌──────┐  ┌─────┐\n",
    "│GAN│  │Trans│  │ GPT  │  │GPT-3 │  │ChatGP│  │GPT-4 │  │Multi│\n",
    "│   │  │former│  │ BERT │  │DALL-E│  │  T   │  │Claude│  │modal│\n",
    "└───┘  └─────┘  └──────┘  └──────┘  └──────┘  └──────┘  └─────┘\n",
    "  │       │         │          │         │         │        │\n",
    "  │       │         │          │         │         │        │\n",
    "圖像    序列到    預訓練    大規模    對話     多模態   AI Agent\n",
    "生成    序列      語言模型   生成     助手     理解     自主性\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: 語言模型基礎\n",
    "\n",
    "### 2.1 什麼是語言模型？\n",
    "\n",
    "語言模型的核心任務：給定前文，預測下一個 token 的機率分布。\n",
    "\n",
    "$$P(w_t | w_1, w_2, ..., w_{t-1})$$\n",
    "\n",
    "生成整個序列的機率：\n",
    "$$P(w_1, w_2, ..., w_T) = \\prod_{t=1}^{T} P(w_t | w_1, ..., w_{t-1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡單的 N-gram 語言模型示範\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class BigramLanguageModel:\n",
    "    \"\"\"最簡單的語言模型：Bigram (2-gram)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "    \n",
    "    def train(self, corpus: List[List[str]]):\n",
    "        \"\"\"訓練 bigram 模型\"\"\"\n",
    "        for sentence in corpus:\n",
    "            # 加入起始和結束標記\n",
    "            tokens = ['<BOS>'] + sentence + ['<EOS>']\n",
    "            for i in range(len(tokens) - 1):\n",
    "                prev_token = tokens[i]\n",
    "                curr_token = tokens[i + 1]\n",
    "                self.bigram_counts[prev_token][curr_token] += 1\n",
    "                self.unigram_counts[prev_token] += 1\n",
    "    \n",
    "    def get_probability(self, prev_token: str, curr_token: str) -> float:\n",
    "        \"\"\"計算 P(curr_token | prev_token)\"\"\"\n",
    "        if self.unigram_counts[prev_token] == 0:\n",
    "            return 0.0\n",
    "        return self.bigram_counts[prev_token][curr_token] / self.unigram_counts[prev_token]\n",
    "    \n",
    "    def generate(self, max_length: int = 20) -> List[str]:\n",
    "        \"\"\"生成文本\"\"\"\n",
    "        result = []\n",
    "        current = '<BOS>'\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # 取得下一個 token 的機率分布\n",
    "            next_tokens = self.bigram_counts[current]\n",
    "            if not next_tokens:\n",
    "                break\n",
    "            \n",
    "            # 依機率抽樣\n",
    "            tokens = list(next_tokens.keys())\n",
    "            weights = list(next_tokens.values())\n",
    "            total = sum(weights)\n",
    "            probs = [w / total for w in weights]\n",
    "            \n",
    "            next_token = random.choices(tokens, probs)[0]\n",
    "            if next_token == '<EOS>':\n",
    "                break\n",
    "            \n",
    "            result.append(next_token)\n",
    "            current = next_token\n",
    "        \n",
    "        return result\n",
    "\n",
    "# 訓練範例\n",
    "corpus = [\n",
    "    ['I', 'love', 'machine', 'learning'],\n",
    "    ['I', 'love', 'deep', 'learning'],\n",
    "    ['machine', 'learning', 'is', 'amazing'],\n",
    "    ['deep', 'learning', 'is', 'powerful'],\n",
    "    ['I', 'study', 'AI', 'every', 'day'],\n",
    "    ['AI', 'is', 'the', 'future'],\n",
    "]\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "model.train(corpus)\n",
    "\n",
    "# 測試機率計算\n",
    "print(\"P('love' | 'I') =\", f\"{model.get_probability('I', 'love'):.3f}\")\n",
    "print(\"P('learning' | 'machine') =\", f\"{model.get_probability('machine', 'learning'):.3f}\")\n",
    "\n",
    "# 生成文本\n",
    "print(\"\\n生成的句子:\")\n",
    "for i in range(5):\n",
    "    generated = model.generate()\n",
    "    print(f\"  {i+1}. {' '.join(generated)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 從 N-gram 到神經網路語言模型\n",
    "\n",
    "N-gram 的限制：\n",
    "- 只能捕捉固定長度的上下文\n",
    "- 詞彙量大時記憶體爆炸\n",
    "- 無法處理未見過的組合\n",
    "\n",
    "神經網路語言模型的優勢：\n",
    "- 學習 word embeddings（詞向量）\n",
    "- 可變長度上下文（RNN → Transformer）\n",
    "- 更好的泛化能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡單的神經網路語言模型\n",
    "class SimpleNeuralLM(nn.Module):\n",
    "    \"\"\"基於 MLP 的簡單語言模型\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embed_dim: int = 64, \n",
    "                 context_length: int = 3, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length\n",
    "        \n",
    "        # 詞嵌入層\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # 前饋網路\n",
    "        self.fc1 = nn.Linear(context_length * embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, context_length)\n",
    "        embeds = self.embedding(x)  # (batch_size, context_length, embed_dim)\n",
    "        embeds = embeds.view(x.size(0), -1)  # 展平\n",
    "        hidden = F.relu(self.fc1(embeds))\n",
    "        logits = self.fc2(hidden)  # (batch_size, vocab_size)\n",
    "        return logits\n",
    "\n",
    "# 視覺化詞嵌入空間\n",
    "def visualize_embeddings(model, vocab, title=\"Word Embeddings\"):\n",
    "    \"\"\"使用 PCA 視覺化詞嵌入\"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    embeddings = model.embedding.weight.detach().cpu().numpy()\n",
    "    \n",
    "    # PCA 降維到 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    coords = pca.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(coords[:, 0], coords[:, 1], alpha=0.6)\n",
    "    \n",
    "    # 標註詞彙\n",
    "    for i, word in enumerate(vocab):\n",
    "        plt.annotate(word, (coords[i, 0], coords[i, 1]), fontsize=9)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%})\")\n",
    "    plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%})\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# 建立簡單模型\n",
    "vocab = ['<PAD>', '<BOS>', '<EOS>', 'I', 'love', 'machine', 'deep', \n",
    "         'learning', 'is', 'amazing', 'powerful', 'AI', 'study', 'the', 'future', 'every', 'day']\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "simple_lm = SimpleNeuralLM(vocab_size=vocab_size, embed_dim=32, context_length=3)\n",
    "print(f\"模型參數量: {sum(p.numel() for p in simple_lm.parameters()):,}\")\n",
    "print(f\"\\n模型結構:\\n{simple_lm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: GPT vs BERT - 兩種預訓練範式\n",
    "\n",
    "### 3.1 架構比較\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│                     GPT vs BERT 比較                           │\n",
    "├──────────────────────────┬─────────────────────────────────────┤\n",
    "│          GPT             │               BERT                  │\n",
    "│   (Decoder-only)         │          (Encoder-only)             │\n",
    "├──────────────────────────┼─────────────────────────────────────┤\n",
    "│                          │                                     │\n",
    "│    ┌───┐                 │         ┌───┐                       │\n",
    "│    │Out│ ← 只能看到      │         │Out│ ← 可以看到            │\n",
    "│    └─▲─┘   左側          │         └─▲─┘   雙向                │\n",
    "│      │                   │           │                         │\n",
    "│  ┌───┴───┐               │       ┌───┴───┐                     │\n",
    "│  │Decoder│               │       │Encoder│                     │\n",
    "│  │ Block │×N             │       │ Block │×N                   │\n",
    "│  └───┬───┘               │       └───┬───┘                     │\n",
    "│      │                   │           │                         │\n",
    "│  [w1][w2][w3]            │       [w1][MASK][w3]                │\n",
    "│    │   │   │             │         │    │    │                 │\n",
    "│    ▼   ▼   ▼             │         ▼    ▼    ▼                 │\n",
    "│   因果注意力             │       雙向注意力                    │\n",
    "│   (Causal)               │       (Bidirectional)               │\n",
    "├──────────────────────────┼─────────────────────────────────────┤\n",
    "│ 預訓練任務:              │ 預訓練任務:                         │\n",
    "│ Next Token Prediction    │ Masked Language Model (MLM)         │\n",
    "├──────────────────────────┼─────────────────────────────────────┤\n",
    "│ 適合任務:                │ 適合任務:                           │\n",
    "│ - 文本生成               │ - 文本分類                          │\n",
    "│ - 對話系統               │ - 命名實體識別                      │\n",
    "│ - 程式碼補全             │ - 問答系統                          │\n",
    "│ - 翻譯                   │ - 句子相似度                        │\n",
    "└──────────────────────────┴─────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化因果注意力 vs 雙向注意力\n",
    "def visualize_attention_masks():\n",
    "    \"\"\"視覺化 GPT (causal) vs BERT (bidirectional) 的注意力遮罩\"\"\"\n",
    "    seq_len = 6\n",
    "    \n",
    "    # GPT: 因果遮罩 (只能看到自己和左邊)\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    \n",
    "    # BERT: 雙向遮罩 (可以看到所有位置，除了 [MASK])\n",
    "    bidirectional_mask = torch.ones(seq_len, seq_len)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # GPT 因果注意力\n",
    "    ax1 = axes[0]\n",
    "    im1 = ax1.imshow(causal_mask, cmap='Blues', vmin=0, vmax=1)\n",
    "    ax1.set_title('GPT: Causal Attention Mask\\n(只能看到左側)', fontsize=12)\n",
    "    ax1.set_xlabel('Key Position')\n",
    "    ax1.set_ylabel('Query Position')\n",
    "    tokens_gpt = ['I', 'love', 'deep', 'learning', 'and', 'AI']\n",
    "    ax1.set_xticks(range(seq_len))\n",
    "    ax1.set_yticks(range(seq_len))\n",
    "    ax1.set_xticklabels(tokens_gpt, fontsize=9)\n",
    "    ax1.set_yticklabels(tokens_gpt, fontsize=9)\n",
    "    \n",
    "    # 在每個格子標註\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            text = '✓' if causal_mask[i, j] == 1 else '✗'\n",
    "            ax1.text(j, i, text, ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    # BERT 雙向注意力\n",
    "    ax2 = axes[1]\n",
    "    im2 = ax2.imshow(bidirectional_mask, cmap='Greens', vmin=0, vmax=1)\n",
    "    ax2.set_title('BERT: Bidirectional Attention Mask\\n(可看到全部)', fontsize=12)\n",
    "    ax2.set_xlabel('Key Position')\n",
    "    ax2.set_ylabel('Query Position')\n",
    "    tokens_bert = ['I', 'love', '[MASK]', 'learning', 'and', 'AI']\n",
    "    ax2.set_xticks(range(seq_len))\n",
    "    ax2.set_yticks(range(seq_len))\n",
    "    ax2.set_xticklabels(tokens_bert, fontsize=9)\n",
    "    ax2.set_yticklabels(tokens_bert, fontsize=9)\n",
    "    \n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            ax2.text(j, i, '✓', ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention_masks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Tokenization - 文本到數字的橋樑\n",
    "\n",
    "### 4.1 為什麼需要 Tokenization？\n",
    "\n",
    "神經網路只能處理數字，因此需要將文本轉換成數字序列。\n",
    "\n",
    "```\n",
    "Tokenization 方式比較:\n",
    "═══════════════════════════════════════════════════════════════\n",
    "\n",
    "方式          │ 範例: \"unhappiness\"          │ 優缺點\n",
    "──────────────┼──────────────────────────────┼───────────────────\n",
    "字元級        │ [u, n, h, a, p, p, i, n, e,  │ + 詞彙量小\n",
    "(Character)  │  s, s]                        │ - 序列太長\n",
    "──────────────┼──────────────────────────────┼───────────────────\n",
    "詞級         │ [unhappiness] 或 [UNK]       │ + 語義清晰\n",
    "(Word)       │                               │ - 詞彙量爆炸, OOV\n",
    "──────────────┼──────────────────────────────┼───────────────────\n",
    "子詞級       │ [un, happiness] 或           │ + 平衡詞彙量與語義\n",
    "(Subword)    │ [un, happ, iness]            │ + 處理未見詞\n",
    "             │                               │ = BPE, WordPiece\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE (Byte Pair Encoding) 演算法簡化實作\n",
    "class SimpleBPE:\n",
    "    \"\"\"簡化版 BPE tokenizer\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 100):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.merges = {}  # 合併規則\n",
    "        self.vocab = {}   # 詞彙表\n",
    "    \n",
    "    def _get_stats(self, corpus: List[List[str]]) -> Dict[Tuple[str, str], int]:\n",
    "        \"\"\"統計相鄰 token pair 頻率\"\"\"\n",
    "        pairs = defaultdict(int)\n",
    "        for word in corpus:\n",
    "            for i in range(len(word) - 1):\n",
    "                pairs[(word[i], word[i+1])] += 1\n",
    "        return pairs\n",
    "    \n",
    "    def _merge(self, corpus: List[List[str]], pair: Tuple[str, str]) -> List[List[str]]:\n",
    "        \"\"\"合併指定的 pair\"\"\"\n",
    "        new_corpus = []\n",
    "        bigram = ' '.join(pair)\n",
    "        replacement = ''.join(pair)\n",
    "        \n",
    "        for word in corpus:\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if i < len(word) - 1 and word[i] == pair[0] and word[i+1] == pair[1]:\n",
    "                    new_word.append(replacement)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_corpus.append(new_word)\n",
    "        return new_corpus\n",
    "    \n",
    "    def train(self, texts: List[str], num_merges: int = 10):\n",
    "        \"\"\"訓練 BPE\"\"\"\n",
    "        # 初始化：將每個字元分開，並加上詞尾標記\n",
    "        corpus = [list(word) + ['</w>'] for text in texts for word in text.split()]\n",
    "        \n",
    "        # 建立初始詞彙表\n",
    "        self.vocab = set()\n",
    "        for word in corpus:\n",
    "            self.vocab.update(word)\n",
    "        \n",
    "        print(f\"初始詞彙量: {len(self.vocab)}\")\n",
    "        print(f\"初始詞彙: {sorted(self.vocab)}\")\n",
    "        print(\"\\n開始 BPE 合併:\")\n",
    "        \n",
    "        # 執行合併\n",
    "        for i in range(num_merges):\n",
    "            pairs = self._get_stats(corpus)\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # 找出最頻繁的 pair\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            merged = ''.join(best_pair)\n",
    "            \n",
    "            print(f\"  {i+1}. 合併 {best_pair} → '{merged}' (頻率: {pairs[best_pair]})\")\n",
    "            \n",
    "            # 執行合併\n",
    "            corpus = self._merge(corpus, best_pair)\n",
    "            self.merges[best_pair] = merged\n",
    "            self.vocab.add(merged)\n",
    "        \n",
    "        print(f\"\\n最終詞彙量: {len(self.vocab)}\")\n",
    "        return corpus\n",
    "\n",
    "# BPE 訓練示範\n",
    "texts = [\n",
    "    \"low lower lowest\",\n",
    "    \"new newer newest\",\n",
    "    \"learning learned learner\"\n",
    "]\n",
    "\n",
    "bpe = SimpleBPE()\n",
    "result = bpe.train(texts, num_merges=15)\n",
    "\n",
    "print(\"\\n分詞結果:\")\n",
    "for i, word in enumerate(result[:6]):\n",
    "    print(f\"  {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Hugging Face transformers 的 Tokenizer\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    \n",
    "    # 載入 GPT-2 tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    # 測試不同文本的 tokenization\n",
    "    test_texts = [\n",
    "        \"Hello, how are you?\",\n",
    "        \"Machine learning is fascinating!\",\n",
    "        \"深度學習很有趣\",  # 中文\n",
    "        \"unhappiness is temporary\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"GPT-2 Tokenization 結果:\")\n",
    "    print(\"=\" * 60)\n",
    "    for text in test_texts:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        print(f\"\\n原文: {text}\")\n",
    "        print(f\"Tokens ({len(tokens)}): {tokens}\")\n",
    "        print(f\"Token IDs: {token_ids}\")\n",
    "    \n",
    "    # 詞彙表大小\n",
    "    print(f\"\\nGPT-2 詞彙表大小: {tokenizer.vocab_size:,}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"請安裝 transformers: pip install transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: 推理參數 - 控制生成行為\n",
    "\n",
    "### 5.1 Temperature（溫度）\n",
    "\n",
    "Temperature 控制機率分布的「銳利度」：\n",
    "\n",
    "$$P_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)}$$\n",
    "\n",
    "- **T = 0**: 最確定性（greedy decoding）\n",
    "- **T = 1**: 原始機率分布\n",
    "- **T > 1**: 更均勻、更隨機\n",
    "- **T < 1**: 更集中、更確定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature 效果視覺化\n",
    "def visualize_temperature_effect():\n",
    "    \"\"\"展示不同 temperature 對機率分布的影響\"\"\"\n",
    "    # 原始 logits (假設有 5 個候選詞)\n",
    "    logits = torch.tensor([2.0, 1.5, 0.5, 0.2, -0.5])\n",
    "    vocab = ['the', 'a', 'an', 'this', 'that']\n",
    "    \n",
    "    temperatures = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(temperatures), figsize=(15, 4))\n",
    "    \n",
    "    for ax, T in zip(axes, temperatures):\n",
    "        if T == 0.1:  # 近似 greedy\n",
    "            probs = F.softmax(logits / T, dim=0).numpy()\n",
    "        else:\n",
    "            probs = F.softmax(logits / T, dim=0).numpy()\n",
    "        \n",
    "        colors = plt.cm.Blues(probs / probs.max())\n",
    "        bars = ax.bar(vocab, probs, color=colors, edgecolor='navy')\n",
    "        ax.set_title(f'T = {T}', fontsize=12)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_ylabel('Probability' if T == 0.1 else '')\n",
    "        \n",
    "        # 標註機率值\n",
    "        for bar, p in zip(bars, probs):\n",
    "            if p > 0.01:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                       f'{p:.2f}', ha='center', fontsize=9)\n",
    "    \n",
    "    plt.suptitle('Temperature 對機率分布的影響', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_temperature_effect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-K 和 Top-P (Nucleus) Sampling 比較\n",
    "def demonstrate_sampling_strategies():\n",
    "    \"\"\"展示不同採樣策略\"\"\"\n",
    "    # 模擬的機率分布（長尾分布）\n",
    "    vocab_size = 20\n",
    "    probs = F.softmax(torch.randn(vocab_size) * 2, dim=0).sort(descending=True)[0]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # 1. Top-K Sampling (K=5)\n",
    "    ax1 = axes[0]\n",
    "    k = 5\n",
    "    colors1 = ['steelblue' if i < k else 'lightgray' for i in range(vocab_size)]\n",
    "    ax1.bar(range(vocab_size), probs.numpy(), color=colors1, edgecolor='black', linewidth=0.5)\n",
    "    ax1.axvline(x=k-0.5, color='red', linestyle='--', label=f'Top-{k} cutoff')\n",
    "    ax1.set_title(f'Top-K Sampling (K={k})', fontsize=12)\n",
    "    ax1.set_xlabel('Token (sorted by prob)')\n",
    "    ax1.set_ylabel('Probability')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Top-P (Nucleus) Sampling (P=0.9)\n",
    "    ax2 = axes[1]\n",
    "    p_threshold = 0.9\n",
    "    cumsum = probs.cumsum(0).numpy()\n",
    "    cutoff_idx = (cumsum < p_threshold).sum() + 1\n",
    "    colors2 = ['forestgreen' if i < cutoff_idx else 'lightgray' for i in range(vocab_size)]\n",
    "    ax2.bar(range(vocab_size), probs.numpy(), color=colors2, edgecolor='black', linewidth=0.5)\n",
    "    ax2.axvline(x=cutoff_idx-0.5, color='red', linestyle='--', label=f'Top-P={p_threshold} cutoff')\n",
    "    ax2.set_title(f'Top-P (Nucleus) Sampling (P={p_threshold})', fontsize=12)\n",
    "    ax2.set_xlabel('Token (sorted by prob)')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. 累積機率曲線\n",
    "    ax3 = axes[2]\n",
    "    ax3.plot(range(vocab_size), cumsum, 'o-', color='purple', markersize=4)\n",
    "    ax3.axhline(y=p_threshold, color='red', linestyle='--', label=f'P={p_threshold}')\n",
    "    ax3.fill_between(range(cutoff_idx), cumsum[:cutoff_idx], alpha=0.3, color='green')\n",
    "    ax3.set_title('Cumulative Probability', fontsize=12)\n",
    "    ax3.set_xlabel('Token (sorted by prob)')\n",
    "    ax3.set_ylabel('Cumulative Prob')\n",
    "    ax3.legend()\n",
    "    ax3.set_ylim(0, 1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Top-K (K=5): 保留前 5 個 token\")\n",
    "    print(f\"Top-P (P=0.9): 保留累積機率達到 90% 的 token (本例 {cutoff_idx} 個)\")\n",
    "\n",
    "demonstrate_sampling_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 生成參數總結\n",
    "\n",
    "```\n",
    "┌────────────────────────────────────────────────────────────────┐\n",
    "│                    生成參數速查表                               │\n",
    "├───────────────┬────────────────────────────────────────────────┤\n",
    "│ 參數           │ 說明                                           │\n",
    "├───────────────┼────────────────────────────────────────────────┤\n",
    "│ temperature   │ 控制隨機性，低=確定，高=隨機                    │\n",
    "│ top_k         │ 只從機率最高的 K 個 token 中採樣                │\n",
    "│ top_p         │ 只從累積機率達到 P 的 token 中採樣              │\n",
    "│ max_tokens    │ 生成的最大 token 數量                           │\n",
    "│ stop          │ 停止生成的特定序列                              │\n",
    "│ frequency_penalty │ 降低重複 token 的機率                       │\n",
    "│ presence_penalty  │ 鼓勵生成新的 token                          │\n",
    "├───────────────┼────────────────────────────────────────────────┤\n",
    "│               │ 建議配置                                        │\n",
    "├───────────────┼────────────────────────────────────────────────┤\n",
    "│ 創意寫作       │ T=0.8-1.0, top_p=0.9                           │\n",
    "│ 程式碼生成     │ T=0.2-0.4, top_p=0.95                          │\n",
    "│ 事實問答       │ T=0.0-0.2 (greedy)                             │\n",
    "│ 翻譯           │ T=0.3-0.5                                      │\n",
    "└───────────────┴────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Prompt Engineering 基礎\n",
    "\n",
    "### 6.1 Prompt 設計原則\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                 Prompt Engineering 最佳實踐                   │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  1. 明確指令 (Be Specific)                                  │\n",
    "│     ❌ \"寫一篇文章\"                                          │\n",
    "│     ✅ \"寫一篇 300 字的科普文章，主題是量子計算，             │\n",
    "│         目標讀者是高中生\"                                    │\n",
    "│                                                             │\n",
    "│  2. 提供範例 (Few-shot)                                     │\n",
    "│     給出輸入-輸出的範例，讓模型學習格式                       │\n",
    "│                                                             │\n",
    "│  3. 角色設定 (System Prompt)                                │\n",
    "│     \"你是一位專業的資料科學家...\"                            │\n",
    "│                                                             │\n",
    "│  4. 結構化輸出 (Output Format)                              │\n",
    "│     要求 JSON、Markdown、或特定格式                          │\n",
    "│                                                             │\n",
    "│  5. 思維鏈 (Chain-of-Thought)                               │\n",
    "│     \"請一步一步思考...\"、\"讓我們分析一下...\"                  │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 模板範例\n",
    "prompt_templates = {\n",
    "    \"zero_shot\": \"\"\"請將以下英文翻譯成中文：\n",
    "{text}\n",
    "\n",
    "中文翻譯：\"\"\",\n",
    "    \n",
    "    \"few_shot\": \"\"\"請將英文翻譯成中文。\n",
    "\n",
    "範例 1:\n",
    "英文: Hello, how are you?\n",
    "中文: 你好，你好嗎？\n",
    "\n",
    "範例 2:\n",
    "英文: Machine learning is amazing.\n",
    "中文: 機器學習太神奇了。\n",
    "\n",
    "現在請翻譯:\n",
    "英文: {text}\n",
    "中文:\"\"\",\n",
    "    \n",
    "    \"chain_of_thought\": \"\"\"問題：{question}\n",
    "\n",
    "請一步一步思考這個問題：\n",
    "\n",
    "步驟 1: 理解問題\n",
    "步驟 2: 分析關鍵資訊\n",
    "步驟 3: 推理過程\n",
    "步驟 4: 得出結論\n",
    "\n",
    "讓我們開始：\"\"\",\n",
    "    \n",
    "    \"role_play\": \"\"\"你是一位經驗豐富的 Python 程式設計師，專精於資料科學和機器學習。\n",
    "你的回答應該：\n",
    "1. 提供可執行的程式碼\n",
    "2. 包含詳細的註解\n",
    "3. 解釋程式碼的工作原理\n",
    "\n",
    "使用者問題：{question}\n",
    "\n",
    "你的回答：\"\"\"\n",
    "}\n",
    "\n",
    "# 展示模板\n",
    "for name, template in prompt_templates.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"模板類型: {name}\")\n",
    "    print('='*60)\n",
    "    print(template.format(text=\"Deep learning is powerful.\", \n",
    "                         question=\"如何用 Python 實作快速排序？\")[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: 實作練習 - 使用 transformers 進行文本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 GPT-2 進行文本生成\n",
    "try:\n",
    "    from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "    import torch\n",
    "    \n",
    "    # 載入模型和 tokenizer\n",
    "    print(\"載入 GPT-2 模型...\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    model.eval()\n",
    "    \n",
    "    # 移到 GPU（如果可用）\n",
    "    model = model.to(device)\n",
    "    print(f\"模型載入完成，使用 {device}\")\n",
    "    print(f\"模型參數量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    def generate_text(prompt: str, max_length: int = 100, \n",
    "                     temperature: float = 1.0, top_k: int = 50, \n",
    "                     top_p: float = 0.95, num_return_sequences: int = 1):\n",
    "        \"\"\"使用 GPT-2 生成文本\"\"\"\n",
    "        # Tokenize 輸入\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "        \n",
    "        # 設定 attention mask\n",
    "        attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        # 生成\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # 解碼\n",
    "        generated_texts = []\n",
    "        for output in outputs:\n",
    "            text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            generated_texts.append(text)\n",
    "        \n",
    "        return generated_texts\n",
    "    \n",
    "    # 測試生成\n",
    "    prompt = \"Artificial intelligence will\"\n",
    "    print(f\"\\n輸入提示: '{prompt}'\")\n",
    "    print(\"\\n生成結果 (temperature=0.7):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = generate_text(prompt, max_length=80, temperature=0.7, num_return_sequences=3)\n",
    "    for i, text in enumerate(results, 1):\n",
    "        print(f\"\\n[{i}] {text}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"請安裝 transformers: pip install transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較不同 temperature 的生成效果\n",
    "try:\n",
    "    prompt = \"The future of technology is\"\n",
    "    temperatures = [0.2, 0.7, 1.0, 1.5]\n",
    "    \n",
    "    print(f\"輸入提示: '{prompt}'\")\n",
    "    print(\"\\n比較不同 Temperature 的生成效果:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n[Temperature = {temp}]\")\n",
    "        result = generate_text(prompt, max_length=60, temperature=temp, num_return_sequences=1)\n",
    "        print(f\"  {result[0]}\")\n",
    "        \n",
    "except NameError:\n",
    "    print(\"請先執行上方的模型載入程式碼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: 練習題\n",
    "\n",
    "### Exercise 1: 實作 Greedy Decoding\n",
    "\n",
    "實作一個簡單的 greedy decoding 函數，每次選擇機率最高的 token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, tokenizer, prompt: str, max_new_tokens: int = 50):\n",
    "    \"\"\"\n",
    "    實作 Greedy Decoding\n",
    "    \n",
    "    Args:\n",
    "        model: GPT-2 模型\n",
    "        tokenizer: tokenizer\n",
    "        prompt: 輸入提示\n",
    "        max_new_tokens: 最大生成 token 數\n",
    "    \n",
    "    Returns:\n",
    "        生成的文本\n",
    "    \"\"\"\n",
    "    # TODO: 實作 greedy decoding\n",
    "    # 提示：\n",
    "    # 1. 將 prompt 轉換成 token ids\n",
    "    # 2. 迴圈生成新 token\n",
    "    # 3. 每次取機率最高的 token (argmax)\n",
    "    # 4. 如果生成 EOS token 則停止\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    generated = input_ids.clone()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 取得模型輸出\n",
    "            outputs = model(generated)\n",
    "            logits = outputs.logits  # (batch, seq_len, vocab_size)\n",
    "            \n",
    "            # 取最後一個位置的 logits\n",
    "            next_token_logits = logits[:, -1, :]  # (batch, vocab_size)\n",
    "            \n",
    "            # Greedy: 選擇機率最高的 token\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # 加入到生成序列\n",
    "            generated = torch.cat([generated, next_token], dim=-1)\n",
    "            \n",
    "            # 檢查是否生成 EOS\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "# 測試\n",
    "try:\n",
    "    prompt = \"Machine learning is\"\n",
    "    result = greedy_decode(model, tokenizer, prompt, max_new_tokens=30)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Greedy output: {result}\")\n",
    "except NameError:\n",
    "    print(\"請先載入 GPT-2 模型\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Temperature Sampling 實作\n",
    "\n",
    "修改上面的 greedy decoding，加入 temperature 參數進行採樣。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temperature_sampling(model, tokenizer, prompt: str, \n",
    "                         max_new_tokens: int = 50, temperature: float = 1.0):\n",
    "    \"\"\"\n",
    "    實作 Temperature Sampling\n",
    "    \n",
    "    Args:\n",
    "        model: GPT-2 模型\n",
    "        tokenizer: tokenizer\n",
    "        prompt: 輸入提示\n",
    "        max_new_tokens: 最大生成 token 數\n",
    "        temperature: 溫度參數 (0 < T <= inf)\n",
    "    \n",
    "    Returns:\n",
    "        生成的文本\n",
    "    \"\"\"\n",
    "    # TODO: 實作 temperature sampling\n",
    "    # 提示：\n",
    "    # 1. 對 logits 除以 temperature\n",
    "    # 2. 使用 softmax 得到機率分布\n",
    "    # 3. 使用 torch.multinomial 進行採樣\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    generated = input_ids.clone()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = model(generated)\n",
    "            logits = outputs.logits[:, -1, :]  # (batch, vocab_size)\n",
    "            \n",
    "            # 應用 temperature\n",
    "            scaled_logits = logits / temperature\n",
    "            \n",
    "            # 轉換成機率\n",
    "            probs = F.softmax(scaled_logits, dim=-1)\n",
    "            \n",
    "            # 採樣\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            generated = torch.cat([generated, next_token], dim=-1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "# 測試不同 temperature\n",
    "try:\n",
    "    prompt = \"The secret to happiness is\"\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    \n",
    "    for temp in [0.3, 0.7, 1.0, 1.5]:\n",
    "        result = temperature_sampling(model, tokenizer, prompt, \n",
    "                                      max_new_tokens=25, temperature=temp)\n",
    "        print(f\"T={temp}: {result}\\n\")\n",
    "except NameError:\n",
    "    print(\"請先載入 GPT-2 模型\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: 實作 Top-K 和 Top-P Sampling\n",
    "\n",
    "結合 Top-K 和 Top-P 進行更精細的生成控制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_sampling(model, tokenizer, prompt: str,\n",
    "                         max_new_tokens: int = 50,\n",
    "                         temperature: float = 1.0,\n",
    "                         top_k: int = 50,\n",
    "                         top_p: float = 0.95):\n",
    "    \"\"\"\n",
    "    實作 Top-K + Top-P (Nucleus) Sampling\n",
    "    \n",
    "    Args:\n",
    "        model: GPT-2 模型\n",
    "        tokenizer: tokenizer\n",
    "        prompt: 輸入提示\n",
    "        max_new_tokens: 最大生成 token 數\n",
    "        temperature: 溫度參數\n",
    "        top_k: 只保留前 K 個機率最高的 token\n",
    "        top_p: 只保留累積機率達到 P 的 token\n",
    "    \n",
    "    Returns:\n",
    "        生成的文本\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    generated = input_ids.clone()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            outputs = model(generated)\n",
    "            logits = outputs.logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Top-K filtering\n",
    "            if top_k > 0:\n",
    "                # 保留前 K 個，其他設為 -inf\n",
    "                top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
    "                logits_filtered = torch.full_like(logits, float('-inf'))\n",
    "                logits_filtered.scatter_(1, top_k_indices, top_k_logits)\n",
    "                logits = logits_filtered\n",
    "            \n",
    "            # Top-P (Nucleus) filtering\n",
    "            if top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                \n",
    "                # 移除累積機率超過 top_p 的 token\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                # 保留第一個超過的（確保至少有一個 token）\n",
    "                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "                sorted_indices_to_remove[:, 0] = False\n",
    "                \n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                    1, sorted_indices, sorted_indices_to_remove\n",
    "                )\n",
    "                logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # 採樣\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            generated = torch.cat([generated, next_token], dim=-1)\n",
    "            \n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "# 測試\n",
    "try:\n",
    "    prompt = \"In the year 2050, humans will\"\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    \n",
    "    # 不同配置比較\n",
    "    configs = [\n",
    "        {\"temperature\": 0.7, \"top_k\": 0, \"top_p\": 1.0, \"name\": \"Only Temperature\"},\n",
    "        {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 1.0, \"name\": \"Top-K=50\"},\n",
    "        {\"temperature\": 0.7, \"top_k\": 0, \"top_p\": 0.9, \"name\": \"Top-P=0.9\"},\n",
    "        {\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.9, \"name\": \"Top-K=50 + Top-P=0.9\"},\n",
    "    ]\n",
    "    \n",
    "    for cfg in configs:\n",
    "        result = top_k_top_p_sampling(\n",
    "            model, tokenizer, prompt,\n",
    "            max_new_tokens=30,\n",
    "            temperature=cfg[\"temperature\"],\n",
    "            top_k=cfg[\"top_k\"],\n",
    "            top_p=cfg[\"top_p\"]\n",
    "        )\n",
    "        print(f\"[{cfg['name']}]\")\n",
    "        print(f\"  {result}\\n\")\n",
    "        \n",
    "except NameError:\n",
    "    print(\"請先載入 GPT-2 模型\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "### 本 Notebook 涵蓋的重點\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    生成式 AI 入門總結                        │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  1. 基礎概念                                                │\n",
    "│     • 判別式 vs 生成式模型                                   │\n",
    "│     • 語言模型的數學定義                                     │\n",
    "│     • N-gram → 神經網路語言模型演進                          │\n",
    "│                                                             │\n",
    "│  2. 架構理解                                                │\n",
    "│     • GPT (Decoder-only, Causal)                           │\n",
    "│     • BERT (Encoder-only, Bidirectional)                   │\n",
    "│     • 預訓練任務差異                                        │\n",
    "│                                                             │\n",
    "│  3. Tokenization                                           │\n",
    "│     • 字元級 vs 詞級 vs 子詞級                               │\n",
    "│     • BPE 演算法                                            │\n",
    "│     • 實際 tokenizer 使用                                   │\n",
    "│                                                             │\n",
    "│  4. 生成控制                                                │\n",
    "│     • Temperature                                          │\n",
    "│     • Top-K Sampling                                       │\n",
    "│     • Top-P (Nucleus) Sampling                             │\n",
    "│                                                             │\n",
    "│  5. Prompt Engineering                                     │\n",
    "│     • Zero-shot / Few-shot                                 │\n",
    "│     • Chain-of-Thought                                     │\n",
    "│     • 角色設定                                              │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 下一步學習\n",
    "\n",
    "- **RAG 系統**: `ai_agents/rag_basic.ipynb`\n",
    "- **LLM 微調**: `language_models/llm_finetuning.ipynb`\n",
    "- **RLHF**: `reinforcement_learning/rlhf_alignment.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 參考資源\n",
    "\n",
    "### 課程\n",
    "- [李宏毅 2025 Fall GenAI-ML](https://speech.ee.ntu.edu.tw/~hylee/GenAI-ML/2025-fall.php)\n",
    "\n",
    "### 論文\n",
    "- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer\n",
    "- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) - GPT-2\n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "### 工具\n",
    "- [Hugging Face Transformers](https://huggingface.co/docs/transformers/)\n",
    "- [OpenAI Tokenizer](https://platform.openai.com/tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
