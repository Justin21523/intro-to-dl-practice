{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8: GAN 與 Diffusion 模型\n",
    "\n",
    "## 學習目標\n",
    "- 理解 GAN 的對抗訓練概念\n",
    "- 實現 DCGAN 生成圖像\n",
    "- 了解 Diffusion 模型的原理\n",
    "- 實現簡單的 DDPM\n",
    "\n",
    "## 為什麼要學這些模型？\n",
    "\n",
    "GAN 和 Diffusion 是目前最強大的圖像生成模型：\n",
    "- **GAN (2014)**：開創性的對抗訓練，生成逼真圖像\n",
    "- **Diffusion (2020+)**：更穩定、更高品質，Stable Diffusion 等的基礎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用設備: {device}\")\n",
    "\n",
    "# 固定隨機種子\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: GAN 基礎概念\n",
    "\n",
    "### 直觀理解\n",
    "\n",
    "GAN 就像是「偽鈔製造者 vs 警察」的對抗遊戲：\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│                                                     │\n",
    "│   噪音 z ──► [生成器 G] ──► 假圖像                   │\n",
    "│               偽鈔製造者      │                     │\n",
    "│                              ▼                     │\n",
    "│   真實圖像 ──────────────► [判別器 D] ──► 真/假？    │\n",
    "│                              警察                   │\n",
    "│                                                     │\n",
    "│   目標：                                            │\n",
    "│   - G 想騙過 D（生成越來越逼真的圖像）              │\n",
    "│   - D 想正確分辨真假                                │\n",
    "│   - 最終 G 生成的圖像逼真到 D 無法分辨              │\n",
    "│                                                     │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 數學表述\n",
    "\n",
    "GAN 的目標函數（最小-最大遊戲）：\n",
    "\n",
    "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]$$\n",
    "\n",
    "- 判別器 D 想最大化這個值（正確分類真假）\n",
    "- 生成器 G 想最小化這個值（騙過判別器）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 MNIST 進行示範\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 歸一化到 [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)\n",
    "\n",
    "print(f\"訓練集大小: {len(train_dataset)}\")\n",
    "\n",
    "# 顯示樣本\n",
    "def show_images(images, nrow=8, title=\"\"):\n",
    "    \"\"\"顯示圖像網格\"\"\"\n",
    "    grid = make_grid(images, nrow=nrow, normalize=True, value_range=(-1, 1))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "sample_images, _ = next(iter(train_loader))\n",
    "show_images(sample_images[:32], title=\"真實 MNIST 樣本\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: 簡單 GAN 實現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    生成器：從隨機噪音生成圖像\n",
    "    \n",
    "    輸入：latent_dim 維的隨機向量\n",
    "    輸出：28x28 的圖像\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=100, img_dim=784):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            \n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            \n",
    "            nn.Linear(1024, img_dim),\n",
    "            nn.Tanh(),  # 輸出在 [-1, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    判別器：判斷圖像是真是假\n",
    "    \n",
    "    輸入：28x28 的圖像\n",
    "    輸出：真假機率（0-1）\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_dim=784):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(img_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),  # 輸出機率\n",
    "        )\n",
    "        \n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        return self.model(img_flat)\n",
    "\n",
    "# 測試\n",
    "latent_dim = 100\n",
    "G = Generator(latent_dim)\n",
    "D = Discriminator()\n",
    "\n",
    "z = torch.randn(4, latent_dim)\n",
    "fake_imgs = G(z)\n",
    "validity = D(fake_imgs.view(-1, 1, 28, 28))\n",
    "\n",
    "print(\"GAN 架構測試：\")\n",
    "print(f\"噪音形狀: {z.shape}\")\n",
    "print(f\"生成圖像形狀: {fake_imgs.shape}\")\n",
    "print(f\"判別器輸出: {validity.squeeze()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, train_loader, epochs=50, latent_dim=100, lr=0.0002):\n",
    "    \"\"\"\n",
    "    訓練 GAN\n",
    "    \n",
    "    關鍵步驟：\n",
    "    1. 訓練判別器：區分真假\n",
    "    2. 訓練生成器：騙過判別器\n",
    "    \"\"\"\n",
    "    \n",
    "    generator = generator.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "    \n",
    "    # 使用 Adam 優化器\n",
    "    opt_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    opt_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # 用於生成樣本的固定噪音\n",
    "    fixed_noise = torch.randn(64, latent_dim).to(device)\n",
    "    \n",
    "    history = {'d_loss': [], 'g_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        g_losses = []\n",
    "        d_losses = []\n",
    "        \n",
    "        for real_imgs, _ in train_loader:\n",
    "            batch_size = real_imgs.size(0)\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            \n",
    "            # 真假標籤\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            \n",
    "            # ========== 訓練判別器 ==========\n",
    "            opt_D.zero_grad()\n",
    "            \n",
    "            # 真實圖像的損失\n",
    "            real_validity = discriminator(real_imgs)\n",
    "            d_loss_real = criterion(real_validity, real_labels)\n",
    "            \n",
    "            # 生成假圖像\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_imgs = generator(z).view(-1, 1, 28, 28)\n",
    "            \n",
    "            # 假圖像的損失（detach 避免梯度傳到生成器）\n",
    "            fake_validity = discriminator(fake_imgs.detach())\n",
    "            d_loss_fake = criterion(fake_validity, fake_labels)\n",
    "            \n",
    "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "            d_loss.backward()\n",
    "            opt_D.step()\n",
    "            \n",
    "            # ========== 訓練生成器 ==========\n",
    "            opt_G.zero_grad()\n",
    "            \n",
    "            # 生成器想讓判別器認為假圖像是真的\n",
    "            fake_validity = discriminator(fake_imgs)\n",
    "            g_loss = criterion(fake_validity, real_labels)  # 注意：用 real_labels\n",
    "            \n",
    "            g_loss.backward()\n",
    "            opt_G.step()\n",
    "            \n",
    "            d_losses.append(d_loss.item())\n",
    "            g_losses.append(g_loss.item())\n",
    "        \n",
    "        avg_d_loss = np.mean(d_losses)\n",
    "        avg_g_loss = np.mean(g_losses)\n",
    "        history['d_loss'].append(avg_d_loss)\n",
    "        history['g_loss'].append(avg_g_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"  D Loss: {avg_d_loss:.4f}, G Loss: {avg_g_loss:.4f}\")\n",
    "            \n",
    "            # 生成樣本\n",
    "            generator.eval()\n",
    "            with torch.no_grad():\n",
    "                samples = generator(fixed_noise).view(-1, 1, 28, 28)\n",
    "            show_images(samples, nrow=8, title=f\"Epoch {epoch+1}\")\n",
    "            generator.train()\n",
    "    \n",
    "    return history\n",
    "\n",
    "# 訓練\n",
    "G = Generator(latent_dim=100).to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "gan_history = train_gan(G, D, train_loader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繪製訓練曲線\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(gan_history['d_loss'], label='Discriminator Loss')\n",
    "plt.plot(gan_history['g_loss'], label='Generator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('GAN 訓練曲線')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: DCGAN (Deep Convolutional GAN)\n",
    "\n",
    "DCGAN 使用卷積/轉置卷積，生成更高品質的圖像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN 生成器\n",
    "    \n",
    "    使用轉置卷積逐步上採樣\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=100, channels=1, features=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # 輸入: [B, latent_dim]\n",
    "        # 輸出: [B, channels, 28, 28]\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # 投影並重塑: [B, latent_dim] -> [B, features*4, 7, 7]\n",
    "            nn.Linear(latent_dim, features * 4 * 7 * 7),\n",
    "            nn.BatchNorm1d(features * 4 * 7 * 7),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # [B, features*4, 7, 7] -> [B, features*2, 14, 14]\n",
    "            nn.ConvTranspose2d(features * 4, features * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # [B, features*2, 14, 14] -> [B, channels, 28, 28]\n",
    "            nn.ConvTranspose2d(features * 2, channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "        self.features = features\n",
    "        \n",
    "    def forward(self, z):\n",
    "        x = self.model(z)\n",
    "        x = x.view(x.size(0), self.features * 4, 7, 7)\n",
    "        return self.conv_blocks(x)\n",
    "\n",
    "class DCDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    DCGAN 判別器\n",
    "    \n",
    "    使用步進卷積逐步下採樣\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels=1, features=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # [B, channels, 28, 28] -> [B, features, 14, 14]\n",
    "            nn.Conv2d(channels, features, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # [B, features, 14, 14] -> [B, features*2, 7, 7]\n",
    "            nn.Conv2d(features, features * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(features * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # [B, features*2, 7, 7] -> [B, 1]\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(features * 2 * 7 * 7, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "\n",
    "# 測試\n",
    "dc_G = DCGenerator(latent_dim=100)\n",
    "dc_D = DCDiscriminator()\n",
    "\n",
    "z = torch.randn(4, 100)\n",
    "fake = dc_G(z)\n",
    "validity = dc_D(fake)\n",
    "\n",
    "print(\"DCGAN 架構測試：\")\n",
    "print(f\"生成圖像形狀: {fake.shape}\")\n",
    "print(f\"判別器輸出: {validity.squeeze()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dcgan(generator, discriminator, train_loader, epochs=30, latent_dim=100, lr=0.0002):\n",
    "    \"\"\"\n",
    "    訓練 DCGAN\n",
    "    \"\"\"\n",
    "    \n",
    "    generator = generator.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "    \n",
    "    opt_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    opt_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    fixed_noise = torch.randn(64, latent_dim).to(device)\n",
    "    \n",
    "    history = {'d_loss': [], 'g_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        g_losses = []\n",
    "        d_losses = []\n",
    "        \n",
    "        for real_imgs, _ in train_loader:\n",
    "            batch_size = real_imgs.size(0)\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            \n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            \n",
    "            # ========== 訓練判別器 ==========\n",
    "            opt_D.zero_grad()\n",
    "            \n",
    "            d_loss_real = criterion(discriminator(real_imgs), real_labels)\n",
    "            \n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_imgs = generator(z)\n",
    "            d_loss_fake = criterion(discriminator(fake_imgs.detach()), fake_labels)\n",
    "            \n",
    "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "            d_loss.backward()\n",
    "            opt_D.step()\n",
    "            \n",
    "            # ========== 訓練生成器 ==========\n",
    "            opt_G.zero_grad()\n",
    "            \n",
    "            g_loss = criterion(discriminator(fake_imgs), real_labels)\n",
    "            g_loss.backward()\n",
    "            opt_G.step()\n",
    "            \n",
    "            d_losses.append(d_loss.item())\n",
    "            g_losses.append(g_loss.item())\n",
    "        \n",
    "        history['d_loss'].append(np.mean(d_losses))\n",
    "        history['g_loss'].append(np.mean(g_losses))\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, D Loss: {history['d_loss'][-1]:.4f}, G Loss: {history['g_loss'][-1]:.4f}\")\n",
    "            \n",
    "            generator.eval()\n",
    "            with torch.no_grad():\n",
    "                samples = generator(fixed_noise)\n",
    "            show_images(samples, nrow=8, title=f\"DCGAN Epoch {epoch+1}\")\n",
    "            generator.train()\n",
    "    \n",
    "    return history\n",
    "\n",
    "# 訓練 DCGAN\n",
    "dc_G = DCGenerator(latent_dim=100).to(device)\n",
    "dc_D = DCDiscriminator().to(device)\n",
    "\n",
    "dcgan_history = train_dcgan(dc_G, dc_D, train_loader, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 潛在空間探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_space_interpolation_gan(generator, latent_dim=100, n_steps=10):\n",
    "    \"\"\"\n",
    "    GAN 潛在空間插值\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    # 兩個隨機起點\n",
    "    z1 = torch.randn(1, latent_dim).to(device)\n",
    "    z2 = torch.randn(1, latent_dim).to(device)\n",
    "    \n",
    "    interpolations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for alpha in np.linspace(0, 1, n_steps):\n",
    "            z = (1 - alpha) * z1 + alpha * z2\n",
    "            img = generator(z)\n",
    "            interpolations.append(img)\n",
    "    \n",
    "    interpolations = torch.cat(interpolations, dim=0)\n",
    "    show_images(interpolations, nrow=n_steps, title=\"GAN 潛在空間插值\")\n",
    "\n",
    "latent_space_interpolation_gan(dc_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Diffusion 模型基礎\n",
    "\n",
    "### 直觀理解\n",
    "\n",
    "Diffusion 模型的想法很簡單：\n",
    "\n",
    "```\n",
    "前向過程（加噪）：逐步向圖像添加高斯噪音\n",
    "x₀ ──► x₁ ──► x₂ ──► ... ──► xT (純噪音)\n",
    "原圖    +噪    +噪           純噪音\n",
    "\n",
    "反向過程（去噪）：學習逐步去除噪音\n",
    "xT ──► xT-1 ──► ... ──► x₁ ──► x₀\n",
    "噪音   -噪      -噪      -噪    清晰圖像\n",
    "\n",
    "訓練目標：預測在每一步添加的噪音\n",
    "```\n",
    "\n",
    "### 數學表述\n",
    "\n",
    "**前向過程**（固定的）：\n",
    "$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)$$\n",
    "\n",
    "**反向過程**（學習的）：\n",
    "$$p_\\theta(x_{t-1} | x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$$\n",
    "\n",
    "**簡化的訓練目標**：\n",
    "$$L = \\mathbb{E}_{t, x_0, \\epsilon}[||\\epsilon - \\epsilon_\\theta(x_t, t)||^2]$$\n",
    "\n",
    "即：預測添加的噪音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionScheduler:\n",
    "    \"\"\"\n",
    "    Diffusion 過程的時間表\n",
    "    \n",
    "    定義了在每個時間步的噪音級別\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        \n",
    "        # 線性調度 beta\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        \n",
    "        # 計算累積乘積\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alpha_cumprod_prev = F.pad(self.alpha_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        # 預計算常用值\n",
    "        self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n",
    "        self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n",
    "        \n",
    "    def add_noise(self, x_0, t, noise=None):\n",
    "        \"\"\"\n",
    "        前向過程：給圖像添加噪音\n",
    "        \n",
    "        x_t = sqrt(alpha_cumprod_t) * x_0 + sqrt(1 - alpha_cumprod_t) * noise\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        \n",
    "        sqrt_alpha = self.sqrt_alpha_cumprod[t].view(-1, 1, 1, 1).to(x_0.device)\n",
    "        sqrt_one_minus_alpha = self.sqrt_one_minus_alpha_cumprod[t].view(-1, 1, 1, 1).to(x_0.device)\n",
    "        \n",
    "        x_t = sqrt_alpha * x_0 + sqrt_one_minus_alpha * noise\n",
    "        return x_t, noise\n",
    "\n",
    "# 可視化前向過程\n",
    "scheduler = DiffusionScheduler(num_timesteps=1000)\n",
    "\n",
    "# 取一張圖片\n",
    "sample_img = sample_images[0:1]  # [1, 1, 28, 28]\n",
    "\n",
    "# 在不同時間步添加噪音\n",
    "timesteps = [0, 100, 250, 500, 750, 999]\n",
    "noisy_images = []\n",
    "\n",
    "for t in timesteps:\n",
    "    t_tensor = torch.tensor([t])\n",
    "    noisy, _ = scheduler.add_noise(sample_img, t_tensor)\n",
    "    noisy_images.append(noisy)\n",
    "\n",
    "noisy_images = torch.cat(noisy_images, dim=0)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(timesteps), figsize=(15, 3))\n",
    "for i, t in enumerate(timesteps):\n",
    "    axes[i].imshow(noisy_images[i].squeeze().numpy(), cmap='gray')\n",
    "    axes[i].set_title(f't = {t}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Diffusion 前向過程：逐步添加噪音')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 簡化的 U-Net 噪音預測器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    時間步的正弦位置編碼\n",
    "    \n",
    "    將時間步 t 編碼為高維向量，讓模型知道當前是第幾步\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, t):\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        \n",
    "        embeddings = np.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = t[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    簡化版 U-Net 用於噪音預測\n",
    "    \n",
    "    對於 MNIST 這樣的簡單資料，不需要完整的 U-Net\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=1, time_dim=256, features=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbedding(time_dim),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "        \n",
    "        # 下採樣路徑\n",
    "        self.down1 = self._conv_block(in_channels, features)\n",
    "        self.down2 = self._conv_block(features, features * 2)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # 中間\n",
    "        self.mid = self._conv_block(features * 2, features * 2)\n",
    "        \n",
    "        # 時間嵌入投影\n",
    "        self.time_proj = nn.Linear(time_dim, features * 2)\n",
    "        \n",
    "        # 上採樣路徑\n",
    "        self.up1 = nn.ConvTranspose2d(features * 2, features * 2, 2, 2)\n",
    "        self.up_conv1 = self._conv_block(features * 4, features)  # 跳躍連接\n",
    "        self.up2 = nn.ConvTranspose2d(features, features, 2, 2)\n",
    "        self.up_conv2 = self._conv_block(features * 2, features)  # 跳躍連接\n",
    "        \n",
    "        # 輸出\n",
    "        self.out = nn.Conv2d(features, in_channels, 1)\n",
    "        \n",
    "    def _conv_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # 時間嵌入\n",
    "        t_emb = self.time_mlp(t.float())\n",
    "        \n",
    "        # 下採樣\n",
    "        d1 = self.down1(x)                    # [B, 64, 28, 28]\n",
    "        d2 = self.down2(self.pool(d1))        # [B, 128, 14, 14]\n",
    "        \n",
    "        # 中間（加入時間資訊）\n",
    "        m = self.mid(self.pool(d2))           # [B, 128, 7, 7]\n",
    "        t_proj = self.time_proj(t_emb)[:, :, None, None]  # [B, 128, 1, 1]\n",
    "        m = m + t_proj\n",
    "        \n",
    "        # 上採樣\n",
    "        u1 = self.up1(m)                      # [B, 128, 14, 14]\n",
    "        u1 = torch.cat([u1, d2], dim=1)       # [B, 256, 14, 14]\n",
    "        u1 = self.up_conv1(u1)                # [B, 64, 14, 14]\n",
    "        \n",
    "        u2 = self.up2(u1)                     # [B, 64, 28, 28]\n",
    "        u2 = torch.cat([u2, d1], dim=1)       # [B, 128, 28, 28]\n",
    "        u2 = self.up_conv2(u2)                # [B, 64, 28, 28]\n",
    "        \n",
    "        return self.out(u2)                   # [B, 1, 28, 28]\n",
    "\n",
    "# 測試\n",
    "unet = SimpleUNet()\n",
    "x = torch.randn(4, 1, 28, 28)\n",
    "t = torch.randint(0, 1000, (4,))\n",
    "out = unet(x, t)\n",
    "print(f\"U-Net 輸入形狀: {x.shape}\")\n",
    "print(f\"U-Net 輸出形狀: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練 Diffusion 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffusion(model, scheduler, train_loader, epochs=20, lr=1e-4):\n",
    "    \"\"\"\n",
    "    訓練 Diffusion 模型\n",
    "    \n",
    "    訓練目標：預測添加到圖像的噪音\n",
    "    \"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for images, _ in pbar:\n",
    "            images = images.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            # 隨機選擇時間步\n",
    "            t = torch.randint(0, scheduler.num_timesteps, (batch_size,)).to(device)\n",
    "            \n",
    "            # 添加噪音\n",
    "            noise = torch.randn_like(images)\n",
    "            noisy_images, _ = scheduler.add_noise(images, t.cpu(), noise)\n",
    "            noisy_images = noisy_images.to(device)\n",
    "            \n",
    "            # 預測噪音\n",
    "            predicted_noise = model(noisy_images, t)\n",
    "            \n",
    "            # 損失：預測噪音與實際噪音的 MSE\n",
    "            loss = F.mse_loss(predicted_noise, noise)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_loss = np.mean(losses)\n",
    "        history['loss'].append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# 訓練\n",
    "diffusion_model = SimpleUNet().to(device)\n",
    "scheduler = DiffusionScheduler(num_timesteps=1000)\n",
    "\n",
    "diffusion_history = train_diffusion(diffusion_model, scheduler, train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 從 Diffusion 模型生成樣本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_diffusion(model, scheduler, n_samples=16, img_size=28):\n",
    "    \"\"\"\n",
    "    從 Diffusion 模型生成樣本\n",
    "    \n",
    "    反向過程：從純噪音逐步去噪\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # 從純噪音開始\n",
    "    x = torch.randn(n_samples, 1, img_size, img_size).to(device)\n",
    "    \n",
    "    # 反向迭代\n",
    "    for t in tqdm(reversed(range(scheduler.num_timesteps)), desc=\"Sampling\", total=scheduler.num_timesteps):\n",
    "        t_batch = torch.full((n_samples,), t, dtype=torch.long).to(device)\n",
    "        \n",
    "        # 預測噪音\n",
    "        predicted_noise = model(x, t_batch)\n",
    "        \n",
    "        # 計算係數\n",
    "        alpha = scheduler.alphas[t]\n",
    "        alpha_cumprod = scheduler.alpha_cumprod[t]\n",
    "        alpha_cumprod_prev = scheduler.alpha_cumprod_prev[t]\n",
    "        beta = scheduler.betas[t]\n",
    "        \n",
    "        # 計算均值\n",
    "        coef1 = 1 / torch.sqrt(alpha)\n",
    "        coef2 = beta / torch.sqrt(1 - alpha_cumprod)\n",
    "        \n",
    "        mean = coef1 * (x - coef2 * predicted_noise)\n",
    "        \n",
    "        # 添加噪音（除了最後一步）\n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma = torch.sqrt(beta)\n",
    "            x = mean + sigma * noise\n",
    "        else:\n",
    "            x = mean\n",
    "    \n",
    "    return x\n",
    "\n",
    "# 生成樣本\n",
    "samples = sample_diffusion(diffusion_model, scheduler, n_samples=64)\n",
    "show_images(samples, nrow=8, title=\"Diffusion 模型生成的樣本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_generation_process(model, scheduler, n_steps_to_show=10):\n",
    "    \"\"\"\n",
    "    可視化生成過程的中間步驟\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    x = torch.randn(1, 1, 28, 28).to(device)\n",
    "    \n",
    "    # 選擇要顯示的時間步\n",
    "    steps_to_show = np.linspace(scheduler.num_timesteps - 1, 0, n_steps_to_show, dtype=int)\n",
    "    intermediate_images = []\n",
    "    \n",
    "    for t in reversed(range(scheduler.num_timesteps)):\n",
    "        t_batch = torch.tensor([t]).to(device)\n",
    "        \n",
    "        predicted_noise = model(x, t_batch)\n",
    "        \n",
    "        alpha = scheduler.alphas[t]\n",
    "        alpha_cumprod = scheduler.alpha_cumprod[t]\n",
    "        beta = scheduler.betas[t]\n",
    "        \n",
    "        coef1 = 1 / torch.sqrt(alpha)\n",
    "        coef2 = beta / torch.sqrt(1 - alpha_cumprod)\n",
    "        \n",
    "        mean = coef1 * (x - coef2 * predicted_noise)\n",
    "        \n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma = torch.sqrt(beta)\n",
    "            x = mean + sigma * noise\n",
    "        else:\n",
    "            x = mean\n",
    "        \n",
    "        if t in steps_to_show:\n",
    "            intermediate_images.append((t, x.clone()))\n",
    "    \n",
    "    # 可視化\n",
    "    fig, axes = plt.subplots(1, len(intermediate_images), figsize=(15, 2))\n",
    "    \n",
    "    for i, (t, img) in enumerate(intermediate_images):\n",
    "        axes[i].imshow(img.cpu().squeeze(), cmap='gray')\n",
    "        axes[i].set_title(f't = {t}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Diffusion 反向過程：從噪音到清晰圖像')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_generation_process(diffusion_model, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 練習題\n",
    "\n",
    "### 練習 1: 條件 GAN (CGAN)\n",
    "\n",
    "**目標**: 實現條件 GAN，可以指定生成特定數字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1: 條件 GAN\n",
    "\n",
    "class ConditionalGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    條件生成器：輸入噪音 + 類別標籤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=100, num_classes=10, img_dim=784):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 類別嵌入\n",
    "        self.label_embedding = nn.Embedding(num_classes, num_classes)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            \n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            \n",
    "            nn.Linear(1024, img_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, z, labels):\n",
    "        label_emb = self.label_embedding(labels)\n",
    "        x = torch.cat([z, label_emb], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "class ConditionalDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    條件判別器：輸入圖像 + 類別標籤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10, img_dim=784):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_embedding = nn.Embedding(num_classes, num_classes)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(img_dim + num_classes, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, img, labels):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        label_emb = self.label_embedding(labels)\n",
    "        x = torch.cat([img_flat, label_emb], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "def train_cgan(generator, discriminator, train_loader, epochs=50, latent_dim=100, lr=0.0002):\n",
    "    \"\"\"\n",
    "    訓練條件 GAN\n",
    "    \"\"\"\n",
    "    \n",
    "    generator = generator.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "    \n",
    "    opt_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    opt_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        g_losses = []\n",
    "        d_losses = []\n",
    "        \n",
    "        for real_imgs, labels in train_loader:\n",
    "            batch_size = real_imgs.size(0)\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            \n",
    "            # 訓練判別器\n",
    "            opt_D.zero_grad()\n",
    "            \n",
    "            d_loss_real = criterion(discriminator(real_imgs, labels), real_labels)\n",
    "            \n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_imgs = generator(z, labels).view(-1, 1, 28, 28)\n",
    "            d_loss_fake = criterion(discriminator(fake_imgs.detach(), labels), fake_labels)\n",
    "            \n",
    "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "            d_loss.backward()\n",
    "            opt_D.step()\n",
    "            \n",
    "            # 訓練生成器\n",
    "            opt_G.zero_grad()\n",
    "            \n",
    "            g_loss = criterion(discriminator(fake_imgs, labels), real_labels)\n",
    "            g_loss.backward()\n",
    "            opt_G.step()\n",
    "            \n",
    "            d_losses.append(d_loss.item())\n",
    "            g_losses.append(g_loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, D Loss: {np.mean(d_losses):.4f}, G Loss: {np.mean(g_losses):.4f}\")\n",
    "    \n",
    "    return generator, discriminator\n",
    "\n",
    "# 訓練\n",
    "cgan_G = ConditionalGenerator(latent_dim=100)\n",
    "cgan_D = ConditionalDiscriminator()\n",
    "\n",
    "cgan_G, cgan_D = train_cgan(cgan_G, cgan_D, train_loader, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試條件生成\n",
    "def generate_conditional_samples_gan(generator, latent_dim=100, n_per_class=5):\n",
    "    generator.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(10, n_per_class, figsize=(n_per_class * 1.5, 15))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for digit in range(10):\n",
    "            z = torch.randn(n_per_class, latent_dim).to(device)\n",
    "            labels = torch.full((n_per_class,), digit, dtype=torch.long).to(device)\n",
    "            samples = generator(z, labels).view(-1, 28, 28)\n",
    "            \n",
    "            for i in range(n_per_class):\n",
    "                axes[digit, i].imshow(samples[i].cpu(), cmap='gray')\n",
    "                axes[digit, i].axis('off')\n",
    "            \n",
    "            axes[digit, 0].set_ylabel(str(digit), fontsize=14, rotation=0, labelpad=15)\n",
    "    \n",
    "    plt.suptitle('條件 GAN 生成結果')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "generate_conditional_samples_gan(cgan_G, n_per_class=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2: WGAN-GP\n",
    "\n",
    "**目標**: 實現 Wasserstein GAN with Gradient Penalty，改善訓練穩定性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2: WGAN-GP\n",
    "\n",
    "class WGANCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    WGAN 評論器（不是判別器）\n",
    "    \n",
    "    輸出不需要 Sigmoid，直接輸出分數\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_dim=784):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(img_dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            \n",
    "            nn.Linear(256, 1),\n",
    "            # 無 Sigmoid！\n",
    "        )\n",
    "        \n",
    "    def forward(self, img):\n",
    "        return self.model(img.view(img.size(0), -1))\n",
    "\n",
    "def compute_gradient_penalty(critic, real_imgs, fake_imgs):\n",
    "    \"\"\"\n",
    "    計算梯度懲罰\n",
    "    \n",
    "    在真實和假圖像之間插值，計算梯度範數\n",
    "    \"\"\"\n",
    "    batch_size = real_imgs.size(0)\n",
    "    \n",
    "    # 隨機插值係數\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1).to(real_imgs.device)\n",
    "    \n",
    "    # 插值圖像\n",
    "    interpolated = (alpha * real_imgs + (1 - alpha) * fake_imgs).requires_grad_(True)\n",
    "    \n",
    "    # 計算評論器輸出\n",
    "    d_interpolated = critic(interpolated)\n",
    "    \n",
    "    # 計算梯度\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolated,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones_like(d_interpolated),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty\n",
    "\n",
    "def train_wgan_gp(generator, critic, train_loader, epochs=50, latent_dim=100, \n",
    "                  lr=1e-4, n_critic=5, lambda_gp=10):\n",
    "    \"\"\"\n",
    "    訓練 WGAN-GP\n",
    "    \n",
    "    關鍵改變：\n",
    "    - 評論器訓練更多次\n",
    "    - 使用 Wasserstein 損失\n",
    "    - 添加梯度懲罰\n",
    "    \"\"\"\n",
    "    \n",
    "    generator = generator.to(device)\n",
    "    critic = critic.to(device)\n",
    "    \n",
    "    opt_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0, 0.9))\n",
    "    opt_C = torch.optim.Adam(critic.parameters(), lr=lr, betas=(0, 0.9))\n",
    "    \n",
    "    fixed_noise = torch.randn(64, latent_dim).to(device)\n",
    "    history = {'c_loss': [], 'g_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        c_losses = []\n",
    "        g_losses = []\n",
    "        \n",
    "        for batch_idx, (real_imgs, _) in enumerate(train_loader):\n",
    "            batch_size = real_imgs.size(0)\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            \n",
    "            # 訓練評論器（多次）\n",
    "            for _ in range(n_critic):\n",
    "                opt_C.zero_grad()\n",
    "                \n",
    "                # 生成假圖像\n",
    "                z = torch.randn(batch_size, latent_dim).to(device)\n",
    "                fake_imgs = generator(z).view(-1, 1, 28, 28)\n",
    "                \n",
    "                # Wasserstein 損失\n",
    "                c_real = critic(real_imgs).mean()\n",
    "                c_fake = critic(fake_imgs.detach()).mean()\n",
    "                \n",
    "                # 梯度懲罰\n",
    "                gp = compute_gradient_penalty(critic, real_imgs, fake_imgs)\n",
    "                \n",
    "                # 評論器損失 = 假 - 真 + 懲罰\n",
    "                c_loss = c_fake - c_real + lambda_gp * gp\n",
    "                \n",
    "                c_loss.backward()\n",
    "                opt_C.step()\n",
    "            \n",
    "            # 訓練生成器\n",
    "            opt_G.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            fake_imgs = generator(z).view(-1, 1, 28, 28)\n",
    "            \n",
    "            # 生成器損失 = -假分數（想最大化假分數）\n",
    "            g_loss = -critic(fake_imgs).mean()\n",
    "            \n",
    "            g_loss.backward()\n",
    "            opt_G.step()\n",
    "            \n",
    "            c_losses.append(c_loss.item())\n",
    "            g_losses.append(g_loss.item())\n",
    "        \n",
    "        history['c_loss'].append(np.mean(c_losses))\n",
    "        history['g_loss'].append(np.mean(g_losses))\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"  C Loss: {history['c_loss'][-1]:.4f}, G Loss: {history['g_loss'][-1]:.4f}\")\n",
    "            \n",
    "            generator.eval()\n",
    "            with torch.no_grad():\n",
    "                samples = generator(fixed_noise).view(-1, 1, 28, 28)\n",
    "            show_images(samples, nrow=8, title=f\"WGAN-GP Epoch {epoch+1}\")\n",
    "            generator.train()\n",
    "    \n",
    "    return history\n",
    "\n",
    "# 訓練 WGAN-GP\n",
    "wgan_G = Generator(latent_dim=100).to(device)\n",
    "wgan_C = WGANCritic().to(device)\n",
    "\n",
    "wgan_history = train_wgan_gp(wgan_G, wgan_C, train_loader, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3: 條件 Diffusion\n",
    "\n",
    "**目標**: 為 Diffusion 模型添加類別條件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 3: 條件 Diffusion\n",
    "\n",
    "class ConditionalUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    帶類別條件的 U-Net\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels=1, num_classes=10, time_dim=256, features=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbedding(time_dim),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim),\n",
    "        )\n",
    "        \n",
    "        # 類別嵌入\n",
    "        self.class_embedding = nn.Embedding(num_classes, time_dim)\n",
    "        \n",
    "        # 下採樣\n",
    "        self.down1 = self._conv_block(in_channels, features)\n",
    "        self.down2 = self._conv_block(features, features * 2)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # 中間\n",
    "        self.mid = self._conv_block(features * 2, features * 2)\n",
    "        \n",
    "        # 條件投影（時間 + 類別）\n",
    "        self.cond_proj = nn.Linear(time_dim * 2, features * 2)\n",
    "        \n",
    "        # 上採樣\n",
    "        self.up1 = nn.ConvTranspose2d(features * 2, features * 2, 2, 2)\n",
    "        self.up_conv1 = self._conv_block(features * 4, features)\n",
    "        self.up2 = nn.ConvTranspose2d(features, features, 2, 2)\n",
    "        self.up_conv2 = self._conv_block(features * 2, features)\n",
    "        \n",
    "        self.out = nn.Conv2d(features, in_channels, 1)\n",
    "        \n",
    "    def _conv_block(self, in_ch, out_ch):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.GroupNorm(8, out_ch),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 噪音圖像\n",
    "            t: 時間步\n",
    "            y: 類別標籤\n",
    "        \"\"\"\n",
    "        # 時間和類別嵌入\n",
    "        t_emb = self.time_mlp(t.float())\n",
    "        c_emb = self.class_embedding(y)\n",
    "        cond = torch.cat([t_emb, c_emb], dim=1)  # 合併條件\n",
    "        \n",
    "        # 下採樣\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(self.pool(d1))\n",
    "        \n",
    "        # 中間（加入條件）\n",
    "        m = self.mid(self.pool(d2))\n",
    "        cond_proj = self.cond_proj(cond)[:, :, None, None]\n",
    "        m = m + cond_proj\n",
    "        \n",
    "        # 上採樣\n",
    "        u1 = self.up1(m)\n",
    "        u1 = torch.cat([u1, d2], dim=1)\n",
    "        u1 = self.up_conv1(u1)\n",
    "        \n",
    "        u2 = self.up2(u1)\n",
    "        u2 = torch.cat([u2, d1], dim=1)\n",
    "        u2 = self.up_conv2(u2)\n",
    "        \n",
    "        return self.out(u2)\n",
    "\n",
    "def train_conditional_diffusion(model, scheduler, train_loader, epochs=20, lr=1e-4):\n",
    "    \"\"\"訓練條件 Diffusion\"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            \n",
    "            t = torch.randint(0, scheduler.num_timesteps, (batch_size,)).to(device)\n",
    "            \n",
    "            noise = torch.randn_like(images)\n",
    "            noisy_images, _ = scheduler.add_noise(images, t.cpu(), noise)\n",
    "            noisy_images = noisy_images.to(device)\n",
    "            \n",
    "            predicted_noise = model(noisy_images, t, labels)\n",
    "            \n",
    "            loss = F.mse_loss(predicted_noise, noise)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Avg Loss: {np.mean(losses):.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 訓練\n",
    "cond_diffusion = ConditionalUNet(num_classes=10).to(device)\n",
    "scheduler = DiffusionScheduler(num_timesteps=1000)\n",
    "\n",
    "cond_diffusion = train_conditional_diffusion(cond_diffusion, scheduler, train_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_conditional_diffusion(model, scheduler, labels, img_size=28):\n",
    "    \"\"\"\n",
    "    條件生成\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    n_samples = len(labels)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    x = torch.randn(n_samples, 1, img_size, img_size).to(device)\n",
    "    \n",
    "    for t in tqdm(reversed(range(scheduler.num_timesteps)), desc=\"Sampling\", total=scheduler.num_timesteps):\n",
    "        t_batch = torch.full((n_samples,), t, dtype=torch.long).to(device)\n",
    "        \n",
    "        predicted_noise = model(x, t_batch, labels)\n",
    "        \n",
    "        alpha = scheduler.alphas[t]\n",
    "        alpha_cumprod = scheduler.alpha_cumprod[t]\n",
    "        beta = scheduler.betas[t]\n",
    "        \n",
    "        coef1 = 1 / torch.sqrt(alpha)\n",
    "        coef2 = beta / torch.sqrt(1 - alpha_cumprod)\n",
    "        \n",
    "        mean = coef1 * (x - coef2 * predicted_noise)\n",
    "        \n",
    "        if t > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "            sigma = torch.sqrt(beta)\n",
    "            x = mean + sigma * noise\n",
    "        else:\n",
    "            x = mean\n",
    "    \n",
    "    return x\n",
    "\n",
    "# 生成每個數字\n",
    "fig, axes = plt.subplots(10, 8, figsize=(10, 12))\n",
    "\n",
    "for digit in range(10):\n",
    "    labels = torch.full((8,), digit, dtype=torch.long)\n",
    "    samples = sample_conditional_diffusion(cond_diffusion, scheduler, labels)\n",
    "    \n",
    "    for i in range(8):\n",
    "        axes[digit, i].imshow(samples[i].cpu().squeeze(), cmap='gray')\n",
    "        axes[digit, i].axis('off')\n",
    "    axes[digit, 0].set_ylabel(str(digit), fontsize=12, rotation=0, labelpad=15)\n",
    "\n",
    "plt.suptitle('條件 Diffusion 生成結果')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Part 5: 進階技巧與實務應用\n\n### 5.1 GAN 訓練技巧\n\nGAN 訓練是出了名的困難。以下是一些實用技巧：",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== GAN 訓練技巧 ==========\n\n# 技巧 1: Label Smoothing - 讓判別器不要太自信\ndef get_smoothed_labels(batch_size, device, label_type='real', smoothing=0.1):\n    \"\"\"\n    標籤平滑：\n    - 真實標籤: 0.9 而不是 1.0\n    - 假標籤: 0.1 而不是 0.0\n    這防止判別器過於自信，有助於生成器學習\n    \"\"\"\n    if label_type == 'real':\n        return torch.full((batch_size, 1), 1.0 - smoothing, device=device)\n    else:\n        return torch.full((batch_size, 1), smoothing, device=device)\n\n# 技巧 2: Two Time-scale Update Rule (TTUR)\n# 判別器使用較高學習率，生成器使用較低學習率\ndef create_ttur_optimizers(generator, discriminator):\n    \"\"\"\n    TTUR: 判別器 lr > 生成器 lr\n    這有助於訓練穩定性\n    \"\"\"\n    opt_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n    opt_D = torch.optim.Adam(discriminator.parameters(), lr=4e-4, betas=(0.5, 0.999))\n    return opt_G, opt_D\n\n# 技巧 3: Feature Matching Loss - 讓生成圖像在特徵空間更接近真實圖像\nclass DiscriminatorWithFeatures(nn.Module):\n    \"\"\"\n    可以輸出中間特徵的判別器\n    用於 Feature Matching Loss\n    \"\"\"\n    \n    def __init__(self, img_dim=784):\n        super().__init__()\n        \n        self.layer1 = nn.Sequential(\n            nn.Linear(img_dim, 512),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n        )\n        \n        self.layer2 = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(0.3),\n        )\n        \n        self.out = nn.Sequential(\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n        \n    def forward(self, img, return_features=False):\n        x = img.view(img.size(0), -1)\n        f1 = self.layer1(x)\n        f2 = self.layer2(f1)\n        out = self.out(f2)\n        \n        if return_features:\n            return out, [f1, f2]\n        return out\n\ndef feature_matching_loss(discriminator, real_imgs, fake_imgs):\n    \"\"\"\n    Feature Matching Loss:\n    讓生成圖像的判別器特徵接近真實圖像的特徵\n    \"\"\"\n    _, real_features = discriminator(real_imgs, return_features=True)\n    _, fake_features = discriminator(fake_imgs, return_features=True)\n    \n    loss = 0\n    for real_f, fake_f in zip(real_features, fake_features):\n        loss += F.mse_loss(fake_f.mean(0), real_f.mean(0).detach())\n    \n    return loss\n\n# 技巧 4: 檢測模式崩潰 (Mode Collapse)\ndef check_mode_collapse(generator, latent_dim=100, n_samples=100, threshold=0.1):\n    \"\"\"\n    模式崩潰檢測：\n    如果生成的圖像太相似，可能發生了模式崩潰\n    \"\"\"\n    generator.eval()\n    with torch.no_grad():\n        z = torch.randn(n_samples, latent_dim).to(device)\n        samples = generator(z).view(n_samples, -1)\n        \n        # 計算樣本間的平均距離\n        distances = []\n        for i in range(n_samples):\n            for j in range(i+1, min(i+10, n_samples)):  # 只計算部分對\n                dist = torch.dist(samples[i], samples[j]).item()\n                distances.append(dist)\n        \n        avg_distance = np.mean(distances)\n        std_distance = np.std(distances)\n        \n        print(f\"樣本間平均距離: {avg_distance:.4f} ± {std_distance:.4f}\")\n        \n        if avg_distance < threshold:\n            print(\"⚠️ 警告：可能發生了模式崩潰！生成的樣本過於相似。\")\n            return True\n        else:\n            print(\"✓ 生成樣本具有足夠的多樣性\")\n            return False\n    \n    generator.train()\n\n# 測試模式崩潰檢測\nprint(\"檢測 DCGAN 的模式崩潰情況：\")\ncheck_mode_collapse(dc_G)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5.2 DDIM: 加速 Diffusion 採樣\n\nDDPM 需要 1000 步才能生成，太慢了！DDIM (Denoising Diffusion Implicit Models) 可以大幅減少步數。\n\n```\nDDPM: 1000 步 → 約 1000 次模型推理\nDDIM: 50-100 步 → 相同品質，快 10-20 倍！\n\n關鍵區別：\n- DDPM: 隨機採樣（每步加噪音）\n- DDIM: 確定性採樣（可以跳過步驟）\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== DDIM 快速採樣 ==========\n\n@torch.no_grad()\ndef ddim_sample(model, scheduler, n_samples=16, img_size=28, \n                ddim_steps=50, eta=0.0):\n    \"\"\"\n    DDIM 採樣 - 大幅減少生成步數\n    \n    Args:\n        ddim_steps: 實際採樣步數（可以遠小於訓練時的 1000 步）\n        eta: 0 = 完全確定性, 1 = 等同 DDPM\n    \"\"\"\n    model.eval()\n    \n    # 選擇子序列時間步 (均勻分布)\n    # 例如: 1000 步中選 50 步 -> [0, 20, 40, 60, ..., 980]\n    timesteps = np.linspace(0, scheduler.num_timesteps - 1, ddim_steps, dtype=int)\n    timesteps = list(reversed(timesteps))  # 從高到低\n    \n    # 從純噪音開始\n    x = torch.randn(n_samples, 1, img_size, img_size).to(device)\n    \n    for i, t in enumerate(tqdm(timesteps, desc=\"DDIM Sampling\")):\n        t_batch = torch.full((n_samples,), t, dtype=torch.long).to(device)\n        \n        # 預測噪音\n        predicted_noise = model(x, t_batch)\n        \n        # 當前和下一個時間步的 alpha\n        alpha_cumprod_t = scheduler.alpha_cumprod[t]\n        \n        if i < len(timesteps) - 1:\n            alpha_cumprod_prev = scheduler.alpha_cumprod[timesteps[i + 1]]\n        else:\n            alpha_cumprod_prev = torch.tensor(1.0)\n        \n        # 預測 x0\n        x0_pred = (x - torch.sqrt(1 - alpha_cumprod_t) * predicted_noise) / torch.sqrt(alpha_cumprod_t)\n        x0_pred = torch.clamp(x0_pred, -1, 1)  # 穩定性\n        \n        # DDIM 更新公式\n        sigma = eta * torch.sqrt((1 - alpha_cumprod_prev) / (1 - alpha_cumprod_t)) * \\\n                torch.sqrt(1 - alpha_cumprod_t / alpha_cumprod_prev)\n        \n        # \"方向\" 指向 x_t\n        dir_xt = torch.sqrt(1 - alpha_cumprod_prev - sigma ** 2) * predicted_noise\n        \n        # 下一步\n        x = torch.sqrt(alpha_cumprod_prev) * x0_pred + dir_xt\n        \n        # 如果 eta > 0, 加入一些隨機噪音\n        if eta > 0 and i < len(timesteps) - 1:\n            noise = torch.randn_like(x)\n            x = x + sigma * noise\n    \n    return x\n\n# 比較 DDPM vs DDIM 速度\nimport time\n\nprint(\"比較生成速度：\")\n\n# DDPM (原始方法)\nstart = time.time()\nsamples_ddpm = sample_diffusion(diffusion_model, scheduler, n_samples=4)\nddpm_time = time.time() - start\nprint(f\"DDPM (1000 步): {ddpm_time:.2f} 秒\")\n\n# DDIM (50 步)\nstart = time.time()\nsamples_ddim_50 = ddim_sample(diffusion_model, scheduler, n_samples=4, ddim_steps=50)\nddim_50_time = time.time() - start\nprint(f\"DDIM (50 步): {ddim_50_time:.2f} 秒\")\n\n# DDIM (100 步)\nstart = time.time()\nsamples_ddim_100 = ddim_sample(diffusion_model, scheduler, n_samples=4, ddim_steps=100)\nddim_100_time = time.time() - start\nprint(f\"DDIM (100 步): {ddim_100_time:.2f} 秒\")\n\nprint(f\"\\n加速比: DDPM vs DDIM-50 = {ddpm_time/ddim_50_time:.1f}x\")\n\n# 可視化比較\nfig, axes = plt.subplots(3, 4, figsize=(8, 6))\nfor i in range(4):\n    axes[0, i].imshow(samples_ddpm[i].cpu().squeeze(), cmap='gray')\n    axes[0, i].axis('off')\n    axes[1, i].imshow(samples_ddim_50[i].cpu().squeeze(), cmap='gray')\n    axes[1, i].axis('off')\n    axes[2, i].imshow(samples_ddim_100[i].cpu().squeeze(), cmap='gray')\n    axes[2, i].axis('off')\n\naxes[0, 0].set_ylabel('DDPM\\n(1000步)', fontsize=10, rotation=0, labelpad=40)\naxes[1, 0].set_ylabel('DDIM\\n(50步)', fontsize=10, rotation=0, labelpad=40)\naxes[2, 0].set_ylabel('DDIM\\n(100步)', fontsize=10, rotation=0, labelpad=40)\n\nplt.suptitle('DDPM vs DDIM 生成品質比較')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5.3 Classifier-Free Guidance (CFG)\n\nClassifier-Free Guidance 是 Stable Diffusion、DALL-E 2 等模型的核心技術，讓你可以控制「條件」的強度。\n\n```\n概念：\n- 訓練時：隨機丟棄條件（比如 10% 機率不提供類別標籤）\n- 推理時：同時預測有條件和無條件的噪音，然後組合\n\nCFG 公式：\nnoise_pred = noise_uncond + guidance_scale × (noise_cond - noise_uncond)\n\nguidance_scale:\n- 1.0 = 正常採樣\n- > 1.0 = 更強的條件引導（更像指定的類別）\n- >> 7.5 通常效果最好（Stable Diffusion 默認值）\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== Classifier-Free Guidance ==========\n\nclass CFGConditionalUNet(nn.Module):\n    \"\"\"\n    支援 Classifier-Free Guidance 的 U-Net\n    \n    關鍵：支持無條件生成（class = null）\n    \"\"\"\n    \n    def __init__(self, in_channels=1, num_classes=10, time_dim=256, features=64):\n        super().__init__()\n        \n        self.num_classes = num_classes\n        \n        self.time_mlp = nn.Sequential(\n            SinusoidalPositionEmbedding(time_dim),\n            nn.Linear(time_dim, time_dim),\n            nn.GELU(),\n            nn.Linear(time_dim, time_dim),\n        )\n        \n        # 類別嵌入（+1 給 \"無條件\" 類別）\n        self.class_embedding = nn.Embedding(num_classes + 1, time_dim)\n        \n        # 網路結構同前\n        self.down1 = self._conv_block(in_channels, features)\n        self.down2 = self._conv_block(features, features * 2)\n        self.pool = nn.MaxPool2d(2)\n        \n        self.mid = self._conv_block(features * 2, features * 2)\n        self.cond_proj = nn.Linear(time_dim * 2, features * 2)\n        \n        self.up1 = nn.ConvTranspose2d(features * 2, features * 2, 2, 2)\n        self.up_conv1 = self._conv_block(features * 4, features)\n        self.up2 = nn.ConvTranspose2d(features, features, 2, 2)\n        self.up_conv2 = self._conv_block(features * 2, features)\n        \n        self.out = nn.Conv2d(features, in_channels, 1)\n        \n    def _conv_block(self, in_ch, out_ch):\n        return nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.GroupNorm(8, out_ch),\n            nn.GELU(),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.GroupNorm(8, out_ch),\n            nn.GELU(),\n        )\n        \n    def forward(self, x, t, y):\n        \"\"\"\n        y: 類別標籤，num_classes 表示無條件\n        \"\"\"\n        t_emb = self.time_mlp(t.float())\n        c_emb = self.class_embedding(y)\n        cond = torch.cat([t_emb, c_emb], dim=1)\n        \n        d1 = self.down1(x)\n        d2 = self.down2(self.pool(d1))\n        \n        m = self.mid(self.pool(d2))\n        cond_proj = self.cond_proj(cond)[:, :, None, None]\n        m = m + cond_proj\n        \n        u1 = self.up1(m)\n        u1 = torch.cat([u1, d2], dim=1)\n        u1 = self.up_conv1(u1)\n        \n        u2 = self.up2(u1)\n        u2 = torch.cat([u2, d1], dim=1)\n        u2 = self.up_conv2(u2)\n        \n        return self.out(u2)\n\ndef train_cfg_diffusion(model, scheduler, train_loader, epochs=15, lr=1e-4, \n                        p_uncond=0.1):\n    \"\"\"\n    訓練 CFG Diffusion\n    \n    Args:\n        p_uncond: 丟棄條件的機率（訓練無條件生成）\n    \"\"\"\n    \n    model = model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    num_classes = model.num_classes\n    \n    for epoch in range(epochs):\n        model.train()\n        losses = []\n        \n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        \n        for images, labels in pbar:\n            images = images.to(device)\n            labels = labels.to(device)\n            batch_size = images.size(0)\n            \n            # 隨機丟棄條件\n            # 用 num_classes 表示 \"無條件\"\n            drop_mask = torch.rand(batch_size) < p_uncond\n            labels = torch.where(\n                drop_mask.to(device),\n                torch.full_like(labels, num_classes),  # 無條件標籤\n                labels\n            )\n            \n            t = torch.randint(0, scheduler.num_timesteps, (batch_size,)).to(device)\n            \n            noise = torch.randn_like(images)\n            noisy_images, _ = scheduler.add_noise(images, t.cpu(), noise)\n            noisy_images = noisy_images.to(device)\n            \n            predicted_noise = model(noisy_images, t, labels)\n            \n            loss = F.mse_loss(predicted_noise, noise)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            losses.append(loss.item())\n            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n        print(f\"Epoch {epoch+1}, Avg Loss: {np.mean(losses):.4f}\")\n    \n    return model\n\n# 訓練\ncfg_model = CFGConditionalUNet(num_classes=10).to(device)\ncfg_model = train_cfg_diffusion(cfg_model, scheduler, train_loader, epochs=15)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 🎯 總結：GAN 與 Diffusion 完整知識框架\n\n### 📊 本模組重點回顧\n\n| 主題 | 核心概念 | 關鍵公式/技巧 |\n|------|----------|---------------|\n| **GAN 基礎** | 對抗訓練 (G vs D) | $\\min_G \\max_D V(D,G)$ |\n| **DCGAN** | 卷積架構 | 轉置卷積上採樣、步進卷積下採樣 |\n| **CGAN** | 條件生成 | 類別嵌入 + 拼接 |\n| **WGAN-GP** | 穩定訓練 | Wasserstein 距離 + 梯度懲罰 |\n| **DDPM** | 去噪擴散 | $L = \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2$ |\n| **DDIM** | 快速採樣 | 確定性跳步採樣 |\n| **CFG** | 引導強度 | $\\epsilon = \\epsilon_{uncond} + s(\\epsilon_{cond} - \\epsilon_{uncond})$ |\n\n### 🔄 GAN vs Diffusion 深度比較\n\n```\n┌────────────────┬────────────────────────┬────────────────────────┐\n│ 維度           │ GAN                    │ Diffusion              │\n├────────────────┼────────────────────────┼────────────────────────┤\n│ 訓練方式       │ 對抗遊戲（minimax）    │ 去噪預測（MSE）        │\n│ 訓練穩定性     │ 困難（模式崩潰）       │ 穩定                   │\n│ 生成速度       │ 快（1次前向）          │ 慢（多步迭代）         │\n│ 圖像品質       │ 好                     │ 非常好                 │\n│ 多樣性         │ 可能較低               │ 高                     │\n│ 條件控制       │ CGAN                   │ CFG                    │\n│ 理論基礎       │ 博弈論                 │ 得分匹配/機率論        │\n│ 代表應用       │ StyleGAN, BigGAN       │ Stable Diffusion, DALL-E │\n│ 採樣加速       │ N/A                    │ DDIM, DPM-Solver       │\n└────────────────┴────────────────────────┴────────────────────────┘\n```\n\n### 🛠️ GAN 訓練問題診斷\n\n```\n問題 1: 模式崩潰 (Mode Collapse)\n  症狀：生成的樣本都很相似\n  解決：\n    - WGAN-GP（梯度懲罰）\n    - Minibatch Discrimination\n    - Feature Matching Loss\n    - Unrolled GAN\n\n問題 2: 訓練不穩定 / 梯度消失\n  症狀：D 太強，G 學不動\n  解決：\n    - Label Smoothing\n    - TTUR（D 學習率 > G）\n    - Spectral Normalization\n    - 減少 D 訓練次數\n\n問題 3: 生成品質差\n  症狀：圖像模糊、有偽影\n  解決：\n    - 更深的網路\n    - Progressive Growing\n    - 更大 batch size\n    - Self-Attention\n```\n\n### 🚀 Diffusion 模型演進路線\n\n```\nDDPM (2020)\n  │  ↓ 加速採樣\nDDIM (2020)\n  │  ↓ 條件控制\nClassifier Guidance (2021)\n  │  ↓ 無需分類器\nClassifier-Free Guidance (2022)\n  │  ↓ 文字條件\nLatent Diffusion / Stable Diffusion (2022)\n  │  ↓ 更快採樣\nDPM-Solver, Consistency Models (2023)\n  │  ↓ 視頻生成\nSora, SVD (2024)\n```\n\n### 📝 實戰 Checklist\n\n**GAN 訓練前**：\n- [ ] 數據預處理：歸一化到 [-1, 1]\n- [ ] 使用 LeakyReLU（不用 ReLU）\n- [ ] G 輸出用 Tanh，D 輸出用 Sigmoid（BCE）或無（WGAN）\n- [ ] 使用 BatchNorm（G）但不在 D 的第一層用\n\n**GAN 訓練中**：\n- [ ] 監控 D 和 G 的 loss 比例\n- [ ] 定期生成樣本檢查品質\n- [ ] 檢測模式崩潰（樣本多樣性）\n\n**Diffusion 訓練前**：\n- [ ] 選擇合適的 noise schedule（linear, cosine）\n- [ ] U-Net 結構設計（足夠深度）\n- [ ] 時間編碼選擇（正弦/學習）\n\n**Diffusion 推理**：\n- [ ] 選擇採樣器（DDPM, DDIM, DPM-Solver）\n- [ ] 設定 CFG scale（通常 5-15）\n- [ ] 調整採樣步數（quality vs speed）\n\n### 🔗 延伸學習資源\n\n| 資源 | 說明 |\n|------|------|\n| [GAN 原論文](https://arxiv.org/abs/1406.2661) | Goodfellow et al., 2014 |\n| [DDPM 論文](https://arxiv.org/abs/2006.11239) | Ho et al., 2020 |\n| [DDIM 論文](https://arxiv.org/abs/2010.02502) | Song et al., 2020 |\n| [CFG 論文](https://arxiv.org/abs/2207.12598) | Ho & Salimans, 2022 |\n| [Stable Diffusion](https://arxiv.org/abs/2112.10752) | Rombach et al., 2022 |\n| [Lilian Weng's Blog](https://lilianweng.github.io/) | 深度學習科普 |\n\n### ⏭️ 下一步\n\n在下一個模組中，我們將學習 **圖神經網路 (GNN)**：\n- 處理圖結構資料（社交網路、分子、知識圖譜）\n- Graph Convolutional Networks (GCN)\n- Graph Attention Networks (GAT)\n- 消息傳遞神經網路 (MPNN)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 總結\n",
    "\n",
    "### 本模組重點\n",
    "\n",
    "1. **GAN 基礎**\n",
    "   - 對抗訓練：生成器 vs 判別器\n",
    "   - 最小-最大遊戲目標\n",
    "   - DCGAN 使用卷積提升品質\n",
    "\n",
    "2. **GAN 變體**\n",
    "   - CGAN：條件生成\n",
    "   - WGAN-GP：更穩定的訓練\n",
    "\n",
    "3. **Diffusion 模型**\n",
    "   - 前向：逐步加噪\n",
    "   - 反向：學習去噪\n",
    "   - U-Net 預測噪音\n",
    "\n",
    "### GAN vs Diffusion 比較\n",
    "\n",
    "```\n",
    "┌────────────┬──────────────────┬──────────────────┐\n",
    "│ 特性       │ GAN              │ Diffusion        │\n",
    "├────────────┼──────────────────┼──────────────────┤\n",
    "│ 訓練穩定性 │ 較難（模式崩潰） │ 較穩定           │\n",
    "│ 生成速度   │ 快（一次前向）   │ 慢（需多步迭代） │\n",
    "│ 圖像品質   │ 好               │ 非常好           │\n",
    "│ 多樣性     │ 可能較低         │ 較高             │\n",
    "│ 理論基礎   │ 博弈論           │ 機率論           │\n",
    "└────────────┴──────────────────┴──────────────────┘\n",
    "```\n",
    "\n",
    "### 下一步\n",
    "\n",
    "在下一個模組中，我們將學習 **圖神經網路 (GNN)**，處理圖結構資料。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}