{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Model Customization\n",
    "\n",
    "**對應課程**: 李宏毅 2025 Fall GenAI-ML HW9 - Diffusion Model and Its Customization\n",
    "\n",
    "本 notebook 介紹如何客製化預訓練的 Diffusion Model，包括：\n",
    "- **Textual Inversion**: 學習新概念的文字嵌入\n",
    "- **DreamBooth**: 微調整個模型學習特定主題\n",
    "- **LoRA for Diffusion**: 低秩適應於擴散模型\n",
    "- **ControlNet**: 增加空間控制條件\n",
    "\n",
    "```\n",
    "Diffusion 客製化方法比較：\n",
    "\n",
    "方法              訓練參數    VRAM需求    訓練時間    效果\n",
    "─────────────────────────────────────────────────────────\n",
    "Textual Inversion  ~768      ~8GB        ~1hr       學習概念\n",
    "DreamBooth         ~1B       ~24GB       ~30min     高品質\n",
    "LoRA               ~4M       ~12GB       ~1hr       靈活高效\n",
    "ControlNet         ~361M     ~16GB       ~數天      空間控制\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Textual Inversion\n",
    "\n",
    "Textual Inversion 透過學習新的文字嵌入來表示新概念，不修改模型參數。\n",
    "\n",
    "```\n",
    "Textual Inversion 流程：\n",
    "\n",
    "訓練階段：\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│  \"A photo of S*\"     S* = 可學習的嵌入向量          │\n",
    "│         │                                           │\n",
    "│         ▼                                           │\n",
    "│  ┌─────────────┐                                    │\n",
    "│  │ Text Encoder│ ──► 文字條件 c                     │\n",
    "│  └─────────────┘                                    │\n",
    "│         │                                           │\n",
    "│         ▼                                           │\n",
    "│  ┌─────────────┐     ┌──────────┐                   │\n",
    "│  │    UNet     │ ◄── │ 目標圖片 │ (3-5張)           │\n",
    "│  │  (frozen)   │     └──────────┘                   │\n",
    "│  └─────────────┘                                    │\n",
    "│         │                                           │\n",
    "│         ▼                                           │\n",
    "│    更新 S* 最小化重建損失                            │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "\n",
    "推理階段：\n",
    "\"A S* in the style of Van Gogh\" → 生成該概念的新圖片\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextualInversionEmbedding(nn.Module):\n",
    "    \"\"\"Textual Inversion：學習新概念的嵌入向量\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_vectors: int = 1,  # 使用多少個 token 表示概念\n",
    "        embedding_dim: int = 768,  # CLIP text encoder 維度\n",
    "        initializer_token: str = \"object\"  # 初始化用的 token\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_vectors = num_vectors\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 可學習的嵌入向量\n",
    "        # 通常初始化為相似概念的嵌入\n",
    "        self.learned_embedding = nn.Parameter(\n",
    "            torch.randn(num_vectors, embedding_dim) * 0.02\n",
    "        )\n",
    "        \n",
    "    def forward(self) -> torch.Tensor:\n",
    "        \"\"\"返回學習到的嵌入\"\"\"\n",
    "        return self.learned_embedding\n",
    "    \n",
    "    def get_embedding_for_prompt(self, placeholder_positions: List[int], \n",
    "                                  text_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        將學習到的嵌入插入到文字嵌入中\n",
    "        \n",
    "        Args:\n",
    "            placeholder_positions: placeholder token 的位置\n",
    "            text_embeddings: 原始文字嵌入 [batch, seq_len, dim]\n",
    "        \"\"\"\n",
    "        batch_size = text_embeddings.shape[0]\n",
    "        modified_embeddings = text_embeddings.clone()\n",
    "        \n",
    "        for i, pos in enumerate(placeholder_positions):\n",
    "            if i < self.num_vectors:\n",
    "                # 替換 placeholder 位置的嵌入\n",
    "                modified_embeddings[:, pos, :] = self.learned_embedding[i].unsqueeze(0)\n",
    "                \n",
    "        return modified_embeddings\n",
    "\n",
    "\n",
    "# 示範\n",
    "ti_embedding = TextualInversionEmbedding(num_vectors=2, embedding_dim=768)\n",
    "print(f\"學習參數數量: {sum(p.numel() for p in ti_embedding.parameters())}\")\n",
    "print(f\"嵌入形狀: {ti_embedding().shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextualInversionTrainer:\n",
    "    \"\"\"Textual Inversion 訓練器\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding: TextualInversionEmbedding,\n",
    "        learning_rate: float = 5e-4,\n",
    "        max_grad_norm: float = 1.0\n",
    "    ):\n",
    "        self.embedding = embedding\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            embedding.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=1e-2\n",
    "        )\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "    def training_step(\n",
    "        self,\n",
    "        noise_pred: torch.Tensor,\n",
    "        noise_target: torch.Tensor\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        單步訓練\n",
    "        \n",
    "        Args:\n",
    "            noise_pred: UNet 預測的噪聲\n",
    "            noise_target: 實際添加的噪聲\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # MSE 損失\n",
    "        loss = F.mse_loss(noise_pred, noise_target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            self.embedding.parameters(), \n",
    "            self.max_grad_norm\n",
    "        )\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "print(\"Textual Inversion 訓練配置：\")\n",
    "print(\"- 典型學習率: 5e-4 到 1e-3\")\n",
    "print(\"- 訓練步數: 3000-5000 步\")\n",
    "print(\"- 訓練圖片: 3-5 張高品質圖片\")\n",
    "print(\"- 記憶體需求: ~8GB VRAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: DreamBooth\n",
    "\n",
    "DreamBooth 透過微調整個 UNet 來學習特定主題，並使用 prior preservation 防止過擬合。\n",
    "\n",
    "```\n",
    "DreamBooth 損失函數：\n",
    "\n",
    "L_total = L_reconstruction + λ · L_prior\n",
    "\n",
    "L_reconstruction: 在目標圖片上的重建損失\n",
    "L_prior: 在類別圖片上的先驗保留損失\n",
    "\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│                    DreamBooth 流程                       │\n",
    "├─────────────────────────────────────────────────────────┤\n",
    "│                                                          │\n",
    "│  輸入: 3-5 張 [V] dog 的圖片                             │\n",
    "│                                                          │\n",
    "│  Prompt 格式:                                            │\n",
    "│  - 主題 prompt: \"a [V] dog\"                             │\n",
    "│  - 類別 prompt: \"a dog\" (用於 prior preservation)       │\n",
    "│                                                          │\n",
    "│  訓練:                                                   │\n",
    "│  ┌────────────┐   ┌────────────┐                        │\n",
    "│  │ 目標圖片   │   │ 生成類別圖 │                        │\n",
    "│  │ + 噪聲     │   │ + 噪聲     │                        │\n",
    "│  └─────┬──────┘   └─────┬──────┘                        │\n",
    "│        │                │                               │\n",
    "│        ▼                ▼                               │\n",
    "│     L_recon    +     L_prior                            │\n",
    "│        └────────┬───────┘                               │\n",
    "│                 ▼                                        │\n",
    "│            更新 UNet                                     │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DreamBoothConfig:\n",
    "    \"\"\"DreamBooth 訓練配置\"\"\"\n",
    "    # 識別符\n",
    "    instance_prompt: str = \"a photo of sks dog\"  # [V] = \"sks\"\n",
    "    class_prompt: str = \"a photo of dog\"\n",
    "    \n",
    "    # 訓練參數\n",
    "    learning_rate: float = 5e-6  # 很小的學習率\n",
    "    max_train_steps: int = 800\n",
    "    train_batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    \n",
    "    # Prior preservation\n",
    "    with_prior_preservation: bool = True\n",
    "    prior_loss_weight: float = 1.0\n",
    "    num_class_images: int = 200  # 生成的類別圖片數量\n",
    "    \n",
    "    # 其他\n",
    "    mixed_precision: str = \"fp16\"\n",
    "    gradient_checkpointing: bool = True  # 節省記憶體\n",
    "\n",
    "\n",
    "class DreamBoothLoss(nn.Module):\n",
    "    \"\"\"DreamBooth 損失函數\"\"\"\n",
    "    \n",
    "    def __init__(self, prior_loss_weight: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.prior_loss_weight = prior_loss_weight\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        model_pred_instance: torch.Tensor,\n",
    "        target_instance: torch.Tensor,\n",
    "        model_pred_class: Optional[torch.Tensor] = None,\n",
    "        target_class: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        計算 DreamBooth 損失\n",
    "        \n",
    "        Args:\n",
    "            model_pred_instance: 在實例圖片上的預測\n",
    "            target_instance: 實例圖片的目標噪聲\n",
    "            model_pred_class: 在類別圖片上的預測 (prior preservation)\n",
    "            target_class: 類別圖片的目標噪聲\n",
    "        \"\"\"\n",
    "        # 實例重建損失\n",
    "        instance_loss = F.mse_loss(model_pred_instance, target_instance)\n",
    "        \n",
    "        metrics = {'instance_loss': instance_loss.item()}\n",
    "        \n",
    "        # Prior preservation 損失\n",
    "        if model_pred_class is not None and target_class is not None:\n",
    "            prior_loss = F.mse_loss(model_pred_class, target_class)\n",
    "            total_loss = instance_loss + self.prior_loss_weight * prior_loss\n",
    "            metrics['prior_loss'] = prior_loss.item()\n",
    "        else:\n",
    "            total_loss = instance_loss\n",
    "            \n",
    "        metrics['total_loss'] = total_loss.item()\n",
    "        \n",
    "        return total_loss, metrics\n",
    "\n",
    "\n",
    "# 示範\n",
    "config = DreamBoothConfig()\n",
    "print(f\"DreamBooth 配置:\")\n",
    "print(f\"  Instance prompt: {config.instance_prompt}\")\n",
    "print(f\"  Class prompt: {config.class_prompt}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")\n",
    "print(f\"  Prior preservation: {config.with_prior_preservation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dreambooth_concept():\n",
    "    \"\"\"視覺化 DreamBooth 概念\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    \n",
    "    # 模擬訓練圖片\n",
    "    np.random.seed(42)\n",
    "    for i, ax in enumerate(axes[0]):\n",
    "        # 創建假的狗圖片（用顏色塊代表）\n",
    "        img = np.ones((64, 64, 3)) * 0.8\n",
    "        # 添加一些特徵\n",
    "        img[20:45, 20:45] = [0.6, 0.4, 0.2]  # 棕色主體\n",
    "        img[25:30, 25:30] = [0.1, 0.1, 0.1]  # 眼睛\n",
    "        img[25:30, 35:40] = [0.1, 0.1, 0.1]  # 眼睛\n",
    "        img[32:35, 30:35] = [0.2, 0.1, 0.1]  # 鼻子\n",
    "        # 添加一些變化\n",
    "        img += np.random.randn(64, 64, 3) * 0.05\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"訓練圖片 {i+1}\\n'sks dog'\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # 模擬生成圖片\n",
    "    prompts = [\n",
    "        \"sks dog on beach\",\n",
    "        \"sks dog painting\",\n",
    "        \"sks dog in snow\"\n",
    "    ]\n",
    "    colors = [[0.9, 0.8, 0.6], [0.7, 0.5, 0.8], [0.95, 0.95, 1.0]]\n",
    "    \n",
    "    for i, (ax, prompt, bg_color) in enumerate(zip(axes[1], prompts, colors)):\n",
    "        img = np.ones((64, 64, 3)) * np.array(bg_color)\n",
    "        # 保持相同的狗特徵\n",
    "        img[20:45, 20:45] = [0.6, 0.4, 0.2]\n",
    "        img[25:30, 25:30] = [0.1, 0.1, 0.1]\n",
    "        img[25:30, 35:40] = [0.1, 0.1, 0.1]\n",
    "        img[32:35, 30:35] = [0.2, 0.1, 0.1]\n",
    "        img += np.random.randn(64, 64, 3) * 0.03\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"生成結果\\n'{prompt}'\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(\"DreamBooth: 用少量圖片學習特定主題\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_dreambooth_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: LoRA for Stable Diffusion\n",
    "\n",
    "LoRA (Low-Rank Adaptation) 可以高效地客製化 Diffusion Model，只訓練低秩分解的權重。\n",
    "\n",
    "```\n",
    "LoRA 在 Stable Diffusion 中的應用：\n",
    "\n",
    "原始權重 W ∈ R^(d×k)\n",
    "LoRA: W' = W + BA，其中 B ∈ R^(d×r), A ∈ R^(r×k), r << min(d,k)\n",
    "\n",
    "應用位置：\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│                    UNet 架構                         │\n",
    "├─────────────────────────────────────────────────────┤\n",
    "│                                                      │\n",
    "│  Cross-Attention 層 (最重要):                       │\n",
    "│  ├─ to_q: Query 投影 ← LoRA                        │\n",
    "│  ├─ to_k: Key 投影   ← LoRA                        │\n",
    "│  ├─ to_v: Value 投影 ← LoRA                        │\n",
    "│  └─ to_out: 輸出投影 ← LoRA                        │\n",
    "│                                                      │\n",
    "│  Self-Attention 層 (可選):                          │\n",
    "│  └─ 同上結構                                        │\n",
    "│                                                      │\n",
    "│  Rank 選擇:                                          │\n",
    "│  ├─ r=4:  最小，適合簡單風格                        │\n",
    "│  ├─ r=8:  平衡，常用選擇                            │\n",
    "│  ├─ r=16: 較強表達力                                │\n",
    "│  └─ r=64: 接近全微調                                │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"LoRA 層：用於 Diffusion Model\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        original_layer: nn.Linear,\n",
    "        rank: int = 4,\n",
    "        alpha: float = 1.0,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        in_features = original_layer.in_features\n",
    "        out_features = original_layer.out_features\n",
    "        \n",
    "        # 凍結原始層\n",
    "        for param in original_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # LoRA 矩陣\n",
    "        self.lora_A = nn.Linear(in_features, rank, bias=False)\n",
    "        self.lora_B = nn.Linear(rank, out_features, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
    "        \n",
    "        # 初始化\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 原始輸出 + LoRA 調整\n",
    "        original_output = self.original_layer(x)\n",
    "        lora_output = self.lora_B(self.lora_A(self.dropout(x)))\n",
    "        return original_output + self.scaling * lora_output\n",
    "    \n",
    "    def merge_weights(self):\n",
    "        \"\"\"合併 LoRA 權重到原始層（推理時使用）\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # W' = W + scaling * B @ A\n",
    "            delta_w = self.scaling * (self.lora_B.weight @ self.lora_A.weight)\n",
    "            self.original_layer.weight.add_(delta_w)\n",
    "            \n",
    "    def get_lora_state_dict(self) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"獲取 LoRA 權重用於儲存\"\"\"\n",
    "        return {\n",
    "            'lora_A': self.lora_A.weight.data,\n",
    "            'lora_B': self.lora_B.weight.data,\n",
    "            'alpha': self.alpha,\n",
    "            'rank': self.rank\n",
    "        }\n",
    "\n",
    "\n",
    "# 示範\n",
    "original = nn.Linear(768, 768)\n",
    "lora_layer = LoRALayer(original, rank=8, alpha=16)\n",
    "\n",
    "print(f\"原始層參數: {768 * 768:,} = {768 * 768 * 4 / 1024 / 1024:.2f} MB\")\n",
    "lora_params = sum(p.numel() for p in [lora_layer.lora_A.weight, lora_layer.lora_B.weight])\n",
    "print(f\"LoRA 參數: {lora_params:,} = {lora_params * 4 / 1024:.2f} KB\")\n",
    "print(f\"參數減少: {(1 - lora_params / (768 * 768)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAInjector:\n",
    "    \"\"\"將 LoRA 注入到模型的指定層\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        target_modules: List[str] = ['to_q', 'to_k', 'to_v', 'to_out'],\n",
    "        rank: int = 8,\n",
    "        alpha: float = 16,\n",
    "        dropout: float = 0.0\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.target_modules = target_modules\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.dropout = dropout\n",
    "        self.lora_layers = {}\n",
    "        \n",
    "    def inject(self) -> int:\n",
    "        \"\"\"\n",
    "        注入 LoRA 層\n",
    "        \n",
    "        Returns:\n",
    "            注入的層數\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        \n",
    "        for name, module in self.model.named_modules():\n",
    "            # 檢查是否是目標模組\n",
    "            if any(target in name for target in self.target_modules):\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    # 創建 LoRA 層\n",
    "                    lora_layer = LoRALayer(\n",
    "                        module, \n",
    "                        rank=self.rank,\n",
    "                        alpha=self.alpha,\n",
    "                        dropout=self.dropout\n",
    "                    )\n",
    "                    \n",
    "                    # 替換原始層\n",
    "                    parent_name = '.'.join(name.split('.')[:-1])\n",
    "                    child_name = name.split('.')[-1]\n",
    "                    \n",
    "                    if parent_name:\n",
    "                        parent = dict(self.model.named_modules())[parent_name]\n",
    "                        setattr(parent, child_name, lora_layer)\n",
    "                    \n",
    "                    self.lora_layers[name] = lora_layer\n",
    "                    count += 1\n",
    "                    \n",
    "        return count\n",
    "    \n",
    "    def get_trainable_params(self) -> List[nn.Parameter]:\n",
    "        \"\"\"獲取所有可訓練的 LoRA 參數\"\"\"\n",
    "        params = []\n",
    "        for lora_layer in self.lora_layers.values():\n",
    "            params.extend([\n",
    "                lora_layer.lora_A.weight,\n",
    "                lora_layer.lora_B.weight\n",
    "            ])\n",
    "        return params\n",
    "    \n",
    "    def save_lora(self, path: str):\n",
    "        \"\"\"儲存 LoRA 權重\"\"\"\n",
    "        state_dict = {}\n",
    "        for name, layer in self.lora_layers.items():\n",
    "            state_dict[name] = layer.get_lora_state_dict()\n",
    "        torch.save(state_dict, path)\n",
    "        print(f\"LoRA 權重已儲存至 {path}\")\n",
    "\n",
    "\n",
    "# 示範用的簡單模型\n",
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super().__init__()\n",
    "        self.to_q = nn.Linear(dim, dim)\n",
    "        self.to_k = nn.Linear(dim, dim)\n",
    "        self.to_v = nn.Linear(dim, dim)\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "        attn = F.softmax(q @ k.transpose(-2, -1) / math.sqrt(q.shape[-1]), dim=-1)\n",
    "        return self.to_out(attn @ v)\n",
    "\n",
    "\n",
    "# 測試注入\n",
    "model = SimpleAttention()\n",
    "injector = LoRAInjector(model, rank=8, alpha=16)\n",
    "num_injected = injector.inject()\n",
    "\n",
    "print(f\"注入了 {num_injected} 個 LoRA 層\")\n",
    "print(f\"可訓練參數: {len(injector.get_trainable_params())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: ControlNet\n",
    "\n",
    "ControlNet 為 Diffusion Model 添加額外的空間控制條件（邊緣、深度、姿態等）。\n",
    "\n",
    "```\n",
    "ControlNet 架構：\n",
    "\n",
    "                     ┌──────────────────┐\n",
    "                     │   Control Image  │\n",
    "                     │  (edge/depth/...)│\n",
    "                     └────────┬─────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "                     ┌──────────────────┐\n",
    "                     │  Zero Convolution │ (訓練開始時輸出為 0)\n",
    "                     └────────┬─────────┘\n",
    "                              │\n",
    "┌─────────────────────────────┼─────────────────────────────┐\n",
    "│                             │                             │\n",
    "│  ┌─────────────┐            │            ┌─────────────┐  │\n",
    "│  │   Original  │            │            │  ControlNet │  │\n",
    "│  │    UNet     │◄───────────┴───────────►│   (Copy)    │  │\n",
    "│  │  (Locked)   │          加法            │(Trainable) │  │\n",
    "│  └─────────────┘                          └─────────────┘  │\n",
    "│                                                            │\n",
    "└────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Zero Convolution 的作用：\n",
    "- 訓練開始時，ControlNet 輸出為 0\n",
    "- 模型行為與原始 SD 完全相同\n",
    "- 隨著訓練進行，逐漸學習控制信號\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroConvolution(nn.Module):\n",
    "    \"\"\"Zero Convolution：初始化為零的卷積層\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "        # 初始化為零\n",
    "        nn.init.zeros_(self.conv.weight)\n",
    "        nn.init.zeros_(self.conv.bias)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class ControlNetBlock(nn.Module):\n",
    "    \"\"\"ControlNet 的基本區塊\"\"\"\n",
    "    \n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 控制編碼器\n",
    "        self.control_encoder = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        # Zero convolution 輸出\n",
    "        self.zero_conv = ZeroConvolution(channels, channels)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, control: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: UNet 特徵\n",
    "            control: 控制信號\n",
    "        \"\"\"\n",
    "        # 編碼控制信號\n",
    "        control_features = self.control_encoder(control)\n",
    "        \n",
    "        # 通過 zero conv 後加到原始特徵\n",
    "        return x + self.zero_conv(control_features)\n",
    "\n",
    "\n",
    "class SimpleControlNet(nn.Module):\n",
    "    \"\"\"簡化版 ControlNet 示意\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 3,  # 控制圖像通道\n",
    "        base_channels: int = 64,\n",
    "        num_blocks: int = 4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 控制圖像編碼器\n",
    "        self.input_hint_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, base_channels, 3, padding=1),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(base_channels, base_channels, 3, padding=1),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        # Zero convolution for input\n",
    "        self.input_zero_conv = ZeroConvolution(base_channels, base_channels)\n",
    "        \n",
    "        # ControlNet blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ControlNetBlock(base_channels * (2 ** min(i, 3)))\n",
    "            for i in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output zero convolutions\n",
    "        self.output_zero_convs = nn.ModuleList([\n",
    "            ZeroConvolution(base_channels * (2 ** min(i, 3)), \n",
    "                          base_channels * (2 ** min(i, 3)))\n",
    "            for i in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        control_image: torch.Tensor,\n",
    "        unet_features: List[torch.Tensor]\n",
    "    ) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            control_image: 控制圖像 [B, 3, H, W]\n",
    "            unet_features: UNet 各層的特徵\n",
    "            \n",
    "        Returns:\n",
    "            要加到 UNet 的控制信號\n",
    "        \"\"\"\n",
    "        # 編碼控制圖像\n",
    "        hint = self.input_hint_block(control_image)\n",
    "        hint = self.input_zero_conv(hint)\n",
    "        \n",
    "        # 對每個 UNet 層產生控制信號\n",
    "        control_outputs = []\n",
    "        \n",
    "        for block, zero_conv, unet_feat in zip(\n",
    "            self.blocks, self.output_zero_convs, unet_features\n",
    "        ):\n",
    "            # 調整 hint 大小以匹配 UNet 特徵\n",
    "            if hint.shape[-2:] != unet_feat.shape[-2:]:\n",
    "                hint = F.interpolate(hint, size=unet_feat.shape[-2:], mode='bilinear')\n",
    "                # 調整通道數\n",
    "                if hint.shape[1] != unet_feat.shape[1]:\n",
    "                    hint = F.conv2d(\n",
    "                        hint, \n",
    "                        torch.randn(unet_feat.shape[1], hint.shape[1], 1, 1, device=hint.device) * 0.02\n",
    "                    )\n",
    "            \n",
    "            control_out = zero_conv(hint)\n",
    "            control_outputs.append(control_out)\n",
    "            \n",
    "        return control_outputs\n",
    "\n",
    "\n",
    "# 測試\n",
    "controlnet = SimpleControlNet()\n",
    "print(f\"ControlNet 參數量: {sum(p.numel() for p in controlnet.parameters()):,}\")\n",
    "\n",
    "# 驗證 zero convolution 初始輸出為 0\n",
    "test_input = torch.randn(1, 64, 32, 32)\n",
    "zero_conv = ZeroConvolution(64, 64)\n",
    "print(f\"Zero conv 輸出範圍: [{zero_conv(test_input).min():.6f}, {zero_conv(test_input).max():.6f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_control_types():\n",
    "    \"\"\"視覺化不同類型的控制條件\"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    size = 64\n",
    "    \n",
    "    # 第一行：控制圖像\n",
    "    control_types = ['Canny Edge', 'Depth', 'Pose', 'Segmentation']\n",
    "    \n",
    "    # Canny Edge\n",
    "    edge = np.zeros((size, size))\n",
    "    edge[20:25, 10:50] = 1\n",
    "    edge[20:50, 10:15] = 1\n",
    "    edge[20:50, 45:50] = 1\n",
    "    edge[45:50, 10:50] = 1\n",
    "    axes[0, 0].imshow(edge, cmap='gray')\n",
    "    axes[0, 0].set_title('Canny Edge')\n",
    "    \n",
    "    # Depth\n",
    "    depth = np.zeros((size, size))\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            depth[i, j] = 1 - abs(i - size//2) / size - abs(j - size//2) / size\n",
    "    axes[0, 1].imshow(depth, cmap='viridis')\n",
    "    axes[0, 1].set_title('Depth Map')\n",
    "    \n",
    "    # Pose (簡化的骨架)\n",
    "    pose = np.zeros((size, size, 3))\n",
    "    # 頭\n",
    "    pose[10:15, 30:35] = [1, 0, 0]\n",
    "    # 身體\n",
    "    pose[15:35, 31:34] = [0, 1, 0]\n",
    "    # 手臂\n",
    "    pose[18:22, 20:32] = [0, 0, 1]\n",
    "    pose[18:22, 33:45] = [0, 0, 1]\n",
    "    # 腿\n",
    "    pose[35:55, 28:31] = [1, 1, 0]\n",
    "    pose[35:55, 34:37] = [1, 1, 0]\n",
    "    axes[0, 2].imshow(pose)\n",
    "    axes[0, 2].set_title('Pose')\n",
    "    \n",
    "    # Segmentation\n",
    "    seg = np.zeros((size, size, 3))\n",
    "    seg[:20, :] = [0.5, 0.8, 1.0]  # 天空\n",
    "    seg[20:40, :] = [0.2, 0.6, 0.2]  # 樹\n",
    "    seg[40:, :] = [0.6, 0.4, 0.2]  # 地面\n",
    "    axes[0, 3].imshow(seg)\n",
    "    axes[0, 3].set_title('Segmentation')\n",
    "    \n",
    "    # 第二行：對應的生成結果（模擬）\n",
    "    results = [\n",
    "        'Generated\\nfrom Edge',\n",
    "        'Generated\\nfrom Depth',\n",
    "        'Generated\\nfrom Pose',\n",
    "        'Generated\\nfrom Seg'\n",
    "    ]\n",
    "    \n",
    "    for i, (ax, title) in enumerate(zip(axes[1], results)):\n",
    "        # 模擬生成結果\n",
    "        img = np.random.rand(size, size, 3) * 0.3 + 0.4\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    for ax in axes.flat:\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.suptitle('ControlNet: 不同控制條件類型', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_control_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: 實際使用範例（使用 diffusers）\n",
    "\n",
    "以下是使用 Hugging Face diffusers 庫進行客製化的程式碼範例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textual Inversion 訓練範例（使用 diffusers）\n",
    "textual_inversion_code = '''\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.loaders import TextualInversionLoaderMixin\n",
    "\n",
    "# 載入基礎模型\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 訓練 Textual Inversion\n",
    "# 使用 accelerate 命令：\n",
    "# accelerate launch diffusers/examples/textual_inversion/textual_inversion.py \\\\\n",
    "#   --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\\\n",
    "#   --train_data_dir=\"./my_concept\" \\\\\n",
    "#   --learnable_property=\"object\" \\\\\n",
    "#   --placeholder_token=\"<my-concept>\" \\\\\n",
    "#   --initializer_token=\"dog\" \\\\\n",
    "#   --resolution=512 \\\\\n",
    "#   --train_batch_size=1 \\\\\n",
    "#   --gradient_accumulation_steps=4 \\\\\n",
    "#   --max_train_steps=3000 \\\\\n",
    "#   --learning_rate=5.0e-04 \\\\\n",
    "#   --output_dir=\"./textual_inversion_output\"\n",
    "\n",
    "# 載入訓練好的嵌入\n",
    "pipe.load_textual_inversion(\"./textual_inversion_output/learned_embeds.bin\")\n",
    "\n",
    "# 使用新概念生成\n",
    "image = pipe(\"A <my-concept> in a forest\").images[0]\n",
    "'''\n",
    "print(\"=== Textual Inversion 使用範例 ===\")\n",
    "print(textual_inversion_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DreamBooth 訓練範例\n",
    "dreambooth_code = '''\n",
    "# DreamBooth 訓練命令（使用 diffusers）\n",
    "# accelerate launch diffusers/examples/dreambooth/train_dreambooth.py \\\\\n",
    "#   --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\\\n",
    "#   --instance_data_dir=\"./my_dog\" \\\\\n",
    "#   --output_dir=\"./dreambooth_output\" \\\\\n",
    "#   --instance_prompt=\"a photo of sks dog\" \\\\\n",
    "#   --resolution=512 \\\\\n",
    "#   --train_batch_size=1 \\\\\n",
    "#   --gradient_accumulation_steps=1 \\\\\n",
    "#   --learning_rate=5e-6 \\\\\n",
    "#   --lr_scheduler=\"constant\" \\\\\n",
    "#   --lr_warmup_steps=0 \\\\\n",
    "#   --max_train_steps=800 \\\\\n",
    "#   --with_prior_preservation \\\\\n",
    "#   --prior_loss_weight=1.0 \\\\\n",
    "#   --class_data_dir=\"./class_dog\" \\\\\n",
    "#   --class_prompt=\"a photo of dog\" \\\\\n",
    "#   --num_class_images=200\n",
    "\n",
    "# 使用訓練好的模型\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"./dreambooth_output\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "image = pipe(\"a photo of sks dog on the beach\").images[0]\n",
    "'''\n",
    "print(\"=== DreamBooth 使用範例 ===\")\n",
    "print(dreambooth_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 訓練與使用範例\n",
    "lora_code = '''\n",
    "# LoRA 訓練（使用 diffusers + PEFT）\n",
    "# accelerate launch diffusers/examples/dreambooth/train_dreambooth_lora.py \\\\\n",
    "#   --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\\\n",
    "#   --instance_data_dir=\"./my_style\" \\\\\n",
    "#   --output_dir=\"./lora_output\" \\\\\n",
    "#   --instance_prompt=\"artwork in style of sks\" \\\\\n",
    "#   --resolution=512 \\\\\n",
    "#   --train_batch_size=1 \\\\\n",
    "#   --gradient_accumulation_steps=4 \\\\\n",
    "#   --learning_rate=1e-4 \\\\\n",
    "#   --max_train_steps=1000 \\\\\n",
    "#   --rank=8\n",
    "\n",
    "# 使用 LoRA\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 載入 LoRA 權重\n",
    "pipe.load_lora_weights(\"./lora_output\")\n",
    "\n",
    "# 調整 LoRA 強度\n",
    "pipe.fuse_lora(lora_scale=0.8)  # 0-1 之間\n",
    "\n",
    "image = pipe(\"a cat in style of sks\").images[0]\n",
    "\n",
    "# 移除 LoRA（恢復原始模型）\n",
    "pipe.unfuse_lora()\n",
    "pipe.unload_lora_weights()\n",
    "'''\n",
    "print(\"=== LoRA 使用範例 ===\")\n",
    "print(lora_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ControlNet 使用範例\n",
    "controlnet_code = '''\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "from diffusers.utils import load_image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 載入 ControlNet（Canny 邊緣檢測版本）\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 準備控制圖像\n",
    "image = load_image(\"your_image.png\")\n",
    "image = np.array(image)\n",
    "\n",
    "# Canny 邊緣檢測\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "canny_image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "canny_image = canny_image[:, :, None]\n",
    "canny_image = np.concatenate([canny_image, canny_image, canny_image], axis=2)\n",
    "\n",
    "# 生成\n",
    "output = pipe(\n",
    "    \"a beautiful landscape\",\n",
    "    image=canny_image,\n",
    "    num_inference_steps=30,\n",
    ").images[0]\n",
    "\n",
    "# 多個 ControlNet 組合\n",
    "from diffusers import MultiControlNetModel\n",
    "\n",
    "controlnet_canny = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16\n",
    ")\n",
    "controlnet_depth = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-depth\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "multi_controlnet = MultiControlNetModel([controlnet_canny, controlnet_depth])\n",
    "\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    controlnet=multi_controlnet,\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "output = pipe(\n",
    "    \"a beautiful scene\",\n",
    "    image=[canny_image, depth_image],\n",
    "    controlnet_conditioning_scale=[0.5, 0.5],  # 調整各自的影響力\n",
    ").images[0]\n",
    "'''\n",
    "print(\"=== ControlNet 使用範例 ===\")\n",
    "print(controlnet_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: 方法比較與選擇指南"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comparison_table():\n",
    "    \"\"\"印出客製化方法比較表\"\"\"\n",
    "    print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════════╗\n",
    "║                     Diffusion Model 客製化方法比較                               ║\n",
    "╠═══════════════════╦═══════════════╦═══════════════╦═══════════════╦══════════════╣\n",
    "║       方法        ║ Textual Inv.  ║  DreamBooth   ║     LoRA      ║  ControlNet  ║\n",
    "╠═══════════════════╬═══════════════╬═══════════════╬═══════════════╬══════════════╣\n",
    "║ 訓練參數          ║    ~768       ║    ~1B        ║    ~4M        ║   ~361M      ║\n",
    "║ VRAM 需求         ║    ~8GB       ║   ~24GB       ║   ~12GB       ║   ~16GB      ║\n",
    "║ 訓練時間          ║   1-2小時     ║   30分鐘      ║   1小時       ║   數天       ║\n",
    "║ 儲存大小          ║    ~4KB       ║   ~4GB        ║   ~50MB       ║   ~1.5GB     ║\n",
    "╠═══════════════════╬═══════════════╬═══════════════╬═══════════════╬══════════════╣\n",
    "║ 適用場景          ║ 學習新概念    ║ 特定主題      ║ 風格/概念     ║ 空間控制     ║\n",
    "║                   ║ 風格遷移      ║ 人物/物品     ║ 靈活組合      ║ 姿態/邊緣    ║\n",
    "╠═══════════════════╬═══════════════╬═══════════════╬═══════════════╬══════════════╣\n",
    "║ 優點              ║ 極小儲存      ║ 效果最好      ║ 可組合        ║ 精確控制     ║\n",
    "║                   ║ 可組合        ║ 保真度高      ║ 效率高        ║ 保持結構     ║\n",
    "╠═══════════════════╬═══════════════╬═══════════════╬═══════════════╬══════════════╣\n",
    "║ 缺點              ║ 表達力有限    ║ 需要大VRAM    ║ 效果略遜      ║ 需要控制圖   ║\n",
    "║                   ║               ║ 不可組合      ║               ║ 訓練成本高   ║\n",
    "╠═══════════════════╬═══════════════╬═══════════════╬═══════════════╬══════════════╣\n",
    "║ 所需圖片          ║    3-5張      ║    3-5張      ║   10-50張     ║   大量成對   ║\n",
    "╚═══════════════════╩═══════════════╩═══════════════╩═══════════════╩══════════════╝\n",
    "\n",
    "選擇指南：\n",
    "┌────────────────────────────────────────────────────────────────────┐\n",
    "│ 需求                                    │ 推薦方法               │\n",
    "├────────────────────────────────────────────────────────────────────┤\n",
    "│ 學習一個新的視覺概念（如特定物品）      │ Textual Inversion      │\n",
    "│ 生成特定人物/寵物的高品質圖片           │ DreamBooth             │\n",
    "│ 學習特定藝術風格並靈活應用              │ LoRA                   │\n",
    "│ 組合多個客製化概念                      │ Textual Inv. + LoRA    │\n",
    "│ 需要精確的空間/姿態控制                 │ ControlNet             │\n",
    "│ 有限的 GPU 記憶體                       │ Textual Inv. 或 LoRA   │\n",
    "│ 需要最佳品質，不在意資源                │ DreamBooth             │\n",
    "└────────────────────────────────────────────────────────────────────┘\n",
    "\"\"\")\n",
    "\n",
    "print_comparison_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習\n",
    "\n",
    "### Exercise 1: 實作 LoRA 權重合併\n",
    "\n",
    "實作一個函數，能夠將多個 LoRA 適配器合併到一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_lora_weights(\n",
    "    lora_weights_list: List[Dict[str, torch.Tensor]],\n",
    "    weights: List[float]\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    合併多個 LoRA 權重\n",
    "    \n",
    "    Args:\n",
    "        lora_weights_list: LoRA 權重字典列表\n",
    "        weights: 各個 LoRA 的權重\n",
    "        \n",
    "    Returns:\n",
    "        合併後的 LoRA 權重\n",
    "    \"\"\"\n",
    "    # TODO: 實作權重合併\n",
    "    # 提示：\n",
    "    # 1. 遍歷所有 LoRA 權重\n",
    "    # 2. 對相同 key 的權重進行加權平均\n",
    "    # 3. 處理可能的維度不匹配\n",
    "    pass\n",
    "\n",
    "# 測試\n",
    "# lora1 = {'layer1.lora_A': torch.randn(8, 768), 'layer1.lora_B': torch.randn(768, 8)}\n",
    "# lora2 = {'layer1.lora_A': torch.randn(8, 768), 'layer1.lora_B': torch.randn(768, 8)}\n",
    "# merged = merge_lora_weights([lora1, lora2], [0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: 實作控制條件預處理\n",
    "\n",
    "實作不同類型控制條件的預處理函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlImageProcessor:\n",
    "    \"\"\"控制圖像預處理器\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_canny(\n",
    "        image: np.ndarray,\n",
    "        low_threshold: int = 100,\n",
    "        high_threshold: int = 200\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Canny 邊緣檢測\n",
    "        \n",
    "        Args:\n",
    "            image: RGB 圖像 [H, W, 3]\n",
    "            low_threshold: 低閾值\n",
    "            high_threshold: 高閾值\n",
    "            \n",
    "        Returns:\n",
    "            邊緣圖像 [H, W, 3]\n",
    "        \"\"\"\n",
    "        # TODO: 實作 Canny 邊緣檢測\n",
    "        # 提示：使用 cv2.Canny 或手動實作\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_depth(image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        估計深度圖（簡化版本）\n",
    "        \n",
    "        Args:\n",
    "            image: RGB 圖像\n",
    "            \n",
    "        Returns:\n",
    "            深度圖 [H, W, 3]\n",
    "        \"\"\"\n",
    "        # TODO: 實作深度估計\n",
    "        # 提示：可以使用亮度作為簡化的深度估計\n",
    "        pass\n",
    "\n",
    "# 測試\n",
    "# processor = ControlImageProcessor()\n",
    "# test_image = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)\n",
    "# canny = processor.to_canny(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: 設計訓練策略\n",
    "\n",
    "根據不同的使用場景，設計合適的客製化訓練策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CustomizationStrategy:\n",
    "    \"\"\"客製化策略配置\"\"\"\n",
    "    method: str  # 'textual_inversion', 'dreambooth', 'lora'\n",
    "    learning_rate: float\n",
    "    train_steps: int\n",
    "    batch_size: int\n",
    "    use_prior_preservation: bool\n",
    "    lora_rank: Optional[int] = None\n",
    "    num_vectors: Optional[int] = None  # for textual inversion\n",
    "    notes: str = \"\"\n",
    "\n",
    "\n",
    "def recommend_strategy(\n",
    "    use_case: str,\n",
    "    num_images: int,\n",
    "    available_vram_gb: int\n",
    ") -> CustomizationStrategy:\n",
    "    \"\"\"\n",
    "    根據使用場景推薦客製化策略\n",
    "    \n",
    "    Args:\n",
    "        use_case: 使用場景\n",
    "            - 'style': 學習藝術風格\n",
    "            - 'person': 學習特定人物\n",
    "            - 'object': 學習特定物品\n",
    "            - 'concept': 學習抽象概念\n",
    "        num_images: 可用的訓練圖片數量\n",
    "        available_vram_gb: 可用 VRAM (GB)\n",
    "        \n",
    "    Returns:\n",
    "        推薦的策略配置\n",
    "    \"\"\"\n",
    "    # TODO: 實作策略推薦邏輯\n",
    "    # 考慮因素：\n",
    "    # 1. VRAM 限制\n",
    "    # 2. 圖片數量\n",
    "    # 3. 使用場景的特性\n",
    "    pass\n",
    "\n",
    "# 測試\n",
    "# strategy = recommend_strategy('style', num_images=10, available_vram_gb=16)\n",
    "# print(strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "```\n",
    "Diffusion 客製化重點回顧：\n",
    "\n",
    "1. Textual Inversion\n",
    "   ├─ 只學習新的文字嵌入\n",
    "   ├─ 參數極少（~768）\n",
    "   ├─ 適合學習新概念\n",
    "   └─ 可與其他方法組合\n",
    "\n",
    "2. DreamBooth\n",
    "   ├─ 微調整個 UNet\n",
    "   ├─ 需要 prior preservation\n",
    "   ├─ 效果最好但資源需求高\n",
    "   └─ 適合學習特定主題\n",
    "\n",
    "3. LoRA\n",
    "   ├─ 低秩分解，高效訓練\n",
    "   ├─ 可以組合多個 LoRA\n",
    "   ├─ 儲存空間小（~50MB）\n",
    "   └─ 平衡效果與效率\n",
    "\n",
    "4. ControlNet\n",
    "   ├─ 添加空間控制條件\n",
    "   ├─ 使用 zero convolution\n",
    "   ├─ 支援多種控制類型\n",
    "   └─ 適合需要精確控制的場景\n",
    "\n",
    "實際應用建議：\n",
    "- RTX 5080 (16GB): 可運行 LoRA、Textual Inversion\n",
    "- DreamBooth 需要 gradient checkpointing 或降低解析度\n",
    "- 優先嘗試 LoRA，效果不佳再考慮 DreamBooth\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
