{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¯è§£é‡‹äººå·¥æ™ºæ…§ (Explainable AI)\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "- ç†è§£æ¨¡å‹å¯è§£é‡‹æ€§çš„é‡è¦æ€§\n",
    "- å¯¦ä½œ Saliency Mapï¼ˆæ¢¯åº¦å¯è¦–åŒ–ï¼‰\n",
    "- å­¸ç¿’ Integrated Gradients\n",
    "- å¯¦ä½œ LIME å±€éƒ¨è§£é‡‹æ–¹æ³•\n",
    "\n",
    "## å°æ‡‰èª²ç¨‹\n",
    "- [æå®æ¯… ML 2021 - Explainable AI](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php)\n",
    "- [HW9: Explainable AI](https://github.com/ga642381/ML2021-Spring/tree/main/HW09)\n",
    "\n",
    "## å¯è§£é‡‹ AI æ¦‚è¿°\n",
    "\n",
    "```\n",
    "ç‚ºä»€éº¼éœ€è¦å¯è§£é‡‹ AIï¼Ÿ\n",
    "â”œâ”€â”€ ä¿¡ä»»: äº†è§£æ¨¡å‹æ±ºç­–ä¾æ“š\n",
    "â”œâ”€â”€ èª¿è©¦: ç™¼ç¾æ¨¡å‹éŒ¯èª¤åŸå› \n",
    "â”œâ”€â”€ å…¬å¹³æ€§: æª¢æ¸¬åè¦‹\n",
    "â””â”€â”€ æ³•è¦: GDPR è¦æ±‚è§£é‡‹æ¬Š\n",
    "\n",
    "è§£é‡‹æ–¹æ³•åˆ†é¡\n",
    "â”œâ”€â”€ å±€éƒ¨è§£é‡‹ (Local)\n",
    "â”‚   â”œâ”€â”€ Saliency Map\n",
    "â”‚   â”œâ”€â”€ LIME\n",
    "â”‚   â””â”€â”€ SHAP\n",
    "â”‚\n",
    "â””â”€â”€ å…¨å±€è§£é‡‹ (Global)\n",
    "    â”œâ”€â”€ Feature Importance\n",
    "    â””â”€â”€ Concept Activation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "# å›ºå®šéš¨æ©Ÿç¨®å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: æº–å‚™å·¥ä½œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è¼‰å…¥é è¨“ç·´æ¨¡å‹ ==========\n",
    "\n",
    "# ä½¿ç”¨é è¨“ç·´çš„ ResNet18\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"æ¨¡å‹: ResNet18\")\n",
    "print(f\"åƒæ•¸é‡: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# ImageNet é è™•ç†\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# ImageNet é¡åˆ¥åç¨±ï¼ˆç°¡åŒ–ç‰ˆï¼‰\n",
    "IMAGENET_CLASSES = {\n",
    "    281: 'tabby cat',\n",
    "    282: 'tiger cat',\n",
    "    285: 'Egyptian cat',\n",
    "    243: 'bull mastiff',\n",
    "    244: 'boxer',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== å»ºç«‹ç¯„ä¾‹åœ–ç‰‡ ==========\n",
    "\n",
    "def create_sample_image():\n",
    "    \"\"\"\n",
    "    å»ºç«‹ä¸€å€‹ç°¡å–®çš„ç¯„ä¾‹åœ–ç‰‡ç”¨æ–¼æ¼”ç¤º\n",
    "    \"\"\"\n",
    "    # å»ºç«‹ä¸€å€‹æœ‰ç‰¹å¾µçš„å‡åœ–ç‰‡\n",
    "    img = np.zeros((224, 224, 3), dtype=np.uint8)\n",
    "    \n",
    "    # æ·»åŠ ä¸€äº›å½¢ç‹€\n",
    "    # åœ“å½¢\n",
    "    for i in range(224):\n",
    "        for j in range(224):\n",
    "            if (i - 112)**2 + (j - 112)**2 < 50**2:\n",
    "                img[i, j] = [255, 200, 100]  # æ©™è‰²åœ“\n",
    "    \n",
    "    # çŸ©å½¢\n",
    "    img[50:100, 150:200] = [100, 150, 255]  # è—è‰²çŸ©å½¢\n",
    "    \n",
    "    return Image.fromarray(img)\n",
    "\n",
    "# å»ºç«‹ç¯„ä¾‹åœ–ç‰‡\n",
    "sample_image = create_sample_image()\n",
    "\n",
    "# é è™•ç†\n",
    "input_tensor = preprocess(sample_image).unsqueeze(0).to(device)\n",
    "input_tensor.requires_grad = True\n",
    "\n",
    "# é¡¯ç¤ºåŸåœ–\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(sample_image)\n",
    "plt.title('Sample Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"è¼¸å…¥å¼µé‡å½¢ç‹€: {input_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Saliency Mapï¼ˆé¡¯è‘—æ€§åœ–ï¼‰\n",
    "\n",
    "Saliency Map é¡¯ç¤ºè¼¸å…¥åœ–åƒä¸­å“ªäº›åƒç´ å°æ¨¡å‹é æ¸¬å½±éŸ¿æœ€å¤§ã€‚\n",
    "\n",
    "### åŸç†\n",
    "$$\\text{Saliency}(x) = \\left| \\frac{\\partial y_c}{\\partial x} \\right|$$\n",
    "\n",
    "å…¶ä¸­ $y_c$ æ˜¯ç›®æ¨™é¡åˆ¥çš„åˆ†æ•¸ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Vanilla Saliency Map ==========\n",
    "\n",
    "def compute_saliency_map(model, input_tensor, target_class=None):\n",
    "    \"\"\"\n",
    "    è¨ˆç®— Saliency Map\n",
    "    \n",
    "    æ­¥é©Ÿ:\n",
    "    1. å‰å‘å‚³æ’­\n",
    "    2. è¨ˆç®—ç›®æ¨™é¡åˆ¥å°è¼¸å…¥çš„æ¢¯åº¦\n",
    "    3. å–çµ•å°å€¼ä¸¦åœ¨é€šé“ç¶­åº¦å–æœ€å¤§å€¼\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_tensor = input_tensor.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # å‰å‘å‚³æ’­\n",
    "    output = model(input_tensor)\n",
    "    \n",
    "    # å¦‚æœæ²’æœ‰æŒ‡å®šç›®æ¨™é¡åˆ¥ï¼Œä½¿ç”¨é æ¸¬é¡åˆ¥\n",
    "    if target_class is None:\n",
    "        target_class = output.argmax(dim=1).item()\n",
    "    \n",
    "    # è¨ˆç®—ç›®æ¨™é¡åˆ¥çš„åˆ†æ•¸å°è¼¸å…¥çš„æ¢¯åº¦\n",
    "    model.zero_grad()\n",
    "    target_score = output[0, target_class]\n",
    "    target_score.backward()\n",
    "    \n",
    "    # ç²å–æ¢¯åº¦\n",
    "    gradients = input_tensor.grad.data.abs()\n",
    "    \n",
    "    # åœ¨é€šé“ç¶­åº¦å–æœ€å¤§å€¼\n",
    "    saliency, _ = gradients.squeeze().max(dim=0)\n",
    "    \n",
    "    return saliency.cpu().numpy(), target_class\n",
    "\n",
    "# è¨ˆç®— Saliency Map\n",
    "saliency, pred_class = compute_saliency_map(model, input_tensor)\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(sample_image)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(saliency, cmap='hot')\n",
    "axes[1].set_title(f'Saliency Map (Class: {pred_class})')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# ç–ŠåŠ é¡¯ç¤º\n",
    "axes[2].imshow(sample_image)\n",
    "axes[2].imshow(saliency, cmap='hot', alpha=0.5)\n",
    "axes[2].set_title('Overlay')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Smooth Gradient ==========\n",
    "\n",
    "def smooth_grad(model, input_tensor, target_class=None, n_samples=50, noise_level=0.1):\n",
    "    \"\"\"\n",
    "    Smooth Gradient: é€šéæ·»åŠ å™ªè²å¹³æ»‘æ¢¯åº¦\n",
    "    \n",
    "    è§£æ±º Vanilla Gradient çš„å™ªè²å•é¡Œ\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # è¨ˆç®—å™ªè²æ¨™æº–å·®\n",
    "    stdev = noise_level * (input_tensor.max() - input_tensor.min()).item()\n",
    "    \n",
    "    total_gradients = torch.zeros_like(input_tensor)\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # æ·»åŠ å™ªè²\n",
    "        noisy_input = input_tensor + torch.randn_like(input_tensor) * stdev\n",
    "        noisy_input.requires_grad_(True)\n",
    "        \n",
    "        output = model(noisy_input)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output[0, target_class].backward()\n",
    "        \n",
    "        total_gradients += noisy_input.grad.data\n",
    "    \n",
    "    # å¹³å‡æ¢¯åº¦\n",
    "    avg_gradients = total_gradients / n_samples\n",
    "    saliency, _ = avg_gradients.abs().squeeze().max(dim=0)\n",
    "    \n",
    "    return saliency.cpu().numpy()\n",
    "\n",
    "# è¨ˆç®— Smooth Gradient\n",
    "smooth_saliency = smooth_grad(model, input_tensor, n_samples=30)\n",
    "\n",
    "# æ¯”è¼ƒ\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(sample_image)\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(saliency, cmap='hot')\n",
    "axes[1].set_title('Vanilla Gradient')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(smooth_saliency, cmap='hot')\n",
    "axes[2].set_title('Smooth Gradient')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Integrated Gradients\n",
    "\n",
    "Integrated Gradients æ˜¯ä¸€ç¨®æ›´å¯é çš„æ­¸å› æ–¹æ³•ï¼Œæ»¿è¶³å…¬ç†æ€§è¦æ±‚ã€‚\n",
    "\n",
    "### å…¬å¼\n",
    "$$\\text{IG}_i(x) = (x_i - x'_i) \\times \\int_{\\alpha=0}^{1} \\frac{\\partial F(x' + \\alpha(x - x'))}{\\partial x_i} d\\alpha$$\n",
    "\n",
    "å…¶ä¸­ $x'$ æ˜¯ baselineï¼ˆé€šå¸¸æ˜¯é»‘è‰²åœ–åƒï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Integrated Gradients ==========\n",
    "\n",
    "def integrated_gradients(model, input_tensor, target_class=None, baseline=None, n_steps=50):\n",
    "    \"\"\"\n",
    "    è¨ˆç®— Integrated Gradients\n",
    "    \n",
    "    Args:\n",
    "        model: æ¨¡å‹\n",
    "        input_tensor: è¼¸å…¥åœ–åƒ\n",
    "        target_class: ç›®æ¨™é¡åˆ¥\n",
    "        baseline: åŸºæº–åœ–åƒï¼ˆé»˜èªé»‘è‰²ï¼‰\n",
    "        n_steps: ç©åˆ†æ­¥æ•¸\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(input_tensor)\n",
    "    \n",
    "    # ç”Ÿæˆæ’å€¼è·¯å¾‘\n",
    "    scaled_inputs = [\n",
    "        baseline + (float(i) / n_steps) * (input_tensor - baseline)\n",
    "        for i in range(n_steps + 1)\n",
    "    ]\n",
    "    \n",
    "    # è¨ˆç®—æ¯å€‹é»çš„æ¢¯åº¦\n",
    "    gradients = []\n",
    "    \n",
    "    for scaled_input in scaled_inputs:\n",
    "        scaled_input = scaled_input.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        output = model(scaled_input)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output[0, target_class].backward()\n",
    "        \n",
    "        gradients.append(scaled_input.grad.data.clone())\n",
    "    \n",
    "    # ç©åˆ†ï¼ˆé»æ›¼å’Œï¼‰\n",
    "    avg_gradients = torch.stack(gradients).mean(dim=0)\n",
    "    \n",
    "    # ä¹˜ä»¥ (input - baseline)\n",
    "    integrated_grad = (input_tensor - baseline) * avg_gradients\n",
    "    \n",
    "    # åœ¨é€šé“ç¶­åº¦æ±‚å’Œä¸¦å–çµ•å°å€¼\n",
    "    attribution = integrated_grad.squeeze().sum(dim=0).abs()\n",
    "    \n",
    "    return attribution.cpu().detach().numpy()\n",
    "\n",
    "# è¨ˆç®— Integrated Gradients\n",
    "ig_attribution = integrated_gradients(model, input_tensor, n_steps=30)\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(sample_image)\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(ig_attribution, cmap='hot')\n",
    "axes[1].set_title('Integrated Gradients')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(sample_image)\n",
    "axes[2].imshow(ig_attribution, cmap='hot', alpha=0.5)\n",
    "axes[2].set_title('Overlay')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: LIME (Local Interpretable Model-agnostic Explanations)\n",
    "\n",
    "LIME é€šéåœ¨å±€éƒ¨æ“¬åˆç°¡å–®æ¨¡å‹ä¾†è§£é‡‹è¤‡é›œæ¨¡å‹çš„é æ¸¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ç°¡åŒ–ç‰ˆ LIME ==========\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from skimage.segmentation import quickshift\n",
    "\n",
    "class SimpleLIME:\n",
    "    \"\"\"\n",
    "    ç°¡åŒ–ç‰ˆ LIME å¯¦ç¾\n",
    "    \n",
    "    æ­¥é©Ÿ:\n",
    "    1. å°‡åœ–åƒåˆ†å‰²æˆè¶…åƒç´ \n",
    "    2. éš¨æ©Ÿé®è“‹è¶…åƒç´ ç”Ÿæˆæ“¾å‹•æ¨£æœ¬\n",
    "    3. ç”¨æ¨¡å‹é æ¸¬æ“¾å‹•æ¨£æœ¬\n",
    "    4. æ“¬åˆç·šæ€§æ¨¡å‹è§£é‡‹é‡è¦æ€§\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, preprocess):\n",
    "        self.model = model\n",
    "        self.preprocess = preprocess\n",
    "    \n",
    "    def explain(self, image, target_class, n_samples=500, n_features=10):\n",
    "        \"\"\"\n",
    "        è§£é‡‹é æ¸¬\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image\n",
    "            target_class: ç›®æ¨™é¡åˆ¥\n",
    "            n_samples: æ“¾å‹•æ¨£æœ¬æ•¸\n",
    "            n_features: é¸æ“‡çš„é‡è¦ç‰¹å¾µæ•¸\n",
    "        \"\"\"\n",
    "        # è½‰æ›ç‚º numpy\n",
    "        img_array = np.array(image)\n",
    "        \n",
    "        # è¶…åƒç´ åˆ†å‰²\n",
    "        segments = quickshift(img_array, kernel_size=4, max_dist=200, ratio=0.2)\n",
    "        n_segments = len(np.unique(segments))\n",
    "        \n",
    "        # ç”Ÿæˆæ“¾å‹•æ¨£æœ¬\n",
    "        perturbations = np.random.randint(0, 2, (n_samples, n_segments))\n",
    "        predictions = []\n",
    "        \n",
    "        for pert in perturbations:\n",
    "            # æ ¹æ“šæ“¾å‹•é®è“‹è¶…åƒç´ \n",
    "            temp_img = img_array.copy()\n",
    "            for i, include in enumerate(pert):\n",
    "                if not include:\n",
    "                    temp_img[segments == i] = 0  # é®è“‹ç‚ºé»‘è‰²\n",
    "            \n",
    "            # é æ¸¬\n",
    "            pil_img = Image.fromarray(temp_img)\n",
    "            input_tensor = self.preprocess(pil_img).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = self.model(input_tensor)\n",
    "                prob = F.softmax(output, dim=1)[0, target_class].item()\n",
    "            \n",
    "            predictions.append(prob)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        # æ“¬åˆç·šæ€§æ¨¡å‹\n",
    "        linear_model = Ridge(alpha=1.0)\n",
    "        linear_model.fit(perturbations, predictions)\n",
    "        \n",
    "        # ç²å–ç‰¹å¾µé‡è¦æ€§\n",
    "        importances = linear_model.coef_\n",
    "        \n",
    "        # å»ºç«‹è§£é‡‹åœ–\n",
    "        explanation = np.zeros_like(segments, dtype=float)\n",
    "        for i, imp in enumerate(importances):\n",
    "            explanation[segments == i] = imp\n",
    "        \n",
    "        return explanation, segments\n",
    "\n",
    "# ä½¿ç”¨ LIME\n",
    "lime_explainer = SimpleLIME(model, preprocess)\n",
    "\n",
    "# ç²å–é æ¸¬é¡åˆ¥\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "\n",
    "explanation, segments = lime_explainer.explain(sample_image, pred_class, n_samples=200)\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "axes[0].imshow(sample_image)\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(segments, cmap='tab20')\n",
    "axes[1].set_title('Superpixels')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(explanation, cmap='RdBu_r')\n",
    "axes[2].set_title('LIME Explanation')\n",
    "axes[2].axis('off')\n",
    "\n",
    "# åªé¡¯ç¤ºæ­£å‘è²¢ç»çš„å€åŸŸ\n",
    "mask = explanation > 0\n",
    "masked_img = np.array(sample_image).copy()\n",
    "masked_img[~mask] = 0\n",
    "axes[3].imshow(masked_img)\n",
    "axes[3].set_title('Positive Contributions')\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Grad-CAM\n",
    "\n",
    "Grad-CAM ä½¿ç”¨å·ç©å±¤çš„æ¢¯åº¦ç”Ÿæˆé¡åˆ¥æ¿€æ´»åœ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Grad-CAM ==========\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Grad-CAM: Gradient-weighted Class Activation Mapping\n",
    "    \n",
    "    åˆ©ç”¨æœ€å¾Œä¸€å€‹å·ç©å±¤çš„æ¢¯åº¦å’Œç‰¹å¾µåœ–ç”Ÿæˆç†±åŠ›åœ–\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        \n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # è¨»å†Š hooks\n",
    "        target_layer.register_forward_hook(self._save_activation)\n",
    "        target_layer.register_full_backward_hook(self._save_gradient)\n",
    "    \n",
    "    def _save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "    \n",
    "    def _save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "    \n",
    "    def generate(self, input_tensor, target_class=None):\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆ Grad-CAM ç†±åŠ›åœ–\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # å‰å‘å‚³æ’­\n",
    "        output = self.model(input_tensor)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        # åå‘å‚³æ’­\n",
    "        self.model.zero_grad()\n",
    "        output[0, target_class].backward()\n",
    "        \n",
    "        # è¨ˆç®—æ¬Šé‡ï¼ˆå…¨å±€å¹³å‡æ± åŒ–æ¢¯åº¦ï¼‰\n",
    "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)  # (1, C, 1, 1)\n",
    "        \n",
    "        # åŠ æ¬Šæ±‚å’Œ\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)  # (1, 1, H, W)\n",
    "        \n",
    "        # ReLU å’Œæ­£è¦åŒ–\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam.squeeze().cpu().numpy()\n",
    "        \n",
    "        # æ­£è¦åŒ–åˆ° [0, 1]\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        \n",
    "        return cam, target_class\n",
    "\n",
    "# ä½¿ç”¨ Grad-CAMï¼ˆå° ResNet çš„æœ€å¾Œä¸€å€‹å·ç©å±¤ï¼‰\n",
    "target_layer = model.layer4[-1].conv2\n",
    "grad_cam = GradCAM(model, target_layer)\n",
    "\n",
    "input_tensor = preprocess(sample_image).unsqueeze(0).to(device)\n",
    "input_tensor.requires_grad = True\n",
    "\n",
    "cam, pred_class = grad_cam.generate(input_tensor)\n",
    "\n",
    "# èª¿æ•´ CAM å¤§å°ä»¥åŒ¹é…è¼¸å…¥åœ–åƒ\n",
    "cam_resized = np.array(Image.fromarray((cam * 255).astype(np.uint8)).resize((224, 224))) / 255.0\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(sample_image)\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(cam_resized, cmap='jet')\n",
    "axes[1].set_title(f'Grad-CAM (Class: {pred_class})')\n",
    "axes[1].axis('off')\n",
    "\n",
    "# ç–ŠåŠ \n",
    "axes[2].imshow(sample_image)\n",
    "axes[2].imshow(cam_resized, cmap='jet', alpha=0.5)\n",
    "axes[2].set_title('Overlay')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: æ–¹æ³•æ¯”è¼ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ‰€æœ‰æ–¹æ³•æ¯”è¼ƒ ==========\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# åŸåœ–\n",
    "axes[0, 0].imshow(sample_image)\n",
    "axes[0, 0].set_title('Original Image', fontsize=12)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Vanilla Saliency\n",
    "input_tensor = preprocess(sample_image).unsqueeze(0).to(device)\n",
    "input_tensor.requires_grad = True\n",
    "saliency, _ = compute_saliency_map(model, input_tensor)\n",
    "axes[0, 1].imshow(saliency, cmap='hot')\n",
    "axes[0, 1].set_title('Vanilla Saliency', fontsize=12)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Smooth Gradient\n",
    "smooth_sal = smooth_grad(model, input_tensor, n_samples=20)\n",
    "axes[0, 2].imshow(smooth_sal, cmap='hot')\n",
    "axes[0, 2].set_title('Smooth Gradient', fontsize=12)\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Integrated Gradients\n",
    "ig = integrated_gradients(model, input_tensor, n_steps=20)\n",
    "axes[1, 0].imshow(ig, cmap='hot')\n",
    "axes[1, 0].set_title('Integrated Gradients', fontsize=12)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Grad-CAM\n",
    "axes[1, 1].imshow(cam_resized, cmap='jet')\n",
    "axes[1, 1].set_title('Grad-CAM', fontsize=12)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# LIME\n",
    "axes[1, 2].imshow(explanation, cmap='RdBu_r')\n",
    "axes[1, 2].set_title('LIME', fontsize=12)\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¯ ç¸½çµ\n",
    "\n",
    "### è§£é‡‹æ–¹æ³•æ¯”è¼ƒ\n",
    "\n",
    "| æ–¹æ³• | é¡å‹ | å„ªé» | ç¼ºé» |\n",
    "|------|------|------|------|\n",
    "| **Saliency Map** | æ¢¯åº¦ | å¿«é€Ÿã€ç°¡å–® | å™ªè²å¤š |\n",
    "| **Smooth Gradient** | æ¢¯åº¦ | è¼ƒå¹³æ»‘ | éœ€è¦å¤šæ¬¡å‰å‘ |\n",
    "| **Integrated Gradients** | æ¢¯åº¦ | æ»¿è¶³å…¬ç†æ€§ | è¨ˆç®—è¼ƒæ…¢ |\n",
    "| **Grad-CAM** | ç‰¹å¾µåœ– | ç›´è§€ã€ç©©å®š | è§£æåº¦ä½ |\n",
    "| **LIME** | æ“¾å‹• | æ¨¡å‹ç„¡é—œ | è¨ˆç®—å¾ˆæ…¢ |\n",
    "\n",
    "### é¸æ“‡å»ºè­°\n",
    "\n",
    "```\n",
    "å¿«é€Ÿèª¿è©¦: Saliency Map, Grad-CAM\n",
    "è©³ç´°åˆ†æ: Integrated Gradients, LIME\n",
    "CNN æ¨¡å‹: Grad-CAM\n",
    "ä»»æ„æ¨¡å‹: LIME, SHAP\n",
    "```\n",
    "\n",
    "### æå®æ¯… HW9 æŠ€å·§\n",
    "\n",
    "```\n",
    "1. å¯¦ä½œ Saliency Map: æ³¨æ„æ¢¯åº¦è¨ˆç®—å’Œæ­£è¦åŒ–\n",
    "2. Smooth Gradient: é©ç•¶çš„å™ªè²ç¨‹åº¦å’Œæ¡æ¨£æ•¸\n",
    "3. Integrated Gradients: ä½¿ç”¨é»‘è‰²ä½œç‚º baseline\n",
    "4. çµæœå¯è¦–åŒ–: ä½¿ç”¨é©ç•¶çš„ colormap\n",
    "```\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å‰å¾€ `robustness_security/adversarial_attack.ipynb` å­¸ç¿’å°æŠ—æ”»æ“Šï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## ğŸ‹ï¸ ç·´ç¿’\n\n### ç·´ç¿’ 1: æ¯”è¼ƒä¸åŒè§£é‡‹æ–¹æ³•åœ¨ç›¸åŒé æ¸¬ä¸Šçš„ä¸€è‡´æ€§\n\nå¯¦ä½œä¸€å€‹å‡½æ•¸ä¾†é‡åŒ–ä¸åŒè§£é‡‹æ–¹æ³•ä¹‹é–“çš„ç›¸ä¼¼åº¦ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== ç·´ç¿’ 1: æ¯”è¼ƒè§£é‡‹æ–¹æ³•çš„ä¸€è‡´æ€§ ==========\n\ndef normalize_attribution(attr):\n    \"\"\"æ­£è¦åŒ–æ­¸å› åœ–åˆ° [0, 1]\"\"\"\n    attr = attr - attr.min()\n    if attr.max() > 0:\n        attr = attr / attr.max()\n    return attr\n\ndef compute_attribution_similarity(attr1, attr2, method='spearman'):\n    \"\"\"\n    è¨ˆç®—å…©å€‹æ­¸å› åœ–çš„ç›¸ä¼¼åº¦\n    \n    Args:\n        attr1, attr2: æ­¸å› åœ– (numpy arrays)\n        method: 'spearman' (æ’åç›¸é—œ) æˆ– 'topk' (top-k é‡ç–Šç‡)\n    \"\"\"\n    from scipy.stats import spearmanr\n    \n    # å±•å¹³\n    a1 = attr1.flatten()\n    a2 = attr2.flatten()\n    \n    if method == 'spearman':\n        corr, _ = spearmanr(a1, a2)\n        return corr\n    elif method == 'topk':\n        # Top 10% åƒç´ é‡ç–Šç‡\n        k = int(0.1 * len(a1))\n        top1 = set(np.argsort(a1)[-k:])\n        top2 = set(np.argsort(a2)[-k:])\n        overlap = len(top1 & top2) / k\n        return overlap\n    \n    return 0.0\n\ndef compare_explanation_methods(model, image, preprocess):\n    \"\"\"\n    æ¯”è¼ƒæ‰€æœ‰è§£é‡‹æ–¹æ³•ä¸¦è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£\n    \"\"\"\n    input_tensor = preprocess(image).unsqueeze(0).to(device)\n    input_tensor.requires_grad = True\n    \n    # è¨ˆç®—å„æ–¹æ³•çš„æ­¸å› \n    attributions = {}\n    \n    # Vanilla Saliency\n    saliency, pred_class = compute_saliency_map(model, input_tensor)\n    attributions['Saliency'] = normalize_attribution(saliency)\n    \n    # Smooth Gradient\n    smooth = smooth_grad(model, input_tensor, target_class=pred_class, n_samples=20)\n    attributions['SmoothGrad'] = normalize_attribution(smooth)\n    \n    # Integrated Gradients\n    ig = integrated_gradients(model, input_tensor, target_class=pred_class, n_steps=20)\n    attributions['IntGrad'] = normalize_attribution(ig)\n    \n    # Grad-CAM (éœ€è¦èª¿æ•´å¤§å°)\n    target_layer = model.layer4[-1].conv2\n    grad_cam_obj = GradCAM(model, target_layer)\n    cam, _ = grad_cam_obj.generate(input_tensor, target_class=pred_class)\n    cam_resized = np.array(Image.fromarray((cam * 255).astype(np.uint8)).resize((224, 224))) / 255.0\n    attributions['GradCAM'] = normalize_attribution(cam_resized)\n    \n    # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£\n    methods = list(attributions.keys())\n    n = len(methods)\n    similarity_matrix = np.zeros((n, n))\n    \n    for i, m1 in enumerate(methods):\n        for j, m2 in enumerate(methods):\n            similarity_matrix[i, j] = compute_attribution_similarity(\n                attributions[m1], attributions[m2], method='spearman'\n            )\n    \n    return attributions, similarity_matrix, methods\n\n# åŸ·è¡Œæ¯”è¼ƒ\nprint(\"ç·´ç¿’ 1: æ¯”è¼ƒè§£é‡‹æ–¹æ³•çš„ä¸€è‡´æ€§\")\nprint(\"=\" * 50)\n\nattributions, sim_matrix, methods = compare_explanation_methods(model, sample_image, preprocess)\n\n# è¦–è¦ºåŒ–æ­¸å› åœ–\nfig, axes = plt.subplots(1, 4, figsize=(14, 3))\nfor ax, (name, attr) in zip(axes, attributions.items()):\n    ax.imshow(attr, cmap='hot')\n    ax.set_title(name)\n    ax.axis('off')\nplt.suptitle('å„æ–¹æ³•çš„æ­¸å› åœ–ï¼ˆå·²æ­£è¦åŒ–ï¼‰')\nplt.tight_layout()\nplt.show()\n\n# è¦–è¦ºåŒ–ç›¸ä¼¼åº¦çŸ©é™£\nfig, ax = plt.subplots(figsize=(6, 5))\nim = ax.imshow(sim_matrix, cmap='RdYlGn', vmin=-1, vmax=1)\nax.set_xticks(range(len(methods)))\nax.set_yticks(range(len(methods)))\nax.set_xticklabels(methods, rotation=45, ha='right')\nax.set_yticklabels(methods)\n\n# æ·»åŠ æ•¸å€¼æ¨™ç±¤\nfor i in range(len(methods)):\n    for j in range(len(methods)):\n        ax.text(j, i, f'{sim_matrix[i,j]:.2f}', ha='center', va='center', fontsize=10)\n\nplt.colorbar(im, label='Spearman Correlation')\nplt.title('è§£é‡‹æ–¹æ³•ç›¸ä¼¼åº¦çŸ©é™£')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nçµè«–ï¼š\")\nprint(\"- åŸºæ–¼æ¢¯åº¦çš„æ–¹æ³•ï¼ˆSaliency, SmoothGrad, IntGradï¼‰é€šå¸¸è¼ƒç›¸ä¼¼\")\nprint(\"- Grad-CAM å› ç‚ºä½è§£æåº¦ï¼Œå¯èƒ½èˆ‡å…¶ä»–æ–¹æ³•ç›¸é—œæ€§è¼ƒä½\")\nprint(\"- é«˜ç›¸é—œæ€§è¡¨ç¤ºæ–¹æ³•ã€ŒèªåŒã€å“ªäº›å€åŸŸé‡è¦\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}