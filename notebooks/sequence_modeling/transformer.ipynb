{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5：Attention 與 Transformer\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 理解 Attention 機制的動機與原理\n",
    "2. 掌握 Self-Attention 的 Q, K, V 概念\n",
    "3. 理解 Multi-Head Attention 的優勢\n",
    "4. 學會 Positional Encoding 的作用\n",
    "5. 完整實作 Transformer Encoder\n",
    "6. 了解 BERT 和 GPT 的核心思想\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "plt.rcParams['figure.figsize'] = (12, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1：Attention 的動機\n",
    "\n",
    "### 1.1 RNN 的瓶頸\n",
    "\n",
    "**問題：** Seq2Seq（如機器翻譯）把整個輸入壓縮成一個固定向量\n",
    "\n",
    "```\n",
    "Encoder:  [I] → [love] → [you] → [context vector]\n",
    "                                        ↓\n",
    "Decoder:                           [我] → [愛] → [你]\n",
    "```\n",
    "\n",
    "**瓶頸：** context vector 容量有限，長句子資訊會丟失\n",
    "\n",
    "### 1.2 Attention 的解決方案\n",
    "\n",
    "**想法：** Decoder 在生成每個詞時，可以「回頭看」Encoder 的所有輸出，決定要關注哪些部分\n",
    "\n",
    "```\n",
    "生成「愛」時：\n",
    "  - 看 \"I\"    → attention weight: 0.1\n",
    "  - 看 \"love\" → attention weight: 0.8  ← 重點關注！\n",
    "  - 看 \"you\"  → attention weight: 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 的核心概念\n",
    "\n",
    "print(\"\"\"\n",
    "Attention 三要素：\n",
    "\n",
    "1. Query (Q)：「我想找什麼」- 當前要生成的位置\n",
    "2. Key (K)：「這裡有什麼」- 每個可以被關注的位置\n",
    "3. Value (V)：「這裡的內容」- 實際要取出的資訊\n",
    "\n",
    "計算過程：\n",
    "┌─────────────────────────────────────────────────────────┐\n",
    "│  1. 計算相似度：score = Q · K^T                          │\n",
    "│  2. 正規化：attention_weights = softmax(score)           │\n",
    "│  3. 加權求和：output = attention_weights · V             │\n",
    "└─────────────────────────────────────────────────────────┘\n",
    "\n",
    "直覺比喻：圖書館查書\n",
    "  - Query：你想找的主題（如「深度學習」）\n",
    "  - Key：每本書的標籤/索引\n",
    "  - Value：每本書的實際內容\n",
    "  - 過程：用 Query 和每個 Key 比對，找出最相關的書，取出內容\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡單的 Attention 實現\n",
    "\n",
    "def simple_attention(query, keys, values):\n",
    "    \"\"\"\n",
    "    最簡單的 Attention\n",
    "    query: (batch, d_k)\n",
    "    keys: (batch, seq_len, d_k)\n",
    "    values: (batch, seq_len, d_v)\n",
    "    \"\"\"\n",
    "    # 計算相似度 (用內積)\n",
    "    # query: (batch, d_k) -> (batch, 1, d_k)\n",
    "    scores = torch.bmm(query.unsqueeze(1), keys.transpose(1, 2))  # (batch, 1, seq_len)\n",
    "    scores = scores.squeeze(1)  # (batch, seq_len)\n",
    "    \n",
    "    # Softmax 得到權重\n",
    "    attention_weights = F.softmax(scores, dim=-1)  # (batch, seq_len)\n",
    "    \n",
    "    # 加權求和\n",
    "    # attention_weights: (batch, seq_len) -> (batch, 1, seq_len)\n",
    "    output = torch.bmm(attention_weights.unsqueeze(1), values)  # (batch, 1, d_v)\n",
    "    output = output.squeeze(1)  # (batch, d_v)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# 測試\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "\n",
    "query = torch.randn(batch_size, d_k)\n",
    "keys = torch.randn(batch_size, seq_len, d_k)\n",
    "values = torch.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "output, attn_weights = simple_attention(query, keys, values)\n",
    "\n",
    "print(f\"Query shape: {query.shape}\")\n",
    "print(f\"Keys shape: {keys.shape}\")\n",
    "print(f\"Values shape: {values.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nAttention weights (sample 0): {attn_weights[0]}\")\n",
    "print(f\"Sum of weights: {attn_weights[0].sum():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2：Self-Attention\n",
    "\n",
    "### 2.1 什麼是 Self-Attention？\n",
    "\n",
    "**普通 Attention：** Query 來自一個序列，Key/Value 來自另一個序列\n",
    "\n",
    "**Self-Attention：** Query, Key, Value 都來自**同一個序列**\n",
    "\n",
    "**效果：** 讓序列中的每個位置都能「看到」其他所有位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attention 示意\n",
    "\n",
    "print(\"\"\"\n",
    "Self-Attention 範例：\n",
    "\n",
    "輸入句子：\"The cat sat on the mat\"\n",
    "\n",
    "當處理 \"sat\" 這個詞時：\n",
    "  - Query：\"sat\" 的表示\n",
    "  - Keys：所有詞 [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"] 的表示\n",
    "  - Values：同上\n",
    "\n",
    "Self-Attention 會計算 \"sat\" 和每個詞的相關性：\n",
    "  \"The\"  →  0.05\n",
    "  \"cat\"  →  0.40  ← 「貓」和「坐」相關！\n",
    "  \"sat\"  →  0.15\n",
    "  \"on\"   →  0.10\n",
    "  \"the\"  →  0.05\n",
    "  \"mat\"  →  0.25  ← 「坐」在「墊子」上\n",
    "\n",
    "優點：\n",
    "1. 任意兩個位置可以直接連接（不用像 RNN 一步步傳遞）\n",
    "2. 可以並行計算（不需要順序處理）\n",
    "3. 能捕捉長距離依賴\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scaled Dot-Product Attention\n",
    "\n",
    "**公式：** \n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**為什麼要除以 $\\sqrt{d_k}$？**\n",
    "- 當 $d_k$ 很大時，內積值會很大\n",
    "- 大的值會讓 softmax 輸出接近 one-hot（梯度接近 0）\n",
    "- 除以 $\\sqrt{d_k}$ 讓分數保持在合理範圍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Dot-Product Attention 實現\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \n",
    "    query: (batch, seq_len_q, d_k)\n",
    "    key: (batch, seq_len_k, d_k)\n",
    "    value: (batch, seq_len_k, d_v)\n",
    "    mask: (batch, seq_len_q, seq_len_k) or None\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    # 計算注意力分數\n",
    "    # (batch, seq_q, d_k) @ (batch, d_k, seq_k) -> (batch, seq_q, seq_k)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # 應用 mask（如果有）\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 加權求和\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# 測試\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "\n",
    "# Self-Attention：Q, K, V 來自同一個序列\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output, attn_weights = scaled_dot_product_attention(x, x, x)\n",
    "\n",
    "print(f\"輸入 shape: {x.shape}\")\n",
    "print(f\"輸出 shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 Attention Weights\n",
    "\n",
    "# 模擬一個句子的 attention\n",
    "words = [\"The\", \"cat\", \"sat\", \"mat\"]\n",
    "seq_len = len(words)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(1, seq_len, 16)\n",
    "_, attn = scaled_dot_product_attention(x, x, x)\n",
    "attn = attn.squeeze(0).numpy()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attn, cmap='Blues')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xticks(range(seq_len), words)\n",
    "plt.yticks(range(seq_len), words)\n",
    "plt.xlabel('Key (被關注的位置)')\n",
    "plt.ylabel('Query (當前位置)')\n",
    "plt.title('Self-Attention Weights')\n",
    "\n",
    "# 標註數值\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        plt.text(j, i, f'{attn[i, j]:.2f}', ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Attention Mask\n",
    "\n",
    "**用途：**\n",
    "1. **Padding Mask**：忽略 padding 位置\n",
    "2. **Causal Mask**：防止「看到未來」（用於 GPT 等自回歸模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal Mask（下三角矩陣）\n",
    "\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"建立因果遮罩，防止看到未來的位置\"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask\n",
    "\n",
    "seq_len = 5\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(\"Causal Mask:\")\n",
    "print(causal_mask)\n",
    "print(\"\\n1 = 可以看, 0 = 不能看\")\n",
    "print(\"每個位置只能看到自己和之前的位置\")\n",
    "\n",
    "# 視覺化\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(causal_mask, cmap='Greens')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Key position')\n",
    "plt.ylabel('Query position')\n",
    "plt.title('Causal Mask (for autoregressive models)')\n",
    "plt.xticks(range(seq_len))\n",
    "plt.yticks(range(seq_len))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有 Mask 和沒有 Mask 的差異\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(1, 5, 8)\n",
    "\n",
    "# 無 mask\n",
    "_, attn_no_mask = scaled_dot_product_attention(x, x, x, mask=None)\n",
    "\n",
    "# 有 causal mask\n",
    "mask = create_causal_mask(5).unsqueeze(0)\n",
    "_, attn_with_mask = scaled_dot_product_attention(x, x, x, mask=mask)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].imshow(attn_no_mask.squeeze().numpy(), cmap='Blues')\n",
    "axes[0].set_title('Without Mask (Bidirectional)')\n",
    "axes[0].set_xlabel('Key')\n",
    "axes[0].set_ylabel('Query')\n",
    "\n",
    "axes[1].imshow(attn_with_mask.squeeze().numpy(), cmap='Blues')\n",
    "axes[1].set_title('With Causal Mask (Unidirectional)')\n",
    "axes[1].set_xlabel('Key')\n",
    "axes[1].set_ylabel('Query')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"觀察：有 Causal Mask 時，上三角區域的 attention 權重為 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3：Multi-Head Attention\n",
    "\n",
    "### 3.1 為什麼需要 Multi-Head？\n",
    "\n",
    "**問題：** 單一的 attention 只能學習一種「關注模式」\n",
    "\n",
    "**解決方案：** 用多個「頭」(heads) 來捕捉不同類型的關係\n",
    "\n",
    "- Head 1：可能學習語法關係（主詞-動詞）\n",
    "- Head 2：可能學習指代關係（代名詞-名詞）\n",
    "- Head 3：可能學習位置關係（相鄰詞）\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention 結構\n",
    "\n",
    "print(\"\"\"\n",
    "Multi-Head Attention 結構：\n",
    "\n",
    "輸入 X (batch, seq_len, d_model)\n",
    "      │\n",
    "      ├──→ Linear → Q₁ ──┐\n",
    "      ├──→ Linear → K₁ ──┼──→ Attention Head 1 ──┐\n",
    "      ├──→ Linear → V₁ ──┘                       │\n",
    "      │                                          │\n",
    "      ├──→ Linear → Q₂ ──┐                       │\n",
    "      ├──→ Linear → K₂ ──┼──→ Attention Head 2 ──┼──→ Concat ──→ Linear ──→ 輸出\n",
    "      ├──→ Linear → V₂ ──┘                       │\n",
    "      │                                          │\n",
    "      ├──→ Linear → Q₃ ──┐                       │\n",
    "      ├──→ Linear → K₃ ──┼──→ Attention Head 3 ──┘\n",
    "      └──→ Linear → V₃ ──┘\n",
    "\n",
    "數學表示：\n",
    "  head_i = Attention(Q·W_i^Q, K·W_i^K, V·W_i^V)\n",
    "  MultiHead(Q, K, V) = Concat(head_1, ..., head_h)·W^O\n",
    "\n",
    "維度關係：\n",
    "  d_model = h × d_k  (例如 512 = 8 × 64)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention 實現\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model 必須能被 num_heads 整除\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Q, K, V 的線性變換\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 輸出的線性變換\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 線性變換\n",
    "        Q = self.W_q(query)  # (batch, seq_len, d_model)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # 分成多個頭\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, num_heads, d_k)\n",
    "        # -> (batch, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Scaled Dot-Product Attention (對每個頭)\n",
    "        # scores: (batch, num_heads, seq_q, seq_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # mask 需要擴展到 (batch, 1, seq_q, seq_k) 或 (batch, num_heads, seq_q, seq_k)\n",
    "            if mask.dim() == 3:\n",
    "                mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 加權求和\n",
    "        context = torch.matmul(attention_weights, V)  # (batch, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 合併多個頭\n",
    "        # (batch, num_heads, seq_len, d_k) -> (batch, seq_len, num_heads, d_k)\n",
    "        # -> (batch, seq_len, d_model)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 最後的線性變換\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# 測試\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = mha(x, x, x)\n",
    "\n",
    "print(f\"輸入 shape: {x.shape}\")\n",
    "print(f\"輸出 shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"  = (batch, num_heads, seq_len_q, seq_len_k)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化不同 Head 的 Attention\n",
    "\n",
    "# 取第一個樣本的 attention weights\n",
    "attn_sample = attn_weights[0].detach().numpy()  # (num_heads, seq_q, seq_k)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(8):\n",
    "    ax = axes[i]\n",
    "    im = ax.imshow(attn_sample[i], cmap='Blues')\n",
    "    ax.set_title(f'Head {i+1}')\n",
    "    ax.set_xlabel('Key')\n",
    "    ax.set_ylabel('Query')\n",
    "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "plt.suptitle('Attention Patterns of Different Heads', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"觀察：不同的 head 學到不同的 attention 模式\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4：Positional Encoding\n",
    "\n",
    "### 4.1 為什麼需要位置編碼？\n",
    "\n",
    "**問題：** Self-Attention 是「位置無關」的\n",
    "- \"I love you\" 和 \"you love I\" 經過 attention 會得到相同的結果！\n",
    "\n",
    "**解決方案：** 加入位置資訊 (Positional Encoding)\n",
    "\n",
    "### 4.2 正弦餘弦位置編碼\n",
    "\n",
    "原始 Transformer 使用正弦/餘弦函數：\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 實現\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # 建立位置編碼矩陣\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # 計算分母\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # 偶數位置用 sin，奇數位置用 cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # 加上 batch 維度並註冊為 buffer（不是參數，但會被保存）\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# 測試\n",
    "pe = PositionalEncoding(d_model=64, max_len=100)\n",
    "x = torch.zeros(1, 50, 64)  # 全零輸入，方便觀察 PE\n",
    "output = pe(x)\n",
    "\n",
    "print(f\"輸入 shape: {x.shape}\")\n",
    "print(f\"輸出 shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 Positional Encoding\n",
    "\n",
    "d_model = 64\n",
    "max_len = 100\n",
    "pe = PositionalEncoding(d_model, max_len, dropout=0)\n",
    "\n",
    "# 取得 PE 矩陣\n",
    "pe_matrix = pe.pe.squeeze().numpy()  # (max_len, d_model)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pe_matrix[:50, :], cmap='RdBu', aspect='auto')\n",
    "plt.colorbar(label='Value')\n",
    "plt.xlabel('Embedding Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Positional Encoding Visualization')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 顯示幾個維度隨位置的變化\n",
    "plt.figure(figsize=(12, 4))\n",
    "positions = np.arange(50)\n",
    "for dim in [0, 1, 4, 5, 20, 21]:\n",
    "    plt.plot(positions, pe_matrix[:50, dim], label=f'dim {dim}')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Positional Encoding: Different Dimensions')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"觀察：\")\n",
    "print(\"- 低維度的週期短（變化快）\")\n",
    "print(\"- 高維度的週期長（變化慢）\")\n",
    "print(\"- 每個位置都有獨特的編碼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5：Transformer Encoder\n",
    "\n",
    "### 5.1 Encoder Block 結構\n",
    "\n",
    "```\n",
    "輸入\n",
    "  │\n",
    "  ├──────────────────────┐\n",
    "  ↓                      │\n",
    "Multi-Head Attention     │\n",
    "  │                      │\n",
    "  ↓                      │\n",
    "Add & Norm ←─────────────┘ (殘差連接)\n",
    "  │\n",
    "  ├──────────────────────┐\n",
    "  ↓                      │\n",
    "Feed Forward Network     │\n",
    "  │                      │\n",
    "  ↓                      │\n",
    "Add & Norm ←─────────────┘ (殘差連接)\n",
    "  │\n",
    "  ↓\n",
    "輸出\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward Network\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise Feed Forward Network\"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        x = self.linear1(x)\n",
    "        x = F.gelu(x)  # 原始用 ReLU，現代常用 GELU\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# 測試\n",
    "ff = FeedForward(d_model=64, d_ff=256)\n",
    "x = torch.randn(2, 10, 64)\n",
    "print(f\"FeedForward: {x.shape} -> {ff(x).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder Layer\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-Attention with residual connection\n",
    "        attn_output, attn_weights = self.self_attention(x, x, x, mask)\n",
    "        x = x + self.dropout1(attn_output)  # 殘差連接\n",
    "        x = self.norm1(x)  # Layer Norm\n",
    "        \n",
    "        # Feed Forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout2(ff_output)  # 殘差連接\n",
    "        x = self.norm2(x)  # Layer Norm\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "# 測試\n",
    "encoder_layer = TransformerEncoderLayer(d_model=64, num_heads=8, d_ff=256)\n",
    "x = torch.randn(2, 10, 64)\n",
    "output, attn = encoder_layer(x)\n",
    "print(f\"Encoder Layer: {x.shape} -> {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的 Transformer Encoder\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, \n",
    "                 num_layers, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embedding 層\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # 多層 Encoder\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (batch, seq_len) - 詞索引\n",
    "        \n",
    "        # Embedding + Positional Encoding\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # 通過所有 Encoder 層\n",
    "        attention_weights = []\n",
    "        for layer in self.layers:\n",
    "            x, attn = layer(x, mask)\n",
    "            attention_weights.append(attn)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x, attention_weights\n",
    "\n",
    "# 測試\n",
    "encoder = TransformerEncoder(\n",
    "    vocab_size=1000,\n",
    "    d_model=64,\n",
    "    num_heads=8,\n",
    "    d_ff=256,\n",
    "    num_layers=4,\n",
    "    max_len=100\n",
    ")\n",
    "\n",
    "x = torch.randint(0, 1000, (2, 20))  # (batch, seq_len)\n",
    "output, attentions = encoder(x)\n",
    "\n",
    "print(f\"輸入 shape: {x.shape}\")\n",
    "print(f\"輸出 shape: {output.shape}\")\n",
    "print(f\"\\n層數: {len(attentions)}\")\n",
    "print(f\"每層 attention shape: {attentions[0].shape}\")\n",
    "print(f\"\\n總參數量: {sum(p.numel() for p in encoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6：實作 - 用 Transformer 做文本分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立資料集（使用 Module 4 的情感分析資料）\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# 正面和負面評論\n",
    "positive_texts = [\n",
    "    \"This movie is great and amazing\",\n",
    "    \"I love this film so much\",\n",
    "    \"Excellent acting and wonderful story\",\n",
    "    \"Best movie I have ever seen\",\n",
    "    \"Fantastic film with great performances\",\n",
    "    \"This is absolutely brilliant\",\n",
    "    \"Amazing movie highly recommended\",\n",
    "    \"Wonderful experience great movie\",\n",
    "    \"I really enjoyed this film\",\n",
    "    \"Perfect movie loved every moment\",\n",
    "    \"Outstanding performance by all actors\",\n",
    "    \"This film exceeded my expectations\",\n",
    "    \"Beautiful story and amazing visuals\",\n",
    "    \"One of the best films ever\",\n",
    "    \"Incredible movie must watch\",\n",
    "]\n",
    "\n",
    "negative_texts = [\n",
    "    \"This movie is terrible and boring\",\n",
    "    \"I hate this film completely\",\n",
    "    \"Worst movie I have ever seen\",\n",
    "    \"Bad acting and terrible story\",\n",
    "    \"Complete waste of time\",\n",
    "    \"This is absolutely awful\",\n",
    "    \"Boring movie do not watch\",\n",
    "    \"Terrible experience bad movie\",\n",
    "    \"I really hated this film\",\n",
    "    \"Worst film avoid at all costs\",\n",
    "    \"Disappointing and poorly made\",\n",
    "    \"This movie was so bad\",\n",
    "    \"Horrible story and bad acting\",\n",
    "    \"One of the worst films ever\",\n",
    "    \"Awful movie not recommended\",\n",
    "]\n",
    "\n",
    "all_texts = positive_texts + negative_texts\n",
    "all_labels = [1] * len(positive_texts) + [0] * len(negative_texts)\n",
    "\n",
    "# 建立詞彙表\n",
    "word_counts = Counter()\n",
    "for text in all_texts:\n",
    "    word_counts.update(text.lower().split())\n",
    "\n",
    "vocab = ['<PAD>', '<UNK>'] + [w for w, c in word_counts.most_common()]\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"詞彙表大小: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料集類別\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word2idx, max_len=20):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        words = self.texts[idx].lower().split()\n",
    "        tokens = [self.word2idx.get(w, 1) for w in words]  # 1 = <UNK>\n",
    "        \n",
    "        # Padding\n",
    "        if len(tokens) < self.max_len:\n",
    "            tokens = tokens + [0] * (self.max_len - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:self.max_len]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(tokens),\n",
    "            'label': torch.tensor(self.labels[idx])\n",
    "        }\n",
    "\n",
    "# 分割資料\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    all_texts, all_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels, word2idx)\n",
    "test_dataset = TextDataset(test_texts, test_labels, word2idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 分類器\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, \n",
    "                 num_layers, num_classes, max_len=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = TransformerEncoder(\n",
    "            vocab_size, d_model, num_heads, d_ff, num_layers, max_len, dropout\n",
    "        )\n",
    "        \n",
    "        # 分類頭\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # x: (batch, seq_len)\n",
    "        \n",
    "        # Encoder\n",
    "        encoded, attentions = self.encoder(x, mask)\n",
    "        \n",
    "        # 取 [CLS] 位置（這裡用平均池化代替）\n",
    "        # pooled = encoded[:, 0, :]  # 如果有 [CLS] token\n",
    "        pooled = encoded.mean(dim=1)  # 平均池化\n",
    "        \n",
    "        # 分類\n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        return logits, attentions\n",
    "\n",
    "# 建立模型\n",
    "model = TransformerClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=32,\n",
    "    num_heads=4,\n",
    "    d_ff=64,\n",
    "    num_layers=2,\n",
    "    num_classes=2,\n",
    "    max_len=50\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\n參數量: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "history = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 訓練\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    train_acc = 100 * correct / total\n",
    "    history['train_loss'].append(total_loss / len(train_loader))\n",
    "    history['train_acc'].append(train_acc)\n",
    "    \n",
    "    # 測試\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits, _ = model(input_ids)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    test_acc = 100 * correct / total\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Loss: {history['train_loss'][-1]:.4f}, \"\n",
    "              f\"Train Acc: {train_acc:.1f}%, Test Acc: {test_acc:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化訓練過程\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'])\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(history['train_acc'], label='Train')\n",
    "axes[1].plot(history['test_acc'], label='Test')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 Attention\n",
    "\n",
    "def visualize_attention(text, model, word2idx):\n",
    "    model.eval()\n",
    "    words = text.lower().split()\n",
    "    tokens = [word2idx.get(w, 1) for w in words]\n",
    "    \n",
    "    # Padding\n",
    "    tokens = tokens + [0] * (20 - len(tokens))\n",
    "    input_ids = torch.tensor([tokens]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits, attentions = model(input_ids)\n",
    "    \n",
    "    prob = F.softmax(logits, dim=1)\n",
    "    pred = \"Positive\" if prob[0, 1] > 0.5 else \"Negative\"\n",
    "    \n",
    "    # 取最後一層的 attention\n",
    "    attn = attentions[-1][0].cpu().numpy()  # (num_heads, seq, seq)\n",
    "    \n",
    "    # 平均所有 head\n",
    "    attn_avg = attn.mean(axis=0)[:len(words), :len(words)]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(attn_avg, cmap='Blues')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(words)), words, rotation=45)\n",
    "    plt.yticks(range(len(words)), words)\n",
    "    plt.xlabel('Key')\n",
    "    plt.ylabel('Query')\n",
    "    plt.title(f'Self-Attention (Pred: {pred}, Conf: {prob[0, 1 if pred==\"Positive\" else 0]:.2%})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 測試\n",
    "visualize_attention(\"This movie is great and amazing\", model, word2idx)\n",
    "visualize_attention(\"This movie is terrible and boring\", model, word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7：BERT 與 GPT 簡介\n",
    "\n",
    "### 7.1 BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "**核心特點：**\n",
    "- 只用 Transformer **Encoder**\n",
    "- **雙向**：可以同時看到左右上下文\n",
    "- 預訓練任務：Masked Language Model (MLM) + Next Sentence Prediction (NSP)\n",
    "\n",
    "**適用任務：** 分類、問答、命名實體識別等「理解」任務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 的 Masked Language Model 示意\n",
    "\n",
    "print(\"\"\"\n",
    "BERT 的預訓練任務 - Masked Language Model (MLM)：\n",
    "\n",
    "輸入：  \"The [MASK] sat on the [MASK]\"\n",
    "目標：       \"cat\"              \"mat\"\n",
    "\n",
    "過程：\n",
    "1. 隨機遮蔽 15% 的 token\n",
    "2. 讓模型預測被遮蔽的 token\n",
    "3. 因為可以看到兩邊的上下文，所以是「雙向」的\n",
    "\n",
    "BERT 結構：\n",
    "┌─────────────────────────────────────────────┐\n",
    "│                                             │\n",
    "│   [CLS] The [MASK] sat on the [MASK] [SEP]  │\n",
    "│     ↓    ↓    ↓    ↓   ↓  ↓   ↓     ↓      │\n",
    "│   ┌─────────────────────────────────────┐   │\n",
    "│   │      Transformer Encoder x 12       │   │\n",
    "│   └─────────────────────────────────────┘   │\n",
    "│     ↓    ↓    ↓    ↓   ↓  ↓   ↓     ↓      │\n",
    "│    分類  ...  cat  ... ...  mat  ...       │\n",
    "│                                             │\n",
    "└─────────────────────────────────────────────┘\n",
    "\n",
    "[CLS]: 用於分類任務的特殊 token\n",
    "[SEP]: 句子分隔符\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 GPT (Generative Pre-trained Transformer)\n",
    "\n",
    "**核心特點：**\n",
    "- 只用 Transformer **Decoder**（有 causal mask）\n",
    "- **單向**：只能看到左邊的上下文\n",
    "- 預訓練任務：Next Token Prediction（自回歸語言模型）\n",
    "\n",
    "**適用任務：** 文本生成、對話、寫作等「生成」任務"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT 的自回歸生成示意\n",
    "\n",
    "print(\"\"\"\n",
    "GPT 的預訓練任務 - Next Token Prediction：\n",
    "\n",
    "輸入：  \"The cat\"\n",
    "目標：       \"sat\"\n",
    "\n",
    "生成過程（逐個 token 生成）：\n",
    "\"The\" → \"cat\"\n",
    "\"The cat\" → \"sat\"\n",
    "\"The cat sat\" → \"on\"\n",
    "...\n",
    "\n",
    "GPT 結構：\n",
    "┌─────────────────────────────────────────────┐\n",
    "│                                             │\n",
    "│   The    cat    sat    on    the    mat     │\n",
    "│    ↓      ↓      ↓      ↓     ↓      ↓      │\n",
    "│   ┌─────────────────────────────────────┐   │\n",
    "│   │ Transformer Decoder x 12 (masked)  │   │\n",
    "│   └─────────────────────────────────────┘   │\n",
    "│    ↓      ↓      ↓      ↓     ↓      ↓      │\n",
    "│   cat    sat    on    the   mat    .       │  ← 預測下一個 token\n",
    "│                                             │\n",
    "└─────────────────────────────────────────────┘\n",
    "\n",
    "關鍵：使用 Causal Mask，每個位置只能看到之前的 token\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT vs GPT 比較\n",
    "\n",
    "print(\"\"\"\n",
    "┌────────────────┬─────────────────────┬─────────────────────┐\n",
    "│                │        BERT         │        GPT          │\n",
    "├────────────────┼─────────────────────┼─────────────────────┤\n",
    "│ 架構           │ Encoder-only        │ Decoder-only        │\n",
    "│ 方向           │ 雙向 (Bidirectional)│ 單向 (Unidirectional)│\n",
    "│ 預訓練         │ MLM + NSP           │ Next Token Predict  │\n",
    "│ Attention Mask │ 無 (全部可見)       │ Causal (只看過去)   │\n",
    "│ 主要用途       │ 理解任務            │ 生成任務            │\n",
    "│ 代表模型       │ BERT, RoBERTa       │ GPT-2/3/4, ChatGPT  │\n",
    "└────────────────┴─────────────────────┴─────────────────────┘\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 練習題（已完成，請閱讀理解）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 1：使用 PyTorch 內建的 Transformer\n",
    "\n",
    "**目標：** 學會使用 `nn.TransformerEncoder`\n",
    "\n",
    "**Hint：** PyTorch 提供了優化過的 Transformer 實現"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1：使用 PyTorch 內建 Transformer\n",
    "\n",
    "# 定義 Transformer Encoder Layer\n",
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=64,\n",
    "    nhead=8,\n",
    "    dim_feedforward=256,\n",
    "    dropout=0.1,\n",
    "    batch_first=True  # 重要！讓輸入格式是 (batch, seq, feature)\n",
    ")\n",
    "\n",
    "# 堆疊多層\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "\n",
    "# 測試\n",
    "x = torch.randn(2, 10, 64)  # (batch, seq_len, d_model)\n",
    "output = transformer_encoder(x)\n",
    "\n",
    "print(f\"輸入 shape: {x.shape}\")\n",
    "print(f\"輸出 shape: {output.shape}\")\n",
    "\n",
    "# 帶 mask\n",
    "mask = torch.triu(torch.ones(10, 10) * float('-inf'), diagonal=1)\n",
    "output_masked = transformer_encoder(x, mask=mask)\n",
    "print(f\"\\n帶 Causal Mask 的輸出 shape: {output_masked.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：不同 Head 數量的影響\n",
    "\n",
    "**目標：** 觀察 Multi-Head 數量對模型的影響\n",
    "\n",
    "**Hint：** head 數量需要能整除 d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2：比較不同 head 數量\n",
    "\n",
    "d_model = 64\n",
    "head_configs = [1, 2, 4, 8]\n",
    "\n",
    "print(\"不同 head 數量的參數量比較：\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for num_heads in head_configs:\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    params = sum(p.numel() for p in mha.parameters())\n",
    "    d_k = d_model // num_heads\n",
    "    print(f\"num_heads={num_heads}, d_k={d_k}, 參數量={params:,}\")\n",
    "\n",
    "print(\"\\n觀察：參數量相同！因為總是 4 個 (d_model, d_model) 的矩陣\")\n",
    "print(\"區別在於：多個 head 可以學習不同的 attention pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3：Learned Positional Encoding\n",
    "\n",
    "**目標：** 比較固定的正弦位置編碼和可學習的位置編碼\n",
    "\n",
    "**Hint：** BERT 和 GPT 都使用 learned positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 3：Learned Positional Embedding\n",
    "\n",
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    \"\"\"可學習的位置編碼（像 BERT/GPT 那樣）\"\"\"\n",
    "    def __init__(self, d_model, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
    "        pos_emb = self.pos_embedding(positions)\n",
    "        return self.dropout(x + pos_emb)\n",
    "\n",
    "# 比較兩種方式\n",
    "d_model = 64\n",
    "max_len = 100\n",
    "\n",
    "sinusoidal_pe = PositionalEncoding(d_model, max_len, dropout=0)\n",
    "learned_pe = LearnedPositionalEncoding(d_model, max_len, dropout=0)\n",
    "\n",
    "print(\"位置編碼方式比較：\")\n",
    "print(f\"固定（正弦餘弦）: 0 參數（預計算）\")\n",
    "print(f\"可學習: {max_len * d_model:,} 參數\")\n",
    "\n",
    "print(\"\\n優缺點：\")\n",
    "print(\"- 固定：可以處理比訓練時更長的序列，不需要額外參數\")\n",
    "print(\"- 可學習：更靈活，可能學到更好的位置表示，但有最大長度限制\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 8：進階 Transformer 技巧\n\n### 8.1 Cross-Attention（交叉注意力）\n\n**用途：** Decoder 關注 Encoder 的輸出（用於翻譯、摘要等 seq2seq 任務）\n\n**區別：**\n- Self-Attention：Q, K, V 都來自同一序列\n- Cross-Attention：Q 來自 Decoder，K 和 V 來自 Encoder",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Module 5 總結\n",
    "\n",
    "### 核心概念\n",
    "\n",
    "1. **Attention 機制**：\n",
    "   - Query, Key, Value 三要素\n",
    "   - 計算：score → softmax → weighted sum\n",
    "   - 解決 RNN 的瓶頸問題\n",
    "\n",
    "2. **Self-Attention**：\n",
    "   - Q, K, V 來自同一個序列\n",
    "   - 每個位置可以直接關注其他所有位置\n",
    "   - Scaled Dot-Product：除以 √d_k\n",
    "\n",
    "3. **Multi-Head Attention**：\n",
    "   - 多個 head 捕捉不同類型的關係\n",
    "   - 並行計算，效率高\n",
    "\n",
    "4. **Positional Encoding**：\n",
    "   - 正弦餘弦編碼（固定）\n",
    "   - Learned Embedding（可學習）\n",
    "\n",
    "5. **Transformer 結構**：\n",
    "   - Encoder：Multi-Head Attention + FFN + 殘差 + LayerNorm\n",
    "   - Decoder：額外加 Cross-Attention 和 Causal Mask\n",
    "\n",
    "6. **預訓練模型**：\n",
    "   - BERT：雙向 Encoder，適合理解任務\n",
    "   - GPT：單向 Decoder，適合生成任務\n",
    "\n",
    "### 下一步：Module 6 - LLM 實務入門"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Cross-Attention 實現\n\nclass CrossAttention(nn.Module):\n    \"\"\"交叉注意力：Query 來自一個序列，Key/Value 來自另一個序列\"\"\"\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.mha = MultiHeadAttention(d_model, num_heads)\n    \n    def forward(self, query_seq, kv_seq, mask=None):\n        \"\"\"\n        query_seq: (batch, seq_len_q, d_model) - Decoder 的表示\n        kv_seq: (batch, seq_len_kv, d_model) - Encoder 的輸出\n        \"\"\"\n        # Query 來自 query_seq，Key 和 Value 來自 kv_seq\n        return self.mha(query_seq, kv_seq, kv_seq, mask)\n\n# 完整的 Transformer Decoder Layer\nclass TransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        \n        # Masked Self-Attention（看不到未來）\n        self.self_attention = MultiHeadAttention(d_model, num_heads)\n        # Cross-Attention（關注 Encoder 輸出）\n        self.cross_attention = MultiHeadAttention(d_model, num_heads)\n        # Feed Forward\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        \n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n        # 1. Masked Self-Attention\n        self_attn, _ = self.self_attention(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(self_attn))\n        \n        # 2. Cross-Attention（關注 Encoder）\n        cross_attn, _ = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n        x = self.norm2(x + self.dropout(cross_attn))\n        \n        # 3. Feed Forward\n        ff = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff))\n        \n        return x\n\n# 測試\ndecoder_layer = TransformerDecoderLayer(d_model=64, num_heads=8, d_ff=256)\n\n# Decoder 輸入和 Encoder 輸出\ndecoder_input = torch.randn(2, 8, 64)   # (batch, tgt_len, d_model)\nencoder_output = torch.randn(2, 12, 64) # (batch, src_len, d_model)\n\n# Causal mask for decoder\ntgt_mask = create_causal_mask(8).unsqueeze(0)\n\noutput = decoder_layer(decoder_input, encoder_output, tgt_mask=tgt_mask)\nprint(f\"Decoder 輸入: {decoder_input.shape}\")\nprint(f\"Encoder 輸出: {encoder_output.shape}\")\nprint(f\"Decoder Layer 輸出: {output.shape}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 8.2 Pre-norm vs Post-norm\n\n**Post-norm（原始 Transformer）：**\n```\nx → Attention → Add → LayerNorm → FFN → Add → LayerNorm\n```\n\n**Pre-norm（現代常用）：**\n```\nx → LayerNorm → Attention → Add → LayerNorm → FFN → Add\n```\n\n**Pre-norm 優點：** 訓練更穩定，尤其對深層網路",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Pre-norm vs Post-norm 比較\n\nclass PreNormEncoderLayer(nn.Module):\n    \"\"\"Pre-norm 版本的 Encoder Layer（現代 LLM 常用）\"\"\"\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        \n        self.self_attention = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        \n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # Pre-norm: 先 normalize，再做操作\n        # Attention\n        normed = self.norm1(x)\n        attn_output, _ = self.self_attention(normed, normed, normed, mask)\n        x = x + self.dropout(attn_output)\n        \n        # FFN\n        normed = self.norm2(x)\n        ff_output = self.feed_forward(normed)\n        x = x + self.dropout(ff_output)\n        \n        return x\n\nprint(\"\"\"\nPre-norm vs Post-norm 比較：\n\n┌────────────────┬─────────────────┬─────────────────┐\n│                │    Post-norm    │    Pre-norm     │\n├────────────────┼─────────────────┼─────────────────┤\n│ LayerNorm 位置 │ 子層之後        │ 子層之前        │\n│ 訓練穩定性     │ 較差（深層）    │ 較好            │\n│ 需要 warmup    │ 是              │ 通常不需要      │\n│ 最終層 norm    │ 不需要          │ 需要額外 norm   │\n│ 使用模型       │ BERT, 原始 GPT  │ GPT-2+, LLaMA   │\n└────────────────┴─────────────────┴─────────────────┘\n\n為什麼 Pre-norm 更穩定？\n- 殘差連接直接連到輸出，梯度流更順暢\n- LayerNorm 在前面，控制輸入範圍\n- 對初始化不太敏感\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 8.3 Transformer 訓練技巧\n\n**重要的超參數和技巧：**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Transformer 訓練最佳實踐\n\nprint(\"\"\"\n╔═══════════════════════════════════════════════════════════════════╗\n║                    Transformer 訓練技巧                           ║\n╠═══════════════════════════════════════════════════════════════════╣\n║                                                                   ║\n║  1. 學習率調度（非常重要）                                         ║\n║     ─────────────────────                                         ║\n║     • Warmup + Decay 是標準做法                                   ║\n║     • 原始論文：warmup 4000 steps，然後 inverse sqrt decay        ║\n║     • 現代做法：cosine annealing 或 linear decay                  ║\n║                                                                   ║\n║  2. 權重初始化                                                    ║\n║     ─────────────                                                 ║\n║     • Xavier/Glorot 初始化（PyTorch 預設）                        ║\n║     • 某些實現用 N(0, 0.02) 初始化 embedding                       ║\n║     • 殘差連接前的層可以縮放：weight * (1/√num_layers)            ║\n║                                                                   ║\n║  3. 正則化                                                        ║\n║     ──────                                                        ║\n║     • Dropout: 0.1 是常見值                                       ║\n║     • Label Smoothing: 0.1 可以提升泛化                           ║\n║     • Weight Decay: AdamW 配合 0.01-0.1                           ║\n║                                                                   ║\n║  4. 梯度裁剪                                                      ║\n║     ──────                                                        ║\n║     • max_norm=1.0 是常見選擇                                     ║\n║     • 對於大型模型更重要                                          ║\n║                                                                   ║\n║  5. Batch Size                                                    ║\n║     ──────────                                                    ║\n║     • 大 batch 配合 warmup 效果更好                               ║\n║     • 若 GPU 記憶體不足，用 gradient accumulation                  ║\n║                                                                   ║\n╚═══════════════════════════════════════════════════════════════════╝\n\"\"\")\n\n# Warmup + Cosine Decay 學習率調度器\nclass TransformerLRScheduler:\n    \"\"\"帶 Warmup 的學習率調度\"\"\"\n    def __init__(self, optimizer, d_model, warmup_steps):\n        self.optimizer = optimizer\n        self.d_model = d_model\n        self.warmup_steps = warmup_steps\n        self.step_num = 0\n    \n    def step(self):\n        self.step_num += 1\n        lr = self._get_lr()\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n    \n    def _get_lr(self):\n        # 原始 Transformer 的學習率公式\n        step = self.step_num\n        return self.d_model ** (-0.5) * min(\n            step ** (-0.5),\n            step * self.warmup_steps ** (-1.5)\n        )\n\n# 視覺化學習率\nwarmup_steps = 4000\nd_model = 512\n\nsteps = list(range(1, 50000))\nscheduler_dummy = TransformerLRScheduler(None, d_model, warmup_steps)\n\nlrs = []\nfor step in steps:\n    scheduler_dummy.step_num = step\n    lrs.append(scheduler_dummy._get_lr())\n\nplt.figure(figsize=(10, 5))\nplt.plot(steps, lrs)\nplt.axvline(x=warmup_steps, color='r', linestyle='--', label='End of warmup')\nplt.xlabel('Training Step')\nplt.ylabel('Learning Rate')\nplt.title('Transformer Learning Rate Schedule (Warmup + Inverse Sqrt Decay)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nprint(f\"Peak LR (at warmup_steps={warmup_steps}): {max(lrs):.6f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 8.4 Attention 效率改進\n\n**標準 Self-Attention 的問題：** \n- 時間複雜度：$O(n^2 \\cdot d)$\n- 空間複雜度：$O(n^2)$\n- 長序列（如 n=4096）會非常慢且耗記憶體",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Attention 複雜度分析\n\nprint(\"\"\"\n高效 Attention 變體：\n\n╔════════════════════════════════════════════════════════════════════════╗\n║  方法                    │ 複雜度       │ 說明                         ║\n╠════════════════════════════════════════════════════════════════════════╣\n║  Standard Attention      │ O(n²)       │ 原始版本                     ║\n║  Sparse Attention        │ O(n√n)      │ 只關注部分位置（GPT-3）      ║\n║  Linear Attention        │ O(n)        │ 改變計算順序（理論上）       ║\n║  Flash Attention         │ O(n²)       │ IO-aware，實際上更快         ║\n║  Multi-Query Attention   │ O(n²)       │ K,V 共享，減少記憶體         ║\n║  Grouped-Query Attention │ O(n²)       │ MQA 和 MHA 的折衷            ║\n╚════════════════════════════════════════════════════════════════════════╝\n\nFlash Attention（PyTorch 2.0+）：\n- 不改變數學計算，只優化 GPU 記憶體訪問\n- 顯著減少記憶體使用和提升速度\n- 使用方式：torch.nn.functional.scaled_dot_product_attention()\n\"\"\")\n\n# 比較標準 attention 和 PyTorch 2.0 的 flash attention\nimport time\n\ndef benchmark_attention(seq_len, d_model, num_heads, device):\n    \"\"\"比較不同 attention 實現的速度\"\"\"\n    batch_size = 8\n    \n    q = torch.randn(batch_size, num_heads, seq_len, d_model // num_heads, device=device)\n    k = torch.randn(batch_size, num_heads, seq_len, d_model // num_heads, device=device)\n    v = torch.randn(batch_size, num_heads, seq_len, d_model // num_heads, device=device)\n    \n    # 標準實現\n    def standard_attention(q, k, v):\n        d_k = q.size(-1)\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n        weights = F.softmax(scores, dim=-1)\n        return torch.matmul(weights, v)\n    \n    # Warmup\n    for _ in range(3):\n        _ = standard_attention(q, k, v)\n        _ = F.scaled_dot_product_attention(q, k, v)\n    \n    if device.type == 'cuda':\n        torch.cuda.synchronize()\n    \n    # 測試標準版本\n    start = time.time()\n    for _ in range(10):\n        _ = standard_attention(q, k, v)\n    if device.type == 'cuda':\n        torch.cuda.synchronize()\n    standard_time = (time.time() - start) / 10\n    \n    # 測試 PyTorch 2.0+ 優化版本\n    start = time.time()\n    for _ in range(10):\n        _ = F.scaled_dot_product_attention(q, k, v)\n    if device.type == 'cuda':\n        torch.cuda.synchronize()\n    optimized_time = (time.time() - start) / 10\n    \n    return standard_time * 1000, optimized_time * 1000\n\nprint(\"\\nAttention 速度比較（ms, 越低越好）：\")\nprint(\"-\" * 60)\n\nfor seq_len in [128, 256, 512]:\n    std_time, opt_time = benchmark_attention(seq_len, 256, 8, device)\n    speedup = std_time / opt_time\n    print(f\"seq_len={seq_len:4d}: 標準={std_time:.2f}ms, 優化={opt_time:.2f}ms, 加速={speedup:.1f}x\")\n\nprint(\"\\n提示：在 GPU 上且序列長度較大時，加速效果更明顯\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Module 5 完整總結\n\n### 🎯 核心概念\n\n| 概念 | 說明 | 重要程度 |\n|------|------|----------|\n| **Attention** | Q·K^T 計算相關性，V 加權求和 | ⭐⭐⭐⭐⭐ |\n| **Self-Attention** | Q, K, V 來自同一序列 | ⭐⭐⭐⭐⭐ |\n| **Multi-Head** | 多個 head 捕捉不同關係 | ⭐⭐⭐⭐⭐ |\n| **Positional Encoding** | 加入位置資訊 | ⭐⭐⭐⭐ |\n| **Layer Normalization** | 穩定訓練 | ⭐⭐⭐⭐ |\n| **殘差連接** | 梯度流動，深層訓練 | ⭐⭐⭐⭐⭐ |\n\n### 🔑 關鍵公式\n\n```\nScaled Dot-Product Attention:\n  Attention(Q, K, V) = softmax(QK^T / √d_k) V\n\nMulti-Head Attention:\n  head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n  MultiHead = Concat(head_1, ..., head_h)W^O\n\nPositional Encoding:\n  PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n  PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n```\n\n### 🏗️ 架構對比\n\n| 模型 | 架構 | 方向 | 預訓練任務 | 用途 |\n|------|------|------|------------|------|\n| **BERT** | Encoder | 雙向 | MLM + NSP | 理解任務 |\n| **GPT** | Decoder | 單向 | Next Token | 生成任務 |\n| **T5** | Enc-Dec | 混合 | Span Corruption | 通用任務 |\n\n### 💡 實務技巧\n\n1. **學習率**：\n   - 必須用 Warmup（4000 steps 或 5-10% of total）\n   - Warmup + Cosine/Linear Decay\n   \n2. **模型配置**：\n   - d_model: 256/512/768/1024\n   - num_heads: 8/12/16（確保 d_model % num_heads == 0）\n   - d_ff = 4 * d_model（標準配置）\n   - num_layers: 6/12/24\n\n3. **正則化**：\n   - Dropout: 0.1 是標準值\n   - Label Smoothing: 0.1 可提升泛化\n   \n4. **效率優化**：\n   - 使用 `F.scaled_dot_product_attention()`（Flash Attention）\n   - 混合精度訓練（AMP）\n   - Gradient Checkpointing（省記憶體）\n\n5. **調試技巧**：\n   - 視覺化 Attention Weights\n   - 檢查不同 head 的模式\n   - 監控梯度範數\n\n### 🚀 下一步\n\n- Module 6：LLM 實務（使用 Hugging Face）\n- 進一步學習：Vision Transformer (ViT)、Multimodal Transformer\n\n---\n\n**恭喜完成 Transformer 模組！** 🎉\n\nTransformer 是現代深度學習最重要的架構，從 NLP 到 CV 再到多模態，幾乎所有 SOTA 模型都基於 Transformer。掌握這個模組的內容，你就具備了理解 GPT、BERT、LLaMA 等大型語言模型的基礎。",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}