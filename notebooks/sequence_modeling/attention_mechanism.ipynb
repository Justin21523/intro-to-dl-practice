{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ³¨æ„åŠ›æ©Ÿåˆ¶ (Attention Mechanism)\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "- ç†è§£ Attention çš„æ ¸å¿ƒæ¦‚å¿µï¼šQuery, Key, Value\n",
    "- å¯¦ä½œ Self-Attention æ©Ÿåˆ¶\n",
    "- å­¸ç¿’ Multi-Head Attention\n",
    "- æ‡‰ç”¨æ–¼èªéŸ³è­˜åˆ¥ä»»å‹™ï¼ˆæå®æ¯… HW4ï¼‰\n",
    "\n",
    "## å°æ‡‰èª²ç¨‹\n",
    "- [æå®æ¯… ML 2021 - Self-Attention](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php)\n",
    "- [HW4: Speaker Recognition with Self-Attention](https://github.com/ga642381/ML2021-Spring/tree/main/HW04)\n",
    "\n",
    "## Attention ç™¼å±•æ­·ç¨‹\n",
    "\n",
    "```\n",
    "Attention æ¼”é€²\n",
    "â”œâ”€â”€ 2014: Seq2Seq Attention (Bahdanau)\n",
    "â”œâ”€â”€ 2015: Luong Attention (ç°¡åŒ–ç‰ˆ)\n",
    "â”œâ”€â”€ 2017: Transformer (Self-Attention)\n",
    "â”œâ”€â”€ 2018: BERT (é›™å‘ Self-Attention)\n",
    "â””â”€â”€ 2020+: Vision Transformer, å„ç¨®è®Šé«”\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "# å›ºå®šéš¨æ©Ÿç¨®å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Attention æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "### Query, Key, Value\n",
    "\n",
    "Attention æ©Ÿåˆ¶å¯ä»¥ç”¨æœå°‹å¼•æ“ä¾†æ¯”å–»ï¼š\n",
    "\n",
    "| æ¦‚å¿µ | æ¯”å–» | ä½œç”¨ |\n",
    "|------|------|------|\n",
    "| **Query (Q)** | æœå°‹é—œéµå­— | ä½ æƒ³æ‰¾ä»€éº¼ |\n",
    "| **Key (K)** | ç¶²é æ¨™é¡Œ | ç”¨ä¾†åŒ¹é…æŸ¥è©¢ |\n",
    "| **Value (V)** | ç¶²é å…§å®¹ | å¯¦éš›è¿”å›çš„è³‡è¨Š |\n",
    "\n",
    "### Scaled Dot-Product Attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Scaled Dot-Product Attention ==========\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor (batch, seq_len, d_k)\n",
    "        K: Key tensor (batch, seq_len, d_k)\n",
    "        V: Value tensor (batch, seq_len, d_v)\n",
    "        mask: å¯é¸çš„é®ç½©\n",
    "    \n",
    "    Returns:\n",
    "        output: æ³¨æ„åŠ›è¼¸å‡º\n",
    "        attention_weights: æ³¨æ„åŠ›æ¬Šé‡\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # è¨ˆç®—æ³¨æ„åŠ›åˆ†æ•¸: QK^T / sqrt(d_k)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # æ‡‰ç”¨é®ç½©ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Softmax ç²å¾—æ³¨æ„åŠ›æ¬Šé‡\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # åŠ æ¬Šæ±‚å’Œ\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# æ¸¬è©¦\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_k = 8\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "output, attention = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention.shape}\")\n",
    "print(f\"\\næ³¨æ„åŠ›æ¬Šé‡ (ç¬¬ä¸€å€‹æ¨£æœ¬):\")\n",
    "print(attention[0].numpy().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== å¯è¦–åŒ– Attention Weights ==========\n",
    "\n",
    "def visualize_attention(attention_weights, title=\"Attention Weights\"):\n",
    "    \"\"\"\n",
    "    å¯è¦–åŒ–æ³¨æ„åŠ›æ¬Šé‡\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(attention_weights.detach().numpy(), cmap='viridis')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# å¯è¦–åŒ–ç¬¬ä¸€å€‹æ¨£æœ¬çš„æ³¨æ„åŠ›\n",
    "visualize_attention(attention[0], \"Attention Weights (Sample 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Self-Attention\n",
    "\n",
    "Self-Attention: Q, K, V éƒ½ä¾†è‡ªåŒä¸€å€‹è¼¸å…¥åºåˆ—ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Self-Attention Layer ==========\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-Attention å±¤\n",
    "    \n",
    "    è¼¸å…¥é€šéä¸‰å€‹ç·šæ€§è®Šæ›å¾—åˆ° Q, K, V\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Q, K, V çš„ç·šæ€§æŠ•å½±\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # è¼¸å‡ºæŠ•å½±\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        # ç·šæ€§æŠ•å½±\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Scaled Dot-Product Attention\n",
    "        output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # è¼¸å‡ºæŠ•å½±\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# æ¸¬è©¦\n",
    "self_attn = SelfAttention(embed_dim=64)\n",
    "x = torch.randn(2, 10, 64)  # (batch, seq_len, embed_dim)\n",
    "output, attn = self_attn(x)\n",
    "\n",
    "print(f\"è¼¸å…¥: {x.shape}\")\n",
    "print(f\"è¼¸å‡º: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Multi-Head Attention\n",
    "\n",
    "å¤šé ­æ³¨æ„åŠ›å…è¨±æ¨¡å‹åœ¨ä¸åŒçš„è¡¨ç¤ºå­ç©ºé–“ä¸­å­¸ç¿’ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Multi-Head Attention ==========\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention\n",
    "    \n",
    "    å°‡ embedding åˆ†æˆå¤šå€‹ headï¼Œæ¯å€‹ head ç¨ç«‹åš attentionï¼Œ\n",
    "    æœ€å¾Œ concatenate å†æŠ•å½±ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim å¿…é ˆèƒ½è¢« num_heads æ•´é™¤\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Q, K, V æŠ•å½±ï¼ˆä¸€æ¬¡æ€§æŠ•å½±åˆ°æ‰€æœ‰ headï¼‰\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # è¼¸å‡ºæŠ•å½±\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # ç·šæ€§æŠ•å½±\n",
    "        Q = self.W_q(x)  # (batch, seq_len, embed_dim)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # é‡å¡‘ç‚ºå¤šé ­: (batch, num_heads, seq_len, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled Dot-Product Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # é‡å¡‘å›åŸå§‹ç¶­åº¦: (batch, seq_len, embed_dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        \n",
    "        # è¼¸å‡ºæŠ•å½±\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# æ¸¬è©¦\n",
    "mha = MultiHeadAttention(embed_dim=64, num_heads=8)\n",
    "x = torch.randn(2, 10, 64)\n",
    "output, attn = mha(x)\n",
    "\n",
    "print(f\"è¼¸å…¥: {x.shape}\")\n",
    "print(f\"è¼¸å‡º: {output.shape}\")\n",
    "print(f\"Attention shape: {attn.shape}  # (batch, heads, seq, seq)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== å¯è¦–åŒ–å¤šé ­æ³¨æ„åŠ› ==========\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(attn[0, i].detach().numpy(), cmap='viridis')\n",
    "    ax.set_title(f'Head {i+1}')\n",
    "    ax.set_xlabel('Key')\n",
    "    ax.set_ylabel('Query')\n",
    "\n",
    "plt.suptitle('Multi-Head Attention Patterns', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Transformer Encoder Block\n",
    "\n",
    "å®Œæ•´çš„ Transformer Encoder åŒ…å«ï¼š\n",
    "- Multi-Head Self-Attention\n",
    "- Position-wise Feed-Forward Network\n",
    "- Layer Normalization\n",
    "- Residual Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Position-wise Feed-Forward Network ==========\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network\n",
    "    \n",
    "    FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.fc2 = nn.Linear(ff_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Transformer Encoder Block ==========\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder Block\n",
    "    \n",
    "    çµæ§‹:\n",
    "    x â†’ LayerNorm â†’ MHA â†’ Dropout â†’ + x (residual)\n",
    "      â†’ LayerNorm â†’ FFN â†’ Dropout â†’ + x (residual)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(embed_dim, ff_dim, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-Attention with residual\n",
    "        attn_output, attn_weights = self.attention(self.norm1(x), mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        \n",
    "        # Feed-Forward with residual\n",
    "        ff_output = self.feed_forward(self.norm2(x))\n",
    "        x = x + self.dropout(ff_output)\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "# æ¸¬è©¦\n",
    "encoder_block = TransformerEncoderBlock(\n",
    "    embed_dim=64,\n",
    "    num_heads=8,\n",
    "    ff_dim=256,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "x = torch.randn(2, 10, 64)\n",
    "output, attn = encoder_block(x)\n",
    "print(f\"Encoder Block è¼¸å‡º: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Speaker Recognitionï¼ˆæå®æ¯… HW4ï¼‰\n",
    "\n",
    "### ä»»å‹™èªªæ˜\n",
    "\n",
    "**ç›®æ¨™**ï¼šæ ¹æ“šèªéŸ³ç‰‡æ®µè­˜åˆ¥èªªè©±è€…\n",
    "\n",
    "**æ¶æ§‹**ï¼š\n",
    "- è¼¸å…¥ï¼šMFCC ç‰¹å¾µåºåˆ—\n",
    "- Conformer/Transformer Encoder\n",
    "- å¹³å‡æ± åŒ–\n",
    "- åˆ†é¡å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ¨¡æ“¬ Speaker Recognition è³‡æ–™ ==========\n",
    "\n",
    "def generate_speaker_data(n_samples=1000, max_seq_len=128, feat_dim=40, n_speakers=600):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆé¡ä¼¼ HW4 çš„ Speaker Recognition è³‡æ–™\n",
    "    \n",
    "    å¯¦éš› HW4:\n",
    "    - 600 ä½èªªè©±è€…\n",
    "    - æ¯å€‹èªéŸ³ç‰‡æ®µæ˜¯è®Šé•·çš„ MFCC åºåˆ—\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # éš¨æ©Ÿé•·åº¦\n",
    "        seq_len = np.random.randint(32, max_seq_len + 1)\n",
    "        # éš¨æ©Ÿç‰¹å¾µ\n",
    "        x = np.random.randn(seq_len, feat_dim).astype(np.float32)\n",
    "        # éš¨æ©Ÿèªªè©±è€…\n",
    "        speaker = np.random.randint(0, n_speakers)\n",
    "        \n",
    "        data.append(x)\n",
    "        labels.append(speaker)\n",
    "    \n",
    "    return data, np.array(labels)\n",
    "\n",
    "# ç”Ÿæˆè³‡æ–™\n",
    "speaker_data, speaker_labels = generate_speaker_data(n_samples=1000, n_speakers=600)\n",
    "\n",
    "print(f\"æ¨£æœ¬æ•¸: {len(speaker_data)}\")\n",
    "print(f\"èªªè©±è€…æ•¸: {len(set(speaker_labels))}\")\n",
    "print(f\"åºåˆ—é•·åº¦ç¯„ä¾‹: {[len(x) for x in speaker_data[:5]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Speaker Recognition Dataset ==========\n",
    "\n",
    "class SpeakerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Speaker Recognition Dataset\n",
    "    \n",
    "    è™•ç†è®Šé•·åºåˆ—\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, labels, max_len=128):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        # Padding æˆ–æˆªæ–·\n",
    "        if len(x) > self.max_len:\n",
    "            x = x[:self.max_len]\n",
    "        elif len(x) < self.max_len:\n",
    "            pad_len = self.max_len - len(x)\n",
    "            x = np.pad(x, ((0, pad_len), (0, 0)), mode='constant')\n",
    "        \n",
    "        return torch.FloatTensor(x), torch.LongTensor([y]).squeeze()\n",
    "\n",
    "# å»ºç«‹ Dataset\n",
    "dataset = SpeakerDataset(speaker_data, speaker_labels, max_len=128)\n",
    "print(f\"Dataset å¤§å°: {len(dataset)}\")\n",
    "print(f\"æ¨£æœ¬å½¢ç‹€: {dataset[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Speaker Recognition Model ==========\n",
    "\n",
    "class SpeakerRecognitionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Speaker Recognition æ¨¡å‹ (HW4 æ¶æ§‹)\n",
    "    \n",
    "    æ¶æ§‹:\n",
    "    1. ç·šæ€§æŠ•å½±å±¤\n",
    "    2. Positional Encoding\n",
    "    3. Transformer Encoder å±¤\n",
    "    4. å¹³å‡æ± åŒ–\n",
    "    5. åˆ†é¡å™¨\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=40,\n",
    "        embed_dim=256,\n",
    "        num_heads=4,\n",
    "        num_layers=3,\n",
    "        ff_dim=1024,\n",
    "        num_speakers=600,\n",
    "        dropout=0.1,\n",
    "        max_len=128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # è¼¸å…¥æŠ•å½±\n",
    "        self.input_projection = nn.Linear(input_dim, embed_dim)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.pos_encoding = self._create_positional_encoding(max_len, embed_dim)\n",
    "        \n",
    "        # Transformer Encoder å±¤\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # åˆ†é¡å™¨\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embed_dim, num_speakers)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def _create_positional_encoding(self, max_len, embed_dim):\n",
    "        \"\"\"å»ºç«‹ Sinusoidal Positional Encoding\"\"\"\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        return nn.Parameter(pe.unsqueeze(0), requires_grad=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_dim)\n",
    "        \n",
    "        # è¼¸å…¥æŠ•å½±\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # åŠ å…¥ä½ç½®ç·¨ç¢¼\n",
    "        x = x + self.pos_encoding[:, :x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # é€šé Encoder å±¤\n",
    "        attention_weights = []\n",
    "        for layer in self.encoder_layers:\n",
    "            x, attn = layer(x)\n",
    "            attention_weights.append(attn)\n",
    "        \n",
    "        # å¹³å‡æ± åŒ–\n",
    "        x = x.mean(dim=1)  # (batch, embed_dim)\n",
    "        \n",
    "        # åˆ†é¡\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "# æ¸¬è©¦æ¨¡å‹\n",
    "model = SpeakerRecognitionModel(\n",
    "    input_dim=40,\n",
    "    embed_dim=256,\n",
    "    num_heads=4,\n",
    "    num_layers=3,\n",
    "    num_speakers=600\n",
    ")\n",
    "\n",
    "x = torch.randn(4, 128, 40)  # (batch, seq_len, feat_dim)\n",
    "logits, attns = model(x)\n",
    "\n",
    "print(f\"è¼¸å…¥: {x.shape}\")\n",
    "print(f\"è¼¸å‡º: {logits.shape}\")\n",
    "print(f\"åƒæ•¸é‡: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è¨“ç·´ Speaker Recognition æ¨¡å‹ ==========\n",
    "\n",
    "# æº–å‚™è³‡æ–™\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# è¨“ç·´é…ç½®\n",
    "model = SpeakerRecognitionModel(\n",
    "    input_dim=40,\n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    num_layers=2,\n",
    "    num_speakers=600\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# è¨“ç·´\n",
    "history = {'train_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(20):\n",
    "    # è¨“ç·´\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(x_batch)\n",
    "        loss = criterion(logits, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    # é©—è­‰\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            logits, _ = model(x_batch)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "    \n",
    "    val_acc = correct / total\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss={train_loss:.4f}, Val Acc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Attention è®Šé«”\n",
    "\n",
    "### å¸¸è¦‹ Attention è®Šé«”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Additive (Bahdanau) Attention ==========\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Additive Attention (Bahdanau, 2014)\n",
    "    \n",
    "    score = v^T tanh(W_1 Q + W_2 K)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, query_dim, key_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(query_dim, hidden_dim, bias=False)\n",
    "        self.W_k = nn.Linear(key_dim, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, query, key, value):\n",
    "        # query: (batch, q_len, query_dim)\n",
    "        # key: (batch, k_len, key_dim)\n",
    "        \n",
    "        q = self.W_q(query)  # (batch, q_len, hidden)\n",
    "        k = self.W_k(key)    # (batch, k_len, hidden)\n",
    "        \n",
    "        # æ“´å±•ç¶­åº¦ä»¥é€²è¡Œå»£æ’­\n",
    "        q = q.unsqueeze(2)  # (batch, q_len, 1, hidden)\n",
    "        k = k.unsqueeze(1)  # (batch, 1, k_len, hidden)\n",
    "        \n",
    "        scores = self.v(torch.tanh(q + k)).squeeze(-1)  # (batch, q_len, k_len)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# æ¸¬è©¦\n",
    "additive_attn = AdditiveAttention(query_dim=64, key_dim=64, hidden_dim=32)\n",
    "q = torch.randn(2, 5, 64)\n",
    "k = torch.randn(2, 10, 64)\n",
    "v = torch.randn(2, 10, 64)\n",
    "\n",
    "out, attn = additive_attn(q, k, v)\n",
    "print(f\"Additive Attention è¼¸å‡º: {out.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Relative Positional Attention ==========\n",
    "\n",
    "class RelativePositionalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Relative Positional Attention\n",
    "    \n",
    "    åœ¨è¨ˆç®— attention score æ™‚è€ƒæ…®ä½ç½®ä¹‹é–“çš„ç›¸å°è·é›¢\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, max_relative_position=32):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # ç›¸å°ä½ç½®åµŒå…¥\n",
    "        self.max_relative_position = max_relative_position\n",
    "        num_embeddings = 2 * max_relative_position + 1\n",
    "        self.relative_position_k = nn.Embedding(num_embeddings, self.head_dim)\n",
    "        self.relative_position_v = nn.Embedding(num_embeddings, self.head_dim)\n",
    "    \n",
    "    def _get_relative_positions(self, seq_len):\n",
    "        \"\"\"ç”Ÿæˆç›¸å°ä½ç½®ç´¢å¼•\"\"\"\n",
    "        positions = torch.arange(seq_len).unsqueeze(0) - torch.arange(seq_len).unsqueeze(1)\n",
    "        positions = positions.clamp(-self.max_relative_position, self.max_relative_position)\n",
    "        positions = positions + self.max_relative_position\n",
    "        return positions\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # æ¨™æº– attention score\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # ç›¸å°ä½ç½®è²¢ç»\n",
    "        relative_positions = self._get_relative_positions(seq_len).to(x.device)\n",
    "        rel_k = self.relative_position_k(relative_positions)  # (seq, seq, head_dim)\n",
    "        \n",
    "        # åŠ å…¥ç›¸å°ä½ç½® score\n",
    "        rel_scores = torch.einsum('bhqd,qkd->bhqk', Q, rel_k) / math.sqrt(self.head_dim)\n",
    "        scores = scores + rel_scores\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# æ¸¬è©¦\n",
    "rel_attn = RelativePositionalAttention(embed_dim=64, num_heads=4)\n",
    "x = torch.randn(2, 16, 64)\n",
    "out, attn = rel_attn(x)\n",
    "print(f\"Relative Positional Attention è¼¸å‡º: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¯ ç¸½çµ\n",
    "\n",
    "### Attention æ©Ÿåˆ¶æ¯”è¼ƒ\n",
    "\n",
    "| é¡å‹ | å…¬å¼ | ç‰¹é» |\n",
    "|------|------|------|\n",
    "| **Dot-Product** | $QK^T/\\sqrt{d}$ | è¨ˆç®—æ•ˆç‡é«˜ |\n",
    "| **Additive** | $v^T\\tanh(W_qQ+W_kK)$ | æ›´æœ‰è¡¨é”åŠ› |\n",
    "| **Multi-Head** | å¤šå€‹ä¸¦è¡Œ attention | æ•æ‰ä¸åŒæ¨¡å¼ |\n",
    "| **Relative Position** | åŠ å…¥ç›¸å°ä½ç½®è³‡è¨Š | æ›´å¥½çš„åºåˆ—å»ºæ¨¡ |\n",
    "\n",
    "### Self-Attention å„ªç¼ºé»\n",
    "\n",
    "| å„ªé» | ç¼ºé» |\n",
    "|------|------|\n",
    "| å¯å¹³è¡Œè¨ˆç®— | O(nÂ²) è¨˜æ†¶é«”è¤‡é›œåº¦ |\n",
    "| é•·è·é›¢ä¾è³´ | ç¼ºä¹ä½ç½®è³‡è¨Š |\n",
    "| å¯è§£é‡‹æ€§å¼· | å°å°è³‡æ–™é›†æ˜“éæ“¬åˆ |\n",
    "\n",
    "### æå®æ¯… HW4 æŠ€å·§\n",
    "\n",
    "```\n",
    "Strong Baseline é”æˆæ–¹æ³•:\n",
    "1. Conformer æ¶æ§‹ï¼ˆçµåˆ CNN å’Œ Transformerï¼‰\n",
    "2. é©ç•¶çš„ embed_dim å’Œ num_heads\n",
    "3. è³‡æ–™å¢å¼·ï¼ˆSpecAugmentï¼‰\n",
    "4. å­¸ç¿’ç‡èª¿åº¦\n",
    "5. Label Smoothing\n",
    "```\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "å‰å¾€ `sequence_modeling/transformer.ipynb` å­¸ç¿’å®Œæ•´çš„ Transformer æ¶æ§‹ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## ç·´ç¿’é¡Œ\n\n### ç·´ç¿’ 1: å¯¦ä½œ Masked Self-Attention\n\n**ç›®æ¨™**: å¯¦ä½œç”¨æ–¼ Decoder çš„ Causal Maskï¼Œé˜²æ­¢çœ‹åˆ°æœªä¾†è³‡è¨Š",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ç·´ç¿’ 1: Masked Self-Attention (Causal Attention)\n\ndef create_causal_mask(seq_len):\n    \"\"\"\n    å‰µå»ºå› æœé®ç½©ï¼Œé˜²æ­¢çœ‹åˆ°æœªä¾†è³‡è¨Š\n    \n    è¿”å›ä¸€å€‹ä¸‹ä¸‰è§’çŸ©é™£ï¼š\n    [[1, 0, 0, 0],\n     [1, 1, 0, 0],\n     [1, 1, 1, 0],\n     [1, 1, 1, 1]]\n    \"\"\"\n    mask = torch.tril(torch.ones(seq_len, seq_len))\n    return mask\n\nclass MaskedSelfAttention(nn.Module):\n    \"\"\"Causal Self-Attention for Decoder\"\"\"\n    \n    def __init__(self, embed_dim, num_heads, dropout=0.1):\n        super().__init__()\n        self.mha = MultiHeadAttention(embed_dim, num_heads, dropout)\n    \n    def forward(self, x):\n        seq_len = x.size(1)\n        # å‰µå»ºå› æœé®ç½©ä¸¦æ“´å±•ç‚º (1, 1, seq, seq) ä»¥ä¾¿å»£æ’­\n        causal_mask = create_causal_mask(seq_len).to(x.device)\n        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n        return self.mha(x, mask=causal_mask)\n\n# æ¸¬è©¦\nmasked_attn = MaskedSelfAttention(embed_dim=64, num_heads=4)\nx = torch.randn(2, 8, 64)  # (batch, seq_len, embed_dim)\noutput, attn_weights = masked_attn(x)\n\nprint(f\"è¼¸å…¥: {x.shape}\")\nprint(f\"è¼¸å‡º: {output.shape}\")\n\n# è¦–è¦ºåŒ–å› æœæ³¨æ„åŠ›æ¬Šé‡\nplt.figure(figsize=(10, 4))\n\nplt.subplot(1, 2, 1)\nplt.imshow(create_causal_mask(8).numpy(), cmap='Blues')\nplt.title('Causal Mask')\nplt.xlabel('Key Position')\nplt.ylabel('Query Position')\n\nplt.subplot(1, 2, 2)\nplt.imshow(attn_weights[0, 0].detach().numpy(), cmap='viridis')\nplt.title('Masked Attention Weights (Head 1)')\nplt.xlabel('Key Position')\nplt.ylabel('Query Position')\nplt.colorbar()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nè§€å¯Ÿï¼šæ³¨æ„åŠ›æ¬Šé‡åªé—œæ³¨ç•¶å‰ä½ç½®åŠä¹‹å‰çš„ä½ç½®ï¼ˆä¸‹ä¸‰è§’å½¢å¼ï¼‰\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### ç·´ç¿’ 2: æ¯”è¼ƒä¸åŒ Attention é ­æ•¸çš„æ•ˆæœ\n\n**ç›®æ¨™**: å¯¦é©—ä¸åŒé ­æ•¸å°æ¨¡å‹æ€§èƒ½çš„å½±éŸ¿",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ç·´ç¿’ 2: æ¯”è¼ƒä¸åŒé ­æ•¸\n\ndef experiment_num_heads(embed_dim=64, head_options=[1, 2, 4, 8]):\n    \"\"\"æ¯”è¼ƒä¸åŒ attention é ­æ•¸çš„æ•ˆæœ\"\"\"\n    \n    # å»ºç«‹ç°¡å–®çš„åºåˆ—åˆ†é¡ä»»å‹™\n    torch.manual_seed(42)\n    n_samples = 500\n    seq_len = 32\n    n_classes = 10\n    \n    # åˆæˆè³‡æ–™\n    X = torch.randn(n_samples, seq_len, embed_dim)\n    y = torch.randint(0, n_classes, (n_samples,))\n    \n    # åˆ†å‰²\n    train_X, val_X = X[:400], X[400:]\n    train_y, val_y = y[:400], y[400:]\n    \n    results = []\n    \n    for num_heads in head_options:\n        print(f\"\\n=== num_heads = {num_heads} ===\")\n        \n        # å»ºç«‹æ¨¡å‹\n        model = nn.Sequential(\n            TransformerEncoderBlock(embed_dim, num_heads, embed_dim * 4, dropout=0.1),\n            nn.AdaptiveAvgPool1d(1),\n        )\n        classifier = nn.Linear(embed_dim, n_classes)\n        \n        optimizer = torch.optim.Adam(list(model.parameters()) + list(classifier.parameters()), lr=1e-3)\n        criterion = nn.CrossEntropyLoss()\n        \n        # è¨“ç·´\n        for epoch in range(30):\n            model.train()\n            out, _ = model[0](train_X)\n            out = out.mean(dim=1)  # å¹³å‡æ± åŒ–\n            loss = criterion(classifier(out), train_y)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        \n        # è©•ä¼°\n        model.eval()\n        with torch.no_grad():\n            out, _ = model[0](val_X)\n            out = out.mean(dim=1)\n            pred = classifier(out).argmax(dim=1)\n            acc = (pred == val_y).float().mean().item()\n        \n        results.append({'num_heads': num_heads, 'accuracy': acc})\n        print(f\"  Val Accuracy: {acc:.4f}\")\n    \n    return results\n\n# åŸ·è¡Œå¯¦é©—\nresults = experiment_num_heads()\n\n# è¦–è¦ºåŒ–\nplt.figure(figsize=(8, 5))\nplt.bar([str(r['num_heads']) for r in results], [r['accuracy'] for r in results])\nplt.xlabel('Number of Attention Heads')\nplt.ylabel('Validation Accuracy')\nplt.title('Effect of Number of Attention Heads')\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"\\nçµè«–ï¼šé©ç•¶çš„é ­æ•¸å¯ä»¥è®“æ¨¡å‹æ•æ‰ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}