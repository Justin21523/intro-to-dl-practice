{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深入理解 Transformer (Understanding Transformer Internals)\n",
    "\n",
    "本 notebook 對應李宏毅老師 2025 Spring ML HW3，深入探討 Transformer 的內部機制。\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 深入理解 Attention 機制\n",
    "2. 視覺化 Attention Patterns\n",
    "3. 了解 KV Cache 加速推理\n",
    "4. 學習各種位置編碼方法\n",
    "5. 探索 Transformer 的可解釋性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Self-Attention 深入解析\n",
    "\n",
    "### 1.1 Scaled Dot-Product Attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    query: torch.Tensor,  # [batch, heads, seq_q, d_k]\n",
    "    key: torch.Tensor,    # [batch, heads, seq_k, d_k]\n",
    "    value: torch.Tensor,  # [batch, heads, seq_v, d_v]\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    "    return_attention_weights: bool = False\n",
    ") -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    # 計算 attention scores: (Q @ K^T) / sqrt(d_k)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # 應用 mask\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 應用到 Value\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    if return_attention_weights:\n",
    "        return output, attention_weights\n",
    "    return output, None\n",
    "\n",
    "\n",
    "# 測試\n",
    "batch, heads, seq_len, d_k = 2, 4, 8, 64\n",
    "Q = torch.randn(batch, heads, seq_len, d_k)\n",
    "K = torch.randn(batch, heads, seq_len, d_k)\n",
    "V = torch.randn(batch, heads, seq_len, d_k)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V, return_attention_weights=True)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"Attention weights sum (should be 1): {attn_weights[0, 0, 0].sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化 Attention Pattern\n",
    "def visualize_attention(attention_weights, tokens=None, title=\"Attention Pattern\"):\n",
    "    \"\"\"視覺化 attention 矩陣\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 取第一個 batch, 第一個 head\n",
    "    attn = attention_weights[0, 0].detach().numpy()\n",
    "    \n",
    "    # Heatmap\n",
    "    im = axes[0].imshow(attn, cmap='Blues')\n",
    "    axes[0].set_title('Attention Weights')\n",
    "    axes[0].set_xlabel('Key Position')\n",
    "    axes[0].set_ylabel('Query Position')\n",
    "    plt.colorbar(im, ax=axes[0])\n",
    "    \n",
    "    # Causal mask attention pattern\n",
    "    seq_len = attn.shape[0]\n",
    "    causal_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    masked_scores = torch.randn(seq_len, seq_len)\n",
    "    masked_scores = masked_scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "    causal_attn = F.softmax(masked_scores, dim=-1).numpy()\n",
    "    \n",
    "    im2 = axes[1].imshow(causal_attn, cmap='Blues')\n",
    "    axes[1].set_title('Causal (Decoder) Attention Pattern')\n",
    "    axes[1].set_xlabel('Key Position')\n",
    "    axes[1].set_ylabel('Query Position')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention(attn_weights, title=\"Self-Attention Visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention with detailed implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # 投影層\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention_weights = None  # 儲存以便視覺化\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 線性投影\n",
    "        Q = self.W_q(query)  # [batch, seq, d_model]\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # 重塑為多頭格式\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # 現在: [batch, heads, seq, d_k]\n",
    "        \n",
    "        # Attention\n",
    "        output, attn_weights = scaled_dot_product_attention(\n",
    "            Q, K, V, mask, return_attention_weights=True\n",
    "        )\n",
    "        self.attention_weights = attn_weights\n",
    "        \n",
    "        # 合併多頭\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 最終投影\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 測試\n",
    "mha = MultiHeadAttention(d_model=256, num_heads=8)\n",
    "x = torch.randn(2, 10, 256)\n",
    "output = mha(x, x, x)\n",
    "print(f\"MHA output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. KV Cache 加速推理\n",
    "\n",
    "在自回歸生成時，KV Cache 避免重複計算已生成 token 的 Key 和 Value。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWithKVCache(nn.Module):\n",
    "    \"\"\"\n",
    "    帶 KV Cache 的 Multi-Head Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x, kv_cache=None, use_cache=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 輸入 [batch, seq, d_model]\n",
    "            kv_cache: (cached_k, cached_v) or None\n",
    "            use_cache: 是否返回更新的 cache\n",
    "        \n",
    "        Returns:\n",
    "            output, (new_k_cache, new_v_cache) if use_cache else output\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        \n",
    "        # 計算 Q, K, V\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 如果有 cache，concatenate\n",
    "        if kv_cache is not None:\n",
    "            cached_k, cached_v = kv_cache\n",
    "            K = torch.cat([cached_k, K], dim=2)\n",
    "            V = torch.cat([cached_v, V], dim=2)\n",
    "        \n",
    "        # Attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # 合併多頭\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        if use_cache:\n",
    "            return output, (K, V)\n",
    "        return output\n",
    "\n",
    "# 示範 KV Cache\n",
    "print(\"KV Cache 示範\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "mha_cache = MultiHeadAttentionWithKVCache(d_model=256, num_heads=8)\n",
    "\n",
    "# 第一個 token (無 cache)\n",
    "x1 = torch.randn(1, 1, 256)  # [batch=1, seq=1, d_model]\n",
    "out1, cache = mha_cache(x1, kv_cache=None, use_cache=True)\n",
    "print(f\"Token 1 - Output: {out1.shape}, Cache K: {cache[0].shape}\")\n",
    "\n",
    "# 第二個 token (使用 cache)\n",
    "x2 = torch.randn(1, 1, 256)\n",
    "out2, cache = mha_cache(x2, kv_cache=cache, use_cache=True)\n",
    "print(f\"Token 2 - Output: {out2.shape}, Cache K: {cache[0].shape}\")\n",
    "\n",
    "# 第三個 token\n",
    "x3 = torch.randn(1, 1, 256)\n",
    "out3, cache = mha_cache(x3, kv_cache=cache, use_cache=True)\n",
    "print(f\"Token 3 - Output: {out3.shape}, Cache K: {cache[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 位置編碼 (Positional Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"原始 Transformer 的正弦位置編碼\"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                             (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"RoPE (Rotary Position Embedding) - 用於 LLaMA 等現代模型\"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, base: int = 10000):\n",
    "        super().__init__()\n",
    "        \n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # 預計算\n",
    "        t = torch.arange(max_len).float()\n",
    "        freqs = torch.einsum('i,j->ij', t, inv_freq)\n",
    "        emb = torch.cat([freqs, freqs], dim=-1)\n",
    "        self.register_buffer('cos_cache', emb.cos())\n",
    "        self.register_buffer('sin_cache', emb.sin())\n",
    "    \n",
    "    def _rotate_half(self, x):\n",
    "        x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "        return torch.cat([-x2, x1], dim=-1)\n",
    "    \n",
    "    def forward(self, q, k, positions=None):\n",
    "        seq_len = q.size(2)\n",
    "        cos = self.cos_cache[:seq_len].unsqueeze(0).unsqueeze(0)\n",
    "        sin = self.sin_cache[:seq_len].unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        q_embed = (q * cos) + (self._rotate_half(q) * sin)\n",
    "        k_embed = (k * cos) + (self._rotate_half(k) * sin)\n",
    "        \n",
    "        return q_embed, k_embed\n",
    "\n",
    "\n",
    "# 視覺化位置編碼\n",
    "def visualize_positional_encodings():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 正弦位置編碼\n",
    "    pe = SinusoidalPositionalEncoding(d_model=128, max_len=100)\n",
    "    pe_values = pe.pe[0, :50, :64].numpy()\n",
    "    \n",
    "    im1 = axes[0].imshow(pe_values, cmap='RdBu', aspect='auto')\n",
    "    axes[0].set_title('Sinusoidal Positional Encoding')\n",
    "    axes[0].set_xlabel('Dimension')\n",
    "    axes[0].set_ylabel('Position')\n",
    "    plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    # RoPE 旋轉角度\n",
    "    rope = RotaryPositionalEmbedding(d_model=64, max_len=100)\n",
    "    angles = rope.sin_cache[:50, :32].numpy()\n",
    "    \n",
    "    im2 = axes[1].imshow(angles, cmap='RdBu', aspect='auto')\n",
    "    axes[1].set_title('RoPE Sin Component')\n",
    "    axes[1].set_xlabel('Dimension')\n",
    "    axes[1].set_ylabel('Position')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_positional_encodings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 練習題\n",
    "\n",
    "### 練習 1：實作 Flash Attention 的簡化版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1：理解 Flash Attention 的分塊計算概念\n",
    "def chunked_attention(Q, K, V, chunk_size=32):\n",
    "    \"\"\"\n",
    "    TODO: 實作分塊 attention（Flash Attention 的簡化概念）\n",
    "    \n",
    "    Flash Attention 的核心思想：\n",
    "    1. 將 Q, K, V 分成小塊\n",
    "    2. 逐塊計算 attention\n",
    "    3. 正確處理 softmax 的數值穩定性\n",
    "    \n",
    "    提示：需要追蹤每個塊的 max 值和 sum 值來正確計算 softmax\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "print(\"練習 1：實作 chunked_attention 函數\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：Attention 可解釋性分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2：分析 attention patterns\n",
    "def analyze_attention_patterns(attention_weights, tokens):\n",
    "    \"\"\"\n",
    "    TODO: 分析不同層和頭的 attention 模式\n",
    "    \n",
    "    分析項目：\n",
    "    1. 找出「全局注意力」的頭（attend to 特定位置如 [CLS]）\n",
    "    2. 找出「局部注意力」的頭（只關注附近 token）\n",
    "    3. 找出「語法相關」的頭（可能關注相關詞彙）\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "print(\"練習 2：實作 analyze_attention_patterns 函數\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 總結\n",
    "\n",
    "### 關鍵概念\n",
    "\n",
    "| 概念 | 說明 |\n",
    "|------|------|\n",
    "| Self-Attention | 序列中每個位置可以關注所有位置 |\n",
    "| Multi-Head | 多個 attention 頭學習不同的關注模式 |\n",
    "| KV Cache | 快取已計算的 K, V 加速自回歸生成 |\n",
    "| 位置編碼 | 為 attention 提供位置資訊 |\n",
    "| RoPE | 相對位置編碼，支援長序列外推 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"深入理解 Transformer - 學習完成！\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n你已經學會：\")\n",
    "print(\"✓ Attention 機制的深入理解\")\n",
    "print(\"✓ Multi-Head Attention 實作\")\n",
    "print(\"✓ KV Cache 加速推理\")\n",
    "print(\"✓ 各種位置編碼方法\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
