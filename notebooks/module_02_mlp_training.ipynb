{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2：多層感知機 (MLP) 與訓練技巧\n",
    "\n",
    "## 學習目標\n",
    "\n",
    "1. 理解感知機和多層感知機 (MLP) 的結構\n",
    "2. 掌握各種激活函數及其特性\n",
    "3. 學會使用不同的優化器 (SGD, Adam)\n",
    "4. 理解過擬合與正則化技術 (Dropout, BatchNorm, L2)\n",
    "5. 實作：用 MLP 分類 MNIST 手寫數字\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 固定隨機種子\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1：從感知機到多層感知機\n",
    "\n",
    "### 1.1 感知機 (Perceptron)\n",
    "\n",
    "**結構：** 最簡單的神經元模型\n",
    "\n",
    "$$y = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b)$$\n",
    "\n",
    "1. 輸入 $\\mathbf{x}$ 和權重 $\\mathbf{w}$ 做內積\n",
    "2. 加上偏置 $b$\n",
    "3. 通過激活函數 $\\sigma$\n",
    "\n",
    "**問題：** 單層感知機只能學習**線性可分**的問題（例如無法學習 XOR）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化：線性可分 vs 非線性可分\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# AND 問題（線性可分）\n",
    "ax = axes[0]\n",
    "ax.scatter([0, 0, 1], [0, 1, 0], c='red', s=100, label='0')\n",
    "ax.scatter([1], [1], c='blue', s=100, label='1')\n",
    "ax.plot([0, 1.5], [1.5, 0], 'g--', linewidth=2)  # 分隔線\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_title('AND (Linearly Separable)')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# OR 問題（線性可分）\n",
    "ax = axes[1]\n",
    "ax.scatter([0], [0], c='red', s=100, label='0')\n",
    "ax.scatter([0, 1, 1], [1, 0, 1], c='blue', s=100, label='1')\n",
    "ax.plot([-0.5, 1], [0.5, -0.5], 'g--', linewidth=2)\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_title('OR (Linearly Separable)')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# XOR 問題（非線性可分）\n",
    "ax = axes[2]\n",
    "ax.scatter([0, 1], [0, 1], c='red', s=100, label='0')\n",
    "ax.scatter([0, 1], [1, 0], c='blue', s=100, label='1')\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_title('XOR (NOT Linearly Separable!)')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "ax.text(0.5, -0.3, 'No single line can separate!', ha='center', fontsize=10, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 多層感知機 (MLP / Feedforward Neural Network)\n",
    "\n",
    "**解決方案：** 堆疊多層神經元，並在層之間加入**非線性激活函數**\n",
    "\n",
    "```\n",
    "Input → [Linear + Activation] → [Linear + Activation] → ... → Output\n",
    "         \\____Hidden Layer____/   \\____Hidden Layer____/\n",
    "```\n",
    "\n",
    "**為什麼需要非線性激活？**\n",
    "- 如果沒有激活函數，多層線性層 = 一層線性層（線性的組合還是線性）\n",
    "- 非線性激活讓網路能學習複雜的非線性關係"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 MLP 解決 XOR 問題\n",
    "\n",
    "# XOR 資料\n",
    "X_xor = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "y_xor = torch.tensor([[0.], [1.], [1.], [0.]])\n",
    "\n",
    "# 定義 MLP\n",
    "class XOR_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 4)   # 2 -> 4\n",
    "        self.layer2 = nn.Linear(4, 1)   # 4 -> 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))  # 非線性激活！\n",
    "        x = torch.sigmoid(self.layer2(x))  # 輸出 0-1 之間\n",
    "        return x\n",
    "\n",
    "# 訓練\n",
    "model = XOR_MLP()\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "losses = []\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_xor)\n",
    "    loss = criterion(output, y_xor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "# 結果\n",
    "print(\"XOR 問題結果：\")\n",
    "print(f\"Input: {X_xor.tolist()}\")\n",
    "print(f\"Target: {y_xor.squeeze().tolist()}\")\n",
    "print(f\"Prediction: {model(X_xor).squeeze().detach().round().tolist()}\")\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('XOR Learning Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2：激活函數 (Activation Functions)\n",
    "\n",
    "### 2.1 為什麼需要激活函數？\n",
    "\n",
    "1. **引入非線性**：讓網路能學習複雜的函數\n",
    "2. **控制輸出範圍**：例如 sigmoid 輸出 (0, 1)，適合機率\n",
    "\n",
    "### 2.2 常用激活函數比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函數視覺化\n",
    "\n",
    "x = torch.linspace(-5, 5, 200)\n",
    "\n",
    "activations = {\n",
    "    'Sigmoid': torch.sigmoid(x),\n",
    "    'Tanh': torch.tanh(x),\n",
    "    'ReLU': torch.relu(x),\n",
    "    'LeakyReLU': F.leaky_relu(x, 0.1),\n",
    "    'GELU': F.gelu(x),\n",
    "    'SiLU/Swish': F.silu(x),\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, y) in enumerate(activations.items()):\n",
    "    ax = axes[i]\n",
    "    ax.plot(x.numpy(), y.numpy(), linewidth=2)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.set_title(name, fontsize=14)\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 各激活函數的特點\n",
    "\n",
    "| 激活函數 | 公式 | 輸出範圍 | 優點 | 缺點 |\n",
    "|---------|------|---------|------|------|\n",
    "| **Sigmoid** | $\\frac{1}{1+e^{-x}}$ | (0, 1) | 輸出可解釋為機率 | 梯度消失、非零中心 |\n",
    "| **Tanh** | $\\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (-1, 1) | 零中心 | 梯度消失 |\n",
    "| **ReLU** | $\\max(0, x)$ | [0, ∞) | 計算快、緩解梯度消失 | Dead ReLU 問題 |\n",
    "| **LeakyReLU** | $\\max(0.01x, x)$ | (-∞, ∞) | 解決 Dead ReLU | 多一個超參數 |\n",
    "| **GELU** | $x \\cdot \\Phi(x)$ | ≈(-0.17, ∞) | Transformer 常用 | 計算較慢 |\n",
    "| **SiLU/Swish** | $x \\cdot \\sigma(x)$ | ≈(-0.28, ∞) | 平滑、效果好 | 計算較慢 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度消失問題示範\n",
    "\n",
    "# Sigmoid 的導數\n",
    "def sigmoid_derivative(x):\n",
    "    s = torch.sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# ReLU 的導數\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).float()\n",
    "\n",
    "x = torch.linspace(-5, 5, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Sigmoid 和其導數\n",
    "ax = axes[0]\n",
    "ax.plot(x, torch.sigmoid(x), label='Sigmoid', linewidth=2)\n",
    "ax.plot(x, sigmoid_derivative(x), label='Sigmoid Derivative', linewidth=2)\n",
    "ax.axhline(y=0.25, color='r', linestyle='--', alpha=0.5)\n",
    "ax.set_title('Sigmoid: Max derivative = 0.25')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# ReLU 和其導數\n",
    "ax = axes[1]\n",
    "ax.plot(x, torch.relu(x), label='ReLU', linewidth=2)\n",
    "ax.plot(x, relu_derivative(x), label='ReLU Derivative', linewidth=2)\n",
    "ax.set_title('ReLU: Derivative is 0 or 1')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"梯度消失問題：\")\n",
    "print(\"- Sigmoid 的最大導數只有 0.25\")\n",
    "print(\"- 經過多層後：0.25^10 ≈ 0.0000009，梯度幾乎消失！\")\n",
    "print(\"- ReLU 的導數是 0 或 1，不會越乘越小\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3：損失函數 (Loss Functions)\n",
    "\n",
    "### 3.1 回歸任務：MSE Loss\n",
    "\n",
    "$$L = \\frac{1}{n}\\sum_i(y_i - \\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE Loss 範例\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "y_pred = torch.tensor([2.5, 0.0, 2.1, 1.8])\n",
    "y_true = torch.tensor([3.0, -0.5, 2.0, 2.0])\n",
    "\n",
    "loss = mse_loss(y_pred, y_true)\n",
    "print(f\"MSE Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 手算驗證\n",
    "manual = ((y_pred - y_true) ** 2).mean()\n",
    "print(f\"手算: {manual.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 分類任務：Cross-Entropy Loss\n",
    "\n",
    "**二元分類：** `nn.BCELoss` 或 `nn.BCEWithLogitsLoss`\n",
    "\n",
    "**多類別分類：** `nn.CrossEntropyLoss`（自動包含 softmax）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss 範例\n",
    "\n",
    "# 注意：CrossEntropyLoss 的輸入是 logits（未經 softmax），不是機率！\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# 3 個樣本，4 個類別\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1, 0.5],   # 預測 class 0\n",
    "                       [0.5, 2.5, 0.3, 0.2],   # 預測 class 1\n",
    "                       [0.1, 0.2, 0.3, 3.0]])  # 預測 class 3\n",
    "\n",
    "targets = torch.tensor([0, 1, 3])  # 真實標籤\n",
    "\n",
    "loss = ce_loss(logits, targets)\n",
    "print(f\"Cross-Entropy Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 看看 softmax 後的機率\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "print(f\"\\nSoftmax probabilities:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Sample {i}: {probs[i].tolist()} -> pred={probs[i].argmax()}, true={targets[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4：優化器 (Optimizers)\n",
    "\n",
    "### 4.1 SGD (Stochastic Gradient Descent)\n",
    "\n",
    "最基本的優化器：$\\theta_{t+1} = \\theta_t - \\eta \\nabla L$\n",
    "\n",
    "**加入 Momentum：** 讓更新有「慣性」，加速收斂\n",
    "\n",
    "$$v_t = \\gamma v_{t-1} + \\eta \\nabla L$$\n",
    "$$\\theta_{t+1} = \\theta_t - v_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比較不同優化器\n",
    "\n",
    "def train_with_optimizer(optimizer_fn, num_epochs=100):\n",
    "    \"\"\"用指定優化器訓練一個簡單模型\"\"\"\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 簡單的二次函數優化問題\n",
    "    # 最小化 f(x, y) = x^2 + 10*y^2\n",
    "    x = torch.tensor([5.0], requires_grad=True)\n",
    "    y = torch.tensor([5.0], requires_grad=True)\n",
    "    \n",
    "    optimizer = optimizer_fn([x, y])\n",
    "    \n",
    "    history = []\n",
    "    for _ in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        loss = x**2 + 10 * y**2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history.append((x.item(), y.item(), loss.item()))\n",
    "    \n",
    "    return history\n",
    "\n",
    "# 不同優化器\n",
    "optimizers = {\n",
    "    'SGD (lr=0.01)': lambda p: optim.SGD(p, lr=0.01),\n",
    "    'SGD + Momentum': lambda p: optim.SGD(p, lr=0.01, momentum=0.9),\n",
    "    'Adam': lambda p: optim.Adam(p, lr=0.1),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, opt_fn in optimizers.items():\n",
    "    results[name] = train_with_optimizer(opt_fn, num_epochs=100)\n",
    "\n",
    "# 視覺化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "for name, history in results.items():\n",
    "    losses = [h[2] for h in history]\n",
    "    ax.plot(losses, label=name, linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss Curves')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True)\n",
    "\n",
    "# Optimization paths\n",
    "ax = axes[1]\n",
    "for name, history in results.items():\n",
    "    xs = [h[0] for h in history]\n",
    "    ys = [h[1] for h in history]\n",
    "    ax.plot(xs, ys, 'o-', label=name, markersize=3, alpha=0.7)\n",
    "ax.scatter([0], [0], c='red', s=100, zorder=5, label='Optimum')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Optimization Paths')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Adam (Adaptive Moment Estimation)\n",
    "\n",
    "**特點：**\n",
    "- 結合 Momentum 和 RMSprop\n",
    "- 為每個參數自動調整學習率\n",
    "- 通常是預設首選優化器\n",
    "\n",
    "**超參數：**\n",
    "- `lr`：學習率，預設 0.001\n",
    "- `betas`：(β1, β2)，預設 (0.9, 0.999)\n",
    "- `weight_decay`：L2 正則化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam 的常見用法\n",
    "model = nn.Linear(10, 2)\n",
    "\n",
    "# 基本用法\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 加上 weight decay (L2 正則化)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# AdamW (更好的 weight decay 實現)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "print(\"Adam optimizer created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5：過擬合與正則化\n",
    "\n",
    "### 5.1 什麼是過擬合 (Overfitting)？\n",
    "\n",
    "**現象：** 模型在訓練資料上表現很好，但在測試資料上表現差。\n",
    "\n",
    "**原因：** 模型「記住」了訓練資料的噪音，而不是學到真正的規律。\n",
    "\n",
    "**解決方案：**\n",
    "1. 更多資料\n",
    "2. 簡化模型\n",
    "3. 正則化技術（Dropout, L2, etc.）\n",
    "4. 早停 (Early Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 過擬合示範\n",
    "\n",
    "# 生成簡單的資料（加噪音）\n",
    "torch.manual_seed(42)\n",
    "n_samples = 50\n",
    "\n",
    "X = torch.linspace(-3, 3, n_samples).unsqueeze(1)\n",
    "y_true = torch.sin(X)  # 真實函數是 sin\n",
    "y = y_true + torch.randn_like(y_true) * 0.3  # 加噪音\n",
    "\n",
    "# 分成訓練和測試\n",
    "X_train, X_test = X[:35], X[35:]\n",
    "y_train, y_test = y[:35], y[35:]\n",
    "\n",
    "# 定義不同複雜度的模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_model(model, epochs=2000):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_losses, test_losses = [], []\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_train)\n",
    "        loss = criterion(pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Test\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_pred = model(X_test)\n",
    "            test_loss = criterion(test_pred, y_test)\n",
    "            test_losses.append(test_loss.item())\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "# 訓練兩個模型\n",
    "simple_model = SimpleModel()\n",
    "complex_model = ComplexModel()\n",
    "\n",
    "simple_train, simple_test = train_model(simple_model)\n",
    "complex_train, complex_test = train_model(complex_model)\n",
    "\n",
    "# 視覺化\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss curves for simple model\n",
    "ax = axes[0]\n",
    "ax.plot(simple_train, label='Train')\n",
    "ax.plot(simple_test, label='Test')\n",
    "ax.set_title('Simple Model (8 hidden units)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 0.5)\n",
    "\n",
    "# Loss curves for complex model\n",
    "ax = axes[1]\n",
    "ax.plot(complex_train, label='Train')\n",
    "ax.plot(complex_test, label='Test')\n",
    "ax.set_title('Complex Model (3x128 hidden units)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 0.5)\n",
    "\n",
    "# Predictions\n",
    "ax = axes[2]\n",
    "X_plot = torch.linspace(-3, 3, 100).unsqueeze(1)\n",
    "\n",
    "simple_model.eval()\n",
    "complex_model.eval()\n",
    "with torch.no_grad():\n",
    "    simple_pred = simple_model(X_plot)\n",
    "    complex_pred = complex_model(X_plot)\n",
    "\n",
    "ax.scatter(X_train, y_train, c='blue', alpha=0.5, label='Train data')\n",
    "ax.scatter(X_test, y_test, c='red', alpha=0.5, label='Test data')\n",
    "ax.plot(X_plot, torch.sin(X_plot), 'g--', label='True function', linewidth=2)\n",
    "ax.plot(X_plot, simple_pred, 'b-', label='Simple model', linewidth=2)\n",
    "ax.plot(X_plot, complex_pred, 'r-', label='Complex model', linewidth=2)\n",
    "ax.set_title('Model Predictions')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Simple Model - Train Loss: {simple_train[-1]:.4f}, Test Loss: {simple_test[-1]:.4f}\")\n",
    "print(f\"Complex Model - Train Loss: {complex_train[-1]:.4f}, Test Loss: {complex_test[-1]:.4f}\")\n",
    "print(\"\\n複雜模型的 train loss 更低，但 test loss 更高 = 過擬合！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Dropout\n",
    "\n",
    "**概念：** 訓練時隨機「關閉」一部分神經元\n",
    "\n",
    "**效果：**\n",
    "- 防止神經元之間產生「共依賴」\n",
    "- 類似於訓練多個子網路的 ensemble\n",
    "- 測試時不 dropout，但要調整輸出（PyTorch 自動處理）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout 示範\n",
    "\n",
    "# 模型加上 Dropout\n",
    "class ModelWithDropout(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),  # Dropout!\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 訓練帶 dropout 的模型\n",
    "dropout_model = ModelWithDropout(dropout_rate=0.5)\n",
    "dropout_train, dropout_test = train_model(dropout_model)\n",
    "\n",
    "# 比較\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(complex_train, label='Train (no dropout)')\n",
    "ax.plot(complex_test, label='Test (no dropout)')\n",
    "ax.plot(dropout_train, '--', label='Train (with dropout)')\n",
    "ax.plot(dropout_test, '--', label='Test (with dropout)')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Effect of Dropout')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 0.5)\n",
    "\n",
    "ax = axes[1]\n",
    "dropout_model.eval()\n",
    "with torch.no_grad():\n",
    "    dropout_pred = dropout_model(X_plot)\n",
    "\n",
    "ax.scatter(X_train, y_train, c='blue', alpha=0.5, label='Train')\n",
    "ax.scatter(X_test, y_test, c='red', alpha=0.5, label='Test')\n",
    "ax.plot(X_plot, torch.sin(X_plot), 'g--', label='True', linewidth=2)\n",
    "ax.plot(X_plot, complex_pred, 'r-', label='No dropout', linewidth=2, alpha=0.7)\n",
    "ax.plot(X_plot, dropout_pred, 'b-', label='With dropout', linewidth=2)\n",
    "ax.set_title('Predictions')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Without Dropout - Test Loss: {complex_test[-1]:.4f}\")\n",
    "print(f\"With Dropout - Test Loss: {dropout_test[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Batch Normalization\n",
    "\n",
    "**概念：** 在每一層之後，把輸出正規化成均值 0、標準差 1\n",
    "\n",
    "**效果：**\n",
    "- 加速訓練收斂\n",
    "- 允許使用更大的學習率\n",
    "- 有輕微的正則化效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Normalization 示範\n",
    "\n",
    "class ModelWithBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.BatchNorm1d(128),  # BatchNorm!\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 比較有無 BatchNorm 的收斂速度\n",
    "def train_model_fast(model, epochs=500, lr=0.1):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_losses = []\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_train)\n",
    "        loss = criterion(pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    return train_losses\n",
    "\n",
    "# 無 BatchNorm 的模型\n",
    "class ModelNoBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_no_bn = ModelNoBN()\n",
    "torch.manual_seed(42)\n",
    "model_with_bn = ModelWithBatchNorm()\n",
    "\n",
    "losses_no_bn = train_model_fast(model_no_bn, lr=0.01)  # 小學習率\n",
    "losses_with_bn = train_model_fast(model_with_bn, lr=0.1)  # 大學習率！\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses_no_bn, label='Without BatchNorm (lr=0.01)')\n",
    "plt.plot(losses_with_bn, label='With BatchNorm (lr=0.1)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('BatchNorm allows higher learning rate')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"BatchNorm 讓我們可以用 10 倍的學習率，收斂更快！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6：完整實作 - MNIST 手寫數字分類\n",
    "\n",
    "現在把所有知識組合起來，實作一個完整的 MNIST 分類器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入 MNIST 資料集\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST 的均值和標準差\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Image shape: {train_dataset[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看看資料長什麼樣\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Label: {label}')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 MLP 模型\n",
    "\n",
    "class MNIST_MLP(nn.Module):\n",
    "    def __init__(self, hidden_size=256, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.flatten = nn.Flatten()  # 28x28 -> 784\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            # 第一層\n",
    "            nn.Linear(784, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # 第二層\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            # 輸出層\n",
    "            nn.Linear(hidden_size, 10)  # 10 個數字類別\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.net(x)\n",
    "\n",
    "# 建立模型並移到 GPU\n",
    "model = MNIST_MLP(hidden_size=256, dropout_rate=0.2).to(device)\n",
    "\n",
    "# 印出模型結構\n",
    "print(model)\n",
    "\n",
    "# 計算參數數量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義訓練和評估函數\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / total, 100. * correct / total\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / total, 100. * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練！\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1:2d}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Final Test Accuracy: {history['test_acc'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化訓練過程\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "ax = axes[0]\n",
    "ax.plot(history['train_loss'], label='Train')\n",
    "ax.plot(history['test_loss'], label='Test')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training and Test Loss')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Accuracy\n",
    "ax = axes[1]\n",
    "ax.plot(history['train_acc'], label='Train')\n",
    "ax.plot(history['test_acc'], label='Test')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Training and Test Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看看一些預測結果\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 取一批測試資料\n",
    "images, labels = next(iter(test_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    _, predicted = outputs.max(1)\n",
    "\n",
    "# 顯示前 15 個\n",
    "fig, axes = plt.subplots(3, 5, figsize=(12, 8))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    img = images[i].cpu().squeeze()\n",
    "    pred = predicted[i].item()\n",
    "    true = labels[i].item()\n",
    "    prob = probs[i, pred].item()\n",
    "    \n",
    "    ax.imshow(img, cmap='gray')\n",
    "    color = 'green' if pred == true else 'red'\n",
    "    ax.set_title(f'Pred: {pred} ({prob:.1%})\\nTrue: {true}', color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混淆矩陣\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# 收集所有預測\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# 計算混淆矩陣\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# 找出最容易混淆的數字\n",
    "print(\"\\n最容易混淆的數字對：\")\n",
    "cm_no_diag = cm.copy()\n",
    "np.fill_diagonal(cm_no_diag, 0)\n",
    "for _ in range(3):\n",
    "    i, j = np.unravel_index(cm_no_diag.argmax(), cm_no_diag.shape)\n",
    "    print(f\"  真實 {i} 被誤判為 {j}: {cm_no_diag[i, j]} 次\")\n",
    "    cm_no_diag[i, j] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 練習題（已完成，請閱讀理解）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 1：比較不同激活函數\n",
    "\n",
    "**目標：** 觀察不同激活函數對 MNIST 訓練的影響\n",
    "\n",
    "**Hint：**\n",
    "- ReLU 是最常用的選擇\n",
    "- GELU 和 SiLU 在某些情況下表現更好\n",
    "- Sigmoid 通常不用在隱藏層（梯度消失問題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 1：比較不同激活函數\n",
    "\n",
    "class MLP_WithActivation(nn.Module):\n",
    "    def __init__(self, activation_fn):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.activation = activation_fn\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def quick_train(model, epochs=5):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        _, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "        history.append(test_acc)\n",
    "    \n",
    "    return history\n",
    "\n",
    "# 比較不同激活函數\n",
    "activations = {\n",
    "    'ReLU': nn.ReLU(),\n",
    "    'LeakyReLU': nn.LeakyReLU(0.1),\n",
    "    'GELU': nn.GELU(),\n",
    "    'SiLU': nn.SiLU(),\n",
    "    'Tanh': nn.Tanh(),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, act_fn in activations.items():\n",
    "    print(f\"Training with {name}...\")\n",
    "    torch.manual_seed(42)\n",
    "    model = MLP_WithActivation(act_fn)\n",
    "    results[name] = quick_train(model, epochs=5)\n",
    "\n",
    "# 視覺化\n",
    "plt.figure(figsize=(10, 5))\n",
    "for name, accs in results.items():\n",
    "    plt.plot(range(1, 6), accs, 'o-', label=f'{name} ({accs[-1]:.1f}%)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('Comparison of Activation Functions on MNIST')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 2：學習率調度 (Learning Rate Scheduling)\n",
    "\n",
    "**目標：** 學會使用學習率調度器\n",
    "\n",
    "**Hint：**\n",
    "- 訓練初期用較大學習率，後期減小\n",
    "- 常用：StepLR, CosineAnnealingLR, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 2：學習率調度\n",
    "\n",
    "# 重新建立模型\n",
    "torch.manual_seed(42)\n",
    "model_with_scheduler = MNIST_MLP().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_with_scheduler.parameters(), lr=0.01)  # 較大的初始學習率\n",
    "\n",
    "# 學習率調度器：每 3 個 epoch 把學習率乘以 0.5\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "history_scheduler = {'test_acc': [], 'lr': []}\n",
    "\n",
    "print(\"Training with LR Scheduler...\")\n",
    "for epoch in range(10):\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # 訓練\n",
    "    model_with_scheduler.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_with_scheduler(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 更新學習率\n",
    "    scheduler.step()\n",
    "    \n",
    "    # 評估\n",
    "    _, test_acc = evaluate(model_with_scheduler, test_loader, criterion, device)\n",
    "    history_scheduler['test_acc'].append(test_acc)\n",
    "    history_scheduler['lr'].append(current_lr)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: LR = {current_lr:.6f}, Test Acc = {test_acc:.2f}%\")\n",
    "\n",
    "# 視覺化\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.plot(history_scheduler['lr'], 'o-')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('Learning Rate Schedule')\n",
    "ax.grid(True)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.plot(history_scheduler['test_acc'], 'o-')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Test Accuracy (%)')\n",
    "ax.set_title('Test Accuracy')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 3：早停 (Early Stopping)\n",
    "\n",
    "**目標：** 實現早停機制，防止過擬合\n",
    "\n",
    "**Hint：**\n",
    "- 監控驗證集的 loss 或 accuracy\n",
    "- 如果連續 N 個 epoch 沒有改善，就停止訓練\n",
    "- 保存最佳模型的參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 3：早停\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"早停機制\"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict().copy()\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict().copy()\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop\n",
    "\n",
    "# 訓練帶早停\n",
    "torch.manual_seed(42)\n",
    "model_early_stop = MNIST_MLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_early_stop.parameters(), lr=0.001)\n",
    "early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "history_es = {'train_loss': [], 'test_loss': []}\n",
    "\n",
    "print(\"Training with Early Stopping (patience=3)...\")\n",
    "for epoch in range(50):  # 最多 50 個 epoch\n",
    "    train_loss, _ = train_epoch(model_early_stop, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model_early_stop, test_loader, criterion, device)\n",
    "    \n",
    "    history_es['train_loss'].append(train_loss)\n",
    "    history_es['test_loss'].append(test_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Test Loss = {test_loss:.4f}, Test Acc = {test_acc:.2f}%\")\n",
    "    \n",
    "    if early_stopping(test_loss, model_early_stop):\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}!\")\n",
    "        break\n",
    "\n",
    "# 載入最佳模型\n",
    "model_early_stop.load_state_dict(early_stopping.best_model_state)\n",
    "_, final_acc = evaluate(model_early_stop, test_loader, criterion, device)\n",
    "print(f\"\\nBest model Test Accuracy: {final_acc:.2f}%\")\n",
    "\n",
    "# 視覺化\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history_es['train_loss'], label='Train Loss')\n",
    "plt.plot(history_es['test_loss'], label='Test Loss')\n",
    "plt.axvline(x=len(history_es['test_loss'])-early_stopping.patience-1, \n",
    "            color='r', linestyle='--', label='Best Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training with Early Stopping')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習 4：模型保存與載入\n",
    "\n",
    "**目標：** 學會保存和載入訓練好的模型\n",
    "\n",
    "**Hint：**\n",
    "- `torch.save(model.state_dict(), path)`：保存參數\n",
    "- `model.load_state_dict(torch.load(path))`：載入參數\n",
    "- 也可以保存整個模型，但不推薦（pickle 問題）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 練習 4：模型保存與載入\n",
    "\n",
    "import os\n",
    "\n",
    "# 建立目錄\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# 方法 1：只保存參數（推薦）\n",
    "save_path = 'models/mnist_mlp.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "print(f\"File size: {os.path.getsize(save_path) / 1024:.1f} KB\")\n",
    "\n",
    "# 載入參數\n",
    "new_model = MNIST_MLP().to(device)\n",
    "new_model.load_state_dict(torch.load(save_path))\n",
    "new_model.eval()\n",
    "\n",
    "# 驗證載入成功\n",
    "_, loaded_acc = evaluate(new_model, test_loader, criterion, device)\n",
    "print(f\"Loaded model Test Accuracy: {loaded_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法 2：保存完整的 checkpoint（包含 optimizer 狀態，可以繼續訓練）\n",
    "\n",
    "checkpoint_path = 'models/mnist_checkpoint.pth'\n",
    "\n",
    "checkpoint = {\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_loss': history['train_loss'][-1],\n",
    "    'test_acc': history['test_acc'][-1],\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "# 載入 checkpoint 並繼續訓練\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "resume_model = MNIST_MLP().to(device)\n",
    "resume_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "resume_optimizer = optim.Adam(resume_model.parameters())\n",
    "resume_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "print(f\"\\nResumed from epoch {checkpoint['epoch']}\")\n",
    "print(f\"Last train loss: {checkpoint['train_loss']:.4f}\")\n",
    "print(f\"Last test accuracy: {checkpoint['test_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Module 2 中場總結\n\n### 核心概念\n\n1. **MLP 結構**：Input → [Linear + Activation] × N → Output\n\n2. **激活函數**：\n   - ReLU：最常用，計算快，緩解梯度消失\n   - GELU/SiLU：Transformer 常用，效果通常更好\n   - Sigmoid/Tanh：輸出層特定用途\n\n3. **損失函數**：\n   - 回歸：MSELoss\n   - 分類：CrossEntropyLoss\n\n4. **優化器**：\n   - SGD：基礎，加 momentum 效果更好\n   - Adam：最常用，自動調整學習率\n\n5. **正則化**：\n   - Dropout：隨機關閉神經元\n   - BatchNorm：正規化每層輸出，加速訓練\n   - L2/Weight Decay：懲罰大權重\n   - Early Stopping：監控驗證集，及時停止"
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Part 7：進階訓練技巧\n\n### 7.1 權重初始化（Weight Initialization）\n\n**為什麼重要？** 好的初始化可以：\n- 避免梯度消失/爆炸\n- 加速收斂\n- 達到更好的最終效果\n\n**常用方法：**\n- **Xavier/Glorot**：適合 Sigmoid/Tanh（保持輸入輸出變異數相同）\n- **He/Kaiming**：適合 ReLU（考慮 ReLU 會把一半值變成 0）",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 權重初始化比較\n\nclass MLP_CustomInit(nn.Module):\n    def __init__(self, init_method='default'):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(784, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 10)\n        \n        # 自訂初始化\n        if init_method == 'xavier':\n            nn.init.xavier_uniform_(self.fc1.weight)\n            nn.init.xavier_uniform_(self.fc2.weight)\n            nn.init.xavier_uniform_(self.fc3.weight)\n        elif init_method == 'kaiming':\n            nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n            nn.init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n            nn.init.kaiming_uniform_(self.fc3.weight, nonlinearity='relu')\n        elif init_method == 'zeros':\n            nn.init.zeros_(self.fc1.weight)\n            nn.init.zeros_(self.fc2.weight)\n            nn.init.zeros_(self.fc3.weight)\n        \n        # Bias 通常初始化為 0\n        nn.init.zeros_(self.fc1.bias)\n        nn.init.zeros_(self.fc2.bias)\n        nn.init.zeros_(self.fc3.bias)\n    \n    def forward(self, x):\n        x = self.flatten(x)\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        return self.fc3(x)\n\n# 比較不同初始化方法\ninit_methods = ['default', 'xavier', 'kaiming', 'zeros']\ninit_results = {}\n\nprint(\"比較不同初始化方法（訓練 3 個 epoch）：\")\nprint(\"-\" * 50)\n\nfor method in init_methods:\n    torch.manual_seed(42)\n    model = MLP_CustomInit(init_method=method).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n    \n    accs = []\n    for epoch in range(3):\n        model.train()\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            loss = criterion(model(images), labels)\n            loss.backward()\n            optimizer.step()\n        \n        _, acc = evaluate(model, test_loader, criterion, device)\n        accs.append(acc)\n    \n    init_results[method] = accs\n    print(f\"{method:>10}: Epoch 1={accs[0]:.1f}%, Epoch 2={accs[1]:.1f}%, Epoch 3={accs[2]:.1f}%\")\n\nprint(\"\\n注意：zeros 初始化會導致對稱性問題，所有神經元學習相同的東西！\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.2 梯度裁剪（Gradient Clipping）\n\n**問題：** 梯度爆炸會導致訓練不穩定\n\n**解決方案：** 限制梯度的大小\n- `clip_grad_norm_`：限制梯度的整體 L2 範數\n- `clip_grad_value_`：限制每個梯度元素的值",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 梯度裁剪範例\n\ndef train_with_grad_clip(model, train_loader, max_norm=1.0, epochs=3):\n    \"\"\"帶梯度裁剪的訓練\"\"\"\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.CrossEntropyLoss()\n    \n    grad_norms = []\n    \n    for epoch in range(epochs):\n        model.train()\n        epoch_norms = []\n        \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            loss = criterion(model(images), labels)\n            loss.backward()\n            \n            # 計算裁剪前的梯度範數\n            total_norm = 0\n            for p in model.parameters():\n                if p.grad is not None:\n                    total_norm += p.grad.data.norm(2).item() ** 2\n            total_norm = total_norm ** 0.5\n            epoch_norms.append(total_norm)\n            \n            # 梯度裁剪！\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n            \n            optimizer.step()\n        \n        grad_norms.append(np.mean(epoch_norms))\n    \n    return grad_norms\n\n# 比較有無梯度裁剪\ntorch.manual_seed(42)\nmodel_no_clip = MNIST_MLP().to(device)\ntorch.manual_seed(42)\nmodel_with_clip = MNIST_MLP().to(device)\n\n# 用較大的學習率測試\nprint(\"測試梯度裁剪效果（使用較大學習率 0.01）：\")\n\n# 不裁剪\noptimizer1 = optim.Adam(model_no_clip.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()\n\nnorms_no_clip = []\nfor epoch in range(3):\n    model_no_clip.train()\n    epoch_norms = []\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer1.zero_grad()\n        loss = criterion(model_no_clip(images), labels)\n        loss.backward()\n        \n        total_norm = sum(p.grad.data.norm(2).item() ** 2 for p in model_no_clip.parameters() if p.grad is not None) ** 0.5\n        epoch_norms.append(total_norm)\n        \n        optimizer1.step()\n    norms_no_clip.append(np.mean(epoch_norms))\n\n# 有裁剪\noptimizer2 = optim.Adam(model_with_clip.parameters(), lr=0.01)\nnorms_with_clip = []\nfor epoch in range(3):\n    model_with_clip.train()\n    epoch_norms = []\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer2.zero_grad()\n        loss = criterion(model_with_clip(images), labels)\n        loss.backward()\n        \n        total_norm = sum(p.grad.data.norm(2).item() ** 2 for p in model_with_clip.parameters() if p.grad is not None) ** 0.5\n        epoch_norms.append(total_norm)\n        \n        # 梯度裁剪\n        torch.nn.utils.clip_grad_norm_(model_with_clip.parameters(), max_norm=1.0)\n        \n        optimizer2.step()\n    norms_with_clip.append(np.mean(epoch_norms))\n\nprint(f\"\\n無梯度裁剪 - 平均梯度範數: {norms_no_clip}\")\nprint(f\"有梯度裁剪 - 平均梯度範數: {norms_with_clip}\")\n\n_, acc_no_clip = evaluate(model_no_clip, test_loader, criterion, device)\n_, acc_with_clip = evaluate(model_with_clip, test_loader, criterion, device)\nprint(f\"\\n最終準確率 - 無裁剪: {acc_no_clip:.2f}%, 有裁剪: {acc_with_clip:.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.3 學習率尋找器（Learning Rate Finder）\n\n**思路：** 從很小的學習率開始，逐漸增大，記錄 loss 變化。\n- Loss 下降最快的區間 = 好的學習率範圍\n- Loss 開始上升 = 學習率太大了",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# 簡單的學習率尋找器\n\ndef lr_finder(model_class, train_loader, lr_min=1e-7, lr_max=1, num_iter=100):\n    \"\"\"\n    學習率尋找器\n    \"\"\"\n    model = model_class().to(device)\n    optimizer = optim.SGD(model.parameters(), lr=lr_min)\n    criterion = nn.CrossEntropyLoss()\n    \n    # 指數增長的學習率\n    lr_mult = (lr_max / lr_min) ** (1 / num_iter)\n    \n    lrs = []\n    losses = []\n    best_loss = float('inf')\n    \n    model.train()\n    data_iter = iter(train_loader)\n    \n    for i in range(num_iter):\n        try:\n            images, labels = next(data_iter)\n        except StopIteration:\n            data_iter = iter(train_loader)\n            images, labels = next(data_iter)\n        \n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # 如果 loss 爆炸就停止\n        if loss.item() > best_loss * 10:\n            break\n        \n        if loss.item() < best_loss:\n            best_loss = loss.item()\n        \n        lrs.append(optimizer.param_groups[0]['lr'])\n        losses.append(loss.item())\n        \n        loss.backward()\n        optimizer.step()\n        \n        # 增加學習率\n        for param_group in optimizer.param_groups:\n            param_group['lr'] *= lr_mult\n    \n    return lrs, losses\n\n# 執行 LR Finder\nprint(\"執行學習率尋找器...\")\nlrs, losses = lr_finder(MNIST_MLP, train_loader)\n\n# 平滑 loss\ndef smooth(values, weight=0.9):\n    smoothed = []\n    last = values[0]\n    for v in values:\n        smoothed.append(last * weight + v * (1 - weight))\n        last = smoothed[-1]\n    return smoothed\n\nsmoothed_losses = smooth(losses)\n\n# 視覺化\nplt.figure(figsize=(10, 4))\nplt.plot(lrs, smoothed_losses)\nplt.xscale('log')\nplt.xlabel('Learning Rate')\nplt.ylabel('Loss')\nplt.title('Learning Rate Finder')\nplt.grid(True)\n\n# 找出建議的學習率（loss 下降最快的點）\nmin_loss_idx = np.argmin(smoothed_losses)\nsuggested_lr = lrs[min_loss_idx] / 10  # 通常取最低點的 1/10\nplt.axvline(x=suggested_lr, color='r', linestyle='--', label=f'Suggested LR: {suggested_lr:.1e}')\nplt.legend()\nplt.show()\n\nprint(f\"\\n建議的學習率: {suggested_lr:.1e}\")\nprint(\"提示: 選擇 loss 開始快速下降的點，通常是最低點學習率的 1/10\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.4 Label Smoothing（標籤平滑）\n\n**概念：** 不要用 hard labels (0 或 1)，而是用 soft labels\n\n例如：`[0, 0, 1, 0]` → `[0.025, 0.025, 0.925, 0.025]`\n\n**效果：** 正則化，防止模型過度自信",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Label Smoothing 範例\n\n# PyTorch 的 CrossEntropyLoss 內建 label_smoothing 參數\ncriterion_smooth = nn.CrossEntropyLoss(label_smoothing=0.1)\ncriterion_hard = nn.CrossEntropyLoss()\n\n# 測試\nlogits = torch.tensor([[2.0, 1.0, 0.5]])\ntarget = torch.tensor([0])\n\nloss_hard = criterion_hard(logits, target)\nloss_smooth = criterion_smooth(logits, target)\n\nprint(f\"Hard labels loss: {loss_hard.item():.4f}\")\nprint(f\"Smooth labels (0.1) loss: {loss_smooth.item():.4f}\")\n\n# 訓練比較\ntorch.manual_seed(42)\nmodel_hard = MNIST_MLP().to(device)\ntorch.manual_seed(42)\nmodel_smooth = MNIST_MLP().to(device)\n\ndef quick_train_with_criterion(model, criterion, epochs=5):\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    accs = []\n    for epoch in range(epochs):\n        model.train()\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            loss = criterion(model(images), labels)\n            loss.backward()\n            optimizer.step()\n        _, acc = evaluate(model, test_loader, nn.CrossEntropyLoss(), device)\n        accs.append(acc)\n    return accs\n\nprint(\"\\n訓練 5 個 epoch 比較：\")\nacc_hard = quick_train_with_criterion(model_hard, criterion_hard)\nacc_smooth = quick_train_with_criterion(model_smooth, criterion_smooth)\n\nprint(f\"Hard labels: {acc_hard[-1]:.2f}%\")\nprint(f\"Label smoothing (0.1): {acc_smooth[-1]:.2f}%\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 清理臨時檔案\nimport shutil\nif os.path.exists('models'):\n    shutil.rmtree('models')\n    print(\"已清理臨時模型檔案\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## 完整總結與實戰 Checklist\n\n### 本 Module 涵蓋的內容：\n\n| 主題 | 技術 | 使用時機 |\n|------|------|----------|\n| **激活函數** | ReLU, GELU, SiLU | 隱藏層非線性 |\n| **損失函數** | MSE, CrossEntropy | 回歸/分類任務 |\n| **優化器** | SGD, Adam, AdamW | 參數更新 |\n| **正則化** | Dropout, BatchNorm, L2 | 防止過擬合 |\n| **初始化** | Xavier, Kaiming | 加速收斂 |\n| **梯度裁剪** | clip_grad_norm_ | 訓練穩定性 |\n| **學習率調度** | StepLR, CosineAnnealing | 動態調整 LR |\n| **早停** | EarlyStopping | 防止過擬合 |\n| **標籤平滑** | label_smoothing | 正則化 |\n\n### 實戰 Checklist：\n\n- [ ] 模型定義：Flatten → Linear + Activation + Dropout/BN → Output\n- [ ] 訓練循環：zero_grad → forward → loss → backward → step\n- [ ] 監控過擬合：比較 train_loss 和 test_loss\n- [ ] 使用 BatchNorm 加速收斂\n- [ ] 使用 Dropout 防止過擬合\n- [ ] 嘗試 Learning Rate Finder 找最佳 LR\n- [ ] 實現 Early Stopping 自動停止\n- [ ] 保存最佳模型的 checkpoint\n\n### 訓練問題排查指南：\n\n| 現象 | 可能原因 | 解決方案 |\n|------|----------|----------|\n| Loss 不下降 | LR 太小/太大 | 調整 LR 或用 LR Finder |\n| Loss 變 NaN | LR 太大/數值問題 | 降低 LR、梯度裁剪 |\n| 訓練 loss 下降但測試 loss 上升 | 過擬合 | 增加 Dropout、早停 |\n| 訓練很慢 | 沒用 GPU/LR 太小 | 確認 device、增加 LR |\n| 準確率卡住不動 | 學習率太大 | 降低 LR 或用調度器 |\n\n### 下一步：Module 3 - CNN 與影像任務",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}