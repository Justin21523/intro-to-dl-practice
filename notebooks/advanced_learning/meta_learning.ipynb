{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å…ƒå­¸ç¿’ (Meta Learning)\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "- ç†è§£ã€Œå­¸ç¿’å¦‚ä½•å­¸ç¿’ã€çš„æ¦‚å¿µ\n",
    "- å¯¦ä½œ MAML (Model-Agnostic Meta-Learning)\n",
    "- å­¸ç¿’ Few-shot Learning\n",
    "- æŒæ¡ Prototypical Networks\n",
    "\n",
    "## å°æ‡‰èª²ç¨‹\n",
    "- [æå®æ¯… ML 2021 - Meta Learning](https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php)\n",
    "- [HW15: Meta Learning](https://github.com/ga642381/ML2021-Spring/tree/main/HW15)\n",
    "\n",
    "## å…ƒå­¸ç¿’æ¦‚è¿°\n",
    "\n",
    "```\n",
    "å‚³çµ±æ©Ÿå™¨å­¸ç¿’ vs å…ƒå­¸ç¿’\n",
    "â”œâ”€â”€ å‚³çµ±: å­¸ç¿’å–®ä¸€ä»»å‹™\n",
    "â”‚   è¨“ç·´: å¤§é‡è©²ä»»å‹™è³‡æ–™\n",
    "â”‚   ç›®æ¨™: è§£æ±ºç‰¹å®šå•é¡Œ\n",
    "â”‚\n",
    "â””â”€â”€ å…ƒå­¸ç¿’: å­¸ç¿’å¦‚ä½•å­¸ç¿’\n",
    "    è¨“ç·´: å¤§é‡ä¸åŒä»»å‹™\n",
    "    ç›®æ¨™: å¿«é€Ÿé©æ‡‰æ–°ä»»å‹™\n",
    "\n",
    "ä¸»è¦æ–¹æ³•:\n",
    "â”œâ”€â”€ Metric-basedï¼ˆåº¦é‡å­¸ç¿’ï¼‰\n",
    "â”‚   Siamese Networks, Prototypical Networks\n",
    "â”œâ”€â”€ Model-basedï¼ˆæ¨¡å‹å­¸ç¿’ï¼‰\n",
    "â”‚   MANN, MetaNet\n",
    "â””â”€â”€ Optimization-basedï¼ˆå„ªåŒ–å­¸ç¿’ï¼‰\n",
    "    MAML, Reptile\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ä½¿ç”¨è¨­å‚™: {device}\")\n",
    "\n",
    "# å›ºå®šéš¨æ©Ÿç¨®å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Few-shot Learning è¨­å®š\n",
    "\n",
    "### N-way K-shot\n",
    "\n",
    "- **N-way**: N å€‹é¡åˆ¥\n",
    "- **K-shot**: æ¯é¡ K å€‹è¨“ç·´æ¨£æœ¬\n",
    "- **Support Set**: ç”¨æ–¼å­¸ç¿’çš„å°‘é‡æ¨£æœ¬\n",
    "- **Query Set**: ç”¨æ–¼è©•ä¼°çš„æ¸¬è©¦æ¨£æœ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Few-shot è³‡æ–™é›† ==========\n",
    "\n",
    "class FewShotDataset(Dataset):\n",
    "    \"\"\"\n",
    "    æ¨¡æ“¬ Few-shot è³‡æ–™é›†\n",
    "    \n",
    "    ç”Ÿæˆå¤šå€‹ä»»å‹™ï¼Œæ¯å€‹ä»»å‹™æœ‰ N å€‹é¡åˆ¥ï¼Œæ¯é¡ K+Q å€‹æ¨£æœ¬\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_tasks=1000, n_classes=100, samples_per_class=20, feat_dim=64):\n",
    "        self.n_tasks = n_tasks\n",
    "        self.n_classes = n_classes\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.feat_dim = feat_dim\n",
    "        \n",
    "        # ç”Ÿæˆæ¯å€‹é¡åˆ¥çš„åŸå‹ï¼ˆé¡ä¸­å¿ƒï¼‰\n",
    "        self.class_prototypes = torch.randn(n_classes, feat_dim) * 2\n",
    "        \n",
    "        # ç”Ÿæˆæ¯å€‹é¡åˆ¥çš„æ¨£æœ¬ï¼ˆåœ¨åŸå‹å‘¨åœï¼‰\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for c in range(n_classes):\n",
    "            samples = self.class_prototypes[c] + torch.randn(samples_per_class, feat_dim) * 0.5\n",
    "            self.data.append(samples)\n",
    "            self.labels.extend([c] * samples_per_class)\n",
    "        \n",
    "        self.data = torch.cat(self.data, dim=0)\n",
    "        self.labels = torch.tensor(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_tasks\n",
    "    \n",
    "    def sample_task(self, n_way, k_shot, q_query):\n",
    "        \"\"\"\n",
    "        æ¡æ¨£ä¸€å€‹ N-way K-shot ä»»å‹™\n",
    "        \n",
    "        Returns:\n",
    "            support_x: (n_way * k_shot, feat_dim)\n",
    "            support_y: (n_way * k_shot,)\n",
    "            query_x: (n_way * q_query, feat_dim)\n",
    "            query_y: (n_way * q_query,)\n",
    "        \"\"\"\n",
    "        # éš¨æ©Ÿé¸æ“‡ N å€‹é¡åˆ¥\n",
    "        selected_classes = np.random.choice(self.n_classes, n_way, replace=False)\n",
    "        \n",
    "        support_x, support_y = [], []\n",
    "        query_x, query_y = [], []\n",
    "        \n",
    "        for i, c in enumerate(selected_classes):\n",
    "            # ç²å–è©²é¡åˆ¥çš„æ‰€æœ‰æ¨£æœ¬\n",
    "            class_indices = (self.labels == c).nonzero().squeeze()\n",
    "            perm = torch.randperm(len(class_indices))\n",
    "            \n",
    "            # åˆ†å‰² support å’Œ query\n",
    "            support_indices = class_indices[perm[:k_shot]]\n",
    "            query_indices = class_indices[perm[k_shot:k_shot + q_query]]\n",
    "            \n",
    "            support_x.append(self.data[support_indices])\n",
    "            support_y.extend([i] * k_shot)  # ä½¿ç”¨ç›¸å°æ¨™ç±¤ 0~N-1\n",
    "            \n",
    "            query_x.append(self.data[query_indices])\n",
    "            query_y.extend([i] * q_query)\n",
    "        \n",
    "        support_x = torch.cat(support_x, dim=0)\n",
    "        support_y = torch.tensor(support_y)\n",
    "        query_x = torch.cat(query_x, dim=0)\n",
    "        query_y = torch.tensor(query_y)\n",
    "        \n",
    "        return support_x, support_y, query_x, query_y\n",
    "\n",
    "# å»ºç«‹è³‡æ–™é›†\n",
    "dataset = FewShotDataset(n_tasks=1000, n_classes=100, samples_per_class=20)\n",
    "\n",
    "# æ¸¬è©¦æ¡æ¨£\n",
    "support_x, support_y, query_x, query_y = dataset.sample_task(n_way=5, k_shot=5, q_query=15)\n",
    "\n",
    "print(f\"5-way 5-shot ä»»å‹™:\")\n",
    "print(f\"  Support set: {support_x.shape}\")\n",
    "print(f\"  Query set: {query_x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Prototypical Networks\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "1. å­¸ç¿’ä¸€å€‹åµŒå…¥ç©ºé–“\n",
    "2. è¨ˆç®—æ¯å€‹é¡åˆ¥çš„ã€ŒåŸå‹ã€ï¼ˆsupport set æ¨£æœ¬çš„å‡å€¼ï¼‰\n",
    "3. å°‡ query æ¨£æœ¬åˆ†é¡åˆ°æœ€è¿‘çš„åŸå‹\n",
    "\n",
    "$$c_k = \\frac{1}{|S_k|} \\sum_{(x_i, y_i) \\in S_k} f_\\theta(x_i)$$\n",
    "\n",
    "$$p(y=k|x) = \\frac{\\exp(-d(f_\\theta(x), c_k))}{\\sum_{k'} \\exp(-d(f_\\theta(x), c_{k'}))}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Prototypical Networks ==========\n",
    "\n",
    "class ProtoNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Prototypical Networks\n",
    "    \n",
    "    åµŒå…¥ç¶²è·¯ + åŸå‹åˆ†é¡\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def compute_prototypes(self, support_x, support_y, n_way):\n",
    "        \"\"\"\n",
    "        è¨ˆç®—æ¯å€‹é¡åˆ¥çš„åŸå‹\n",
    "        \"\"\"\n",
    "        embeddings = self.forward(support_x)\n",
    "        \n",
    "        prototypes = []\n",
    "        for c in range(n_way):\n",
    "            mask = (support_y == c)\n",
    "            class_embeddings = embeddings[mask]\n",
    "            prototype = class_embeddings.mean(dim=0)\n",
    "            prototypes.append(prototype)\n",
    "        \n",
    "        return torch.stack(prototypes)  # (n_way, output_dim)\n",
    "    \n",
    "    def classify(self, query_x, prototypes):\n",
    "        \"\"\"\n",
    "        æ ¹æ“šåŸå‹åˆ†é¡ query æ¨£æœ¬\n",
    "        \"\"\"\n",
    "        query_embeddings = self.forward(query_x)  # (n_query, output_dim)\n",
    "        \n",
    "        # è¨ˆç®—æ­å¼è·é›¢\n",
    "        # query: (n_query, 1, output_dim)\n",
    "        # prototypes: (1, n_way, output_dim)\n",
    "        dists = torch.cdist(query_embeddings, prototypes)  # (n_query, n_way)\n",
    "        \n",
    "        # è² è·é›¢ä½œç‚º logits\n",
    "        logits = -dists\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# æ¸¬è©¦\n",
    "protonet = ProtoNet(input_dim=64, hidden_dim=64, output_dim=32)\n",
    "print(f\"ProtoNet åƒæ•¸é‡: {sum(p.numel() for p in protonet.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è¨“ç·´ Prototypical Networks ==========\n",
    "\n",
    "def train_protonet(model, dataset, n_episodes=1000, n_way=5, k_shot=5, q_query=15, lr=1e-3):\n",
    "    \"\"\"\n",
    "    è¨“ç·´ Prototypical Networks\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        # æ¡æ¨£ä»»å‹™\n",
    "        support_x, support_y, query_x, query_y = dataset.sample_task(n_way, k_shot, q_query)\n",
    "        support_x, support_y = support_x.to(device), support_y.to(device)\n",
    "        query_x, query_y = query_x.to(device), query_y.to(device)\n",
    "        \n",
    "        # è¨ˆç®—åŸå‹\n",
    "        model.train()\n",
    "        prototypes = model.compute_prototypes(support_x, support_y, n_way)\n",
    "        \n",
    "        # åˆ†é¡ query\n",
    "        logits = model.classify(query_x, prototypes)\n",
    "        \n",
    "        # è¨ˆç®—æå¤±\n",
    "        loss = F.cross_entropy(logits, query_y)\n",
    "        \n",
    "        # æ›´æ–°\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # è¨ˆç®—æº–ç¢ºç‡\n",
    "        acc = (logits.argmax(dim=1) == query_y).float().mean().item()\n",
    "        \n",
    "        history['loss'].append(loss.item())\n",
    "        history['acc'].append(acc)\n",
    "        \n",
    "        if (episode + 1) % 200 == 0:\n",
    "            avg_loss = np.mean(history['loss'][-100:])\n",
    "            avg_acc = np.mean(history['acc'][-100:])\n",
    "            print(f\"Episode {episode+1}: Loss={avg_loss:.4f}, Acc={avg_acc:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# è¨“ç·´\n",
    "protonet = ProtoNet(input_dim=64, hidden_dim=64, output_dim=32)\n",
    "protonet, proto_history = train_protonet(protonet, dataset, n_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== å¯è¦–åŒ– ==========\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# æå¤±æ›²ç·š\n",
    "window = 50\n",
    "smoothed_loss = np.convolve(proto_history['loss'], np.ones(window)/window, mode='valid')\n",
    "smoothed_acc = np.convolve(proto_history['acc'], np.ones(window)/window, mode='valid')\n",
    "\n",
    "axes[0].plot(smoothed_loss)\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('ProtoNet Training Loss')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(smoothed_acc)\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('ProtoNet Training Accuracy')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"æœ€çµ‚æº–ç¢ºç‡: {np.mean(proto_history['acc'][-100:]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: MAML (Model-Agnostic Meta-Learning)\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "å­¸ç¿’ä¸€å€‹å¥½çš„åˆå§‹åŒ–åƒæ•¸ $\\theta$ï¼Œä½¿å¾—å¾é€™å€‹åˆå§‹åŒ–å‡ºç™¼ï¼Œåªéœ€å°‘é‡æ¢¯åº¦æ›´æ–°å°±èƒ½å¿«é€Ÿé©æ‡‰æ–°ä»»å‹™ã€‚\n",
    "\n",
    "### æ¼”ç®—æ³•\n",
    "\n",
    "1. **Inner Loop**: åœ¨æ¯å€‹ä»»å‹™ä¸Šç”¨ support set æ›´æ–°åƒæ•¸\n",
    "   $$\\theta'_i = \\theta - \\alpha \\nabla_\\theta L_{S_i}(\\theta)$$\n",
    "\n",
    "2. **Outer Loop**: ç”¨ query set ä¸Šçš„æå¤±æ›´æ–°åˆå§‹åƒæ•¸\n",
    "   $$\\theta = \\theta - \\beta \\nabla_\\theta \\sum_i L_{Q_i}(\\theta'_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== MAML æ¨¡å‹ ==========\n",
    "\n",
    "class MAMLModel(nn.Module):\n",
    "    \"\"\"\n",
    "    ç”¨æ–¼ MAML çš„ç°¡å–®ç¥ç¶“ç¶²è·¯\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, params=None):\n",
    "        \"\"\"\n",
    "        å‰å‘å‚³æ’­\n",
    "        \n",
    "        å¦‚æœæä¾› paramsï¼Œä½¿ç”¨é€™äº›åƒæ•¸ï¼›å¦å‰‡ä½¿ç”¨æ¨¡å‹è‡ªèº«åƒæ•¸\n",
    "        \"\"\"\n",
    "        if params is None:\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "        else:\n",
    "            x = F.relu(F.linear(x, params['fc1.weight'], params['fc1.bias']))\n",
    "            x = F.relu(F.linear(x, params['fc2.weight'], params['fc2.bias']))\n",
    "            x = F.linear(x, params['fc3.weight'], params['fc3.bias'])\n",
    "        return x\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"ç²å–åƒæ•¸å­—å…¸\"\"\"\n",
    "        return {name: param.clone() for name, param in self.named_parameters()}\n",
    "\n",
    "# æ¸¬è©¦\n",
    "maml_model = MAMLModel(input_dim=64, hidden_dim=64, output_dim=5)\n",
    "print(f\"MAML æ¨¡å‹åƒæ•¸é‡: {sum(p.numel() for p in maml_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== MAML è¨“ç·´ ==========\n",
    "\n",
    "class MAML:\n",
    "    \"\"\"\n",
    "    MAML æ¼”ç®—æ³•å¯¦ç¾\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, inner_lr=0.01, outer_lr=0.001, inner_steps=5):\n",
    "        self.model = model.to(device)\n",
    "        self.inner_lr = inner_lr\n",
    "        self.outer_lr = outer_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        \n",
    "        self.meta_optimizer = optim.Adam(self.model.parameters(), lr=outer_lr)\n",
    "    \n",
    "    def inner_update(self, support_x, support_y, params):\n",
    "        \"\"\"\n",
    "        Inner Loop: åœ¨ support set ä¸Šæ›´æ–°åƒæ•¸\n",
    "        \"\"\"\n",
    "        for _ in range(self.inner_steps):\n",
    "            logits = self.model(support_x, params)\n",
    "            loss = F.cross_entropy(logits, support_y)\n",
    "            \n",
    "            # è¨ˆç®—æ¢¯åº¦\n",
    "            grads = torch.autograd.grad(loss, params.values(), create_graph=True)\n",
    "            \n",
    "            # æ›´æ–°åƒæ•¸\n",
    "            params = {\n",
    "                name: param - self.inner_lr * grad\n",
    "                for (name, param), grad in zip(params.items(), grads)\n",
    "            }\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def train_step(self, tasks):\n",
    "        \"\"\"\n",
    "        ä¸€å€‹ meta-training step\n",
    "        \n",
    "        Args:\n",
    "            tasks: list of (support_x, support_y, query_x, query_y)\n",
    "        \"\"\"\n",
    "        meta_loss = 0\n",
    "        meta_acc = 0\n",
    "        \n",
    "        for support_x, support_y, query_x, query_y in tasks:\n",
    "            support_x, support_y = support_x.to(device), support_y.to(device)\n",
    "            query_x, query_y = query_x.to(device), query_y.to(device)\n",
    "            \n",
    "            # Inner Loop\n",
    "            params = self.model.get_params()\n",
    "            adapted_params = self.inner_update(support_x, support_y, params)\n",
    "            \n",
    "            # Outer Loop: è¨ˆç®— query set ä¸Šçš„æå¤±\n",
    "            query_logits = self.model(query_x, adapted_params)\n",
    "            task_loss = F.cross_entropy(query_logits, query_y)\n",
    "            \n",
    "            meta_loss += task_loss\n",
    "            meta_acc += (query_logits.argmax(dim=1) == query_y).float().mean().item()\n",
    "        \n",
    "        # æ›´æ–°å…ƒåƒæ•¸\n",
    "        self.meta_optimizer.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        self.meta_optimizer.step()\n",
    "        \n",
    "        return meta_loss.item() / len(tasks), meta_acc / len(tasks)\n",
    "\n",
    "# å»ºç«‹ MAML\n",
    "maml_model = MAMLModel(input_dim=64, hidden_dim=64, output_dim=5)\n",
    "maml = MAML(maml_model, inner_lr=0.1, outer_lr=0.001, inner_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è¨“ç·´ MAML ==========\n",
    "\n",
    "def train_maml(maml, dataset, n_episodes=500, tasks_per_batch=4, n_way=5, k_shot=5, q_query=15):\n",
    "    \"\"\"\n",
    "    è¨“ç·´ MAML\n",
    "    \"\"\"\n",
    "    history = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        # æ¡æ¨£å¤šå€‹ä»»å‹™\n",
    "        tasks = [\n",
    "            dataset.sample_task(n_way, k_shot, q_query)\n",
    "            for _ in range(tasks_per_batch)\n",
    "        ]\n",
    "        \n",
    "        # è¨“ç·´ä¸€æ­¥\n",
    "        loss, acc = maml.train_step(tasks)\n",
    "        \n",
    "        history['loss'].append(loss)\n",
    "        history['acc'].append(acc)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_loss = np.mean(history['loss'][-50:])\n",
    "            avg_acc = np.mean(history['acc'][-50:])\n",
    "            print(f\"Episode {episode+1}: Loss={avg_loss:.4f}, Acc={avg_acc:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# è¨“ç·´\n",
    "maml_history = train_maml(maml, dataset, n_episodes=500, tasks_per_batch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== æ¯”è¼ƒ MAML vs ProtoNet ==========\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "window = 20\n",
    "\n",
    "# æå¤±æ¯”è¼ƒ\n",
    "maml_loss = np.convolve(maml_history['loss'], np.ones(window)/window, mode='valid')\n",
    "proto_loss = np.convolve(proto_history['loss'][:500], np.ones(window)/window, mode='valid')\n",
    "\n",
    "axes[0].plot(proto_loss, label='ProtoNet')\n",
    "axes[0].plot(maml_loss, label='MAML')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# æº–ç¢ºç‡æ¯”è¼ƒ\n",
    "maml_acc = np.convolve(maml_history['acc'], np.ones(window)/window, mode='valid')\n",
    "proto_acc = np.convolve(proto_history['acc'][:500], np.ones(window)/window, mode='valid')\n",
    "\n",
    "axes[1].plot(proto_acc, label='ProtoNet')\n",
    "axes[1].plot(maml_acc, label='MAML')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\næœ€çµ‚æº–ç¢ºç‡:\")\n",
    "print(f\"  ProtoNet: {np.mean(proto_history['acc'][-50:]):.4f}\")\n",
    "print(f\"  MAML: {np.mean(maml_history['acc'][-50:]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: ä¸åŒ shot æ•¸çš„æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== è©•ä¼°ä¸åŒ K-shot ==========\n",
    "\n",
    "def evaluate_model(model, dataset, n_way, k_shots, n_episodes=100):\n",
    "    \"\"\"\n",
    "    è©•ä¼°æ¨¡å‹åœ¨ä¸åŒ k-shot è¨­å®šä¸‹çš„è¡¨ç¾\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    for k in k_shots:\n",
    "        accs = []\n",
    "        \n",
    "        for _ in range(n_episodes):\n",
    "            support_x, support_y, query_x, query_y = dataset.sample_task(n_way, k, 15)\n",
    "            support_x, support_y = support_x.to(device), support_y.to(device)\n",
    "            query_x, query_y = query_x.to(device), query_y.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                prototypes = model.compute_prototypes(support_x, support_y, n_way)\n",
    "                logits = model.classify(query_x, prototypes)\n",
    "                acc = (logits.argmax(dim=1) == query_y).float().mean().item()\n",
    "                accs.append(acc)\n",
    "        \n",
    "        results[k] = {'mean': np.mean(accs), 'std': np.std(accs)}\n",
    "        print(f\"{k}-shot: {results[k]['mean']:.4f} Â± {results[k]['std']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# è©•ä¼°\n",
    "print(\"ProtoNet åœ¨ä¸åŒ k-shot ä¸‹çš„è¡¨ç¾:\")\n",
    "k_shot_results = evaluate_model(protonet, dataset, n_way=5, k_shots=[1, 3, 5, 10, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== å¯è¦–åŒ– k-shot æ•ˆæœ ==========\n",
    "\n",
    "k_values = list(k_shot_results.keys())\n",
    "means = [k_shot_results[k]['mean'] for k in k_values]\n",
    "stds = [k_shot_results[k]['std'] for k in k_values]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(k_values, means, yerr=stds, marker='o', capsize=5, linewidth=2, markersize=8)\n",
    "plt.xlabel('K (number of shots)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Few-shot Learning Performance vs Number of Shots')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¯ ç¸½çµ\n",
    "\n",
    "### å…ƒå­¸ç¿’æ–¹æ³•æ¯”è¼ƒ\n",
    "\n",
    "| æ–¹æ³• | å„ªé» | ç¼ºé» |\n",
    "|------|------|------|\n",
    "| **ProtoNet** | ç°¡å–®ã€é«˜æ•ˆã€ç©©å®š | ä¾è³´è·é›¢åº¦é‡ |\n",
    "| **MAML** | é€šç”¨ã€éˆæ´» | è¨ˆç®—è¤‡é›œã€äºŒéšæ¢¯åº¦ |\n",
    "| **Reptile** | ç°¡åŒ–ç‰ˆ MAML | æ•ˆæœç•¥å·® |\n",
    "\n",
    "### æå®æ¯… HW15 æŠ€å·§\n",
    "\n",
    "```\n",
    "Prototypical Networks:\n",
    "1. ä½¿ç”¨æ­å¼è·é›¢æˆ–é¤˜å¼¦è·é›¢\n",
    "2. é©ç•¶çš„åµŒå…¥ç¶­åº¦\n",
    "3. è¨“ç·´æ™‚ä½¿ç”¨æ›´å¤š way/shot\n",
    "\n",
    "MAML:\n",
    "1. é©ç•¶çš„ inner learning rate\n",
    "2. åˆé©çš„ inner stepsï¼ˆé€šå¸¸ 1-5ï¼‰\n",
    "3. å¯ä»¥ä½¿ç”¨ä¸€éšè¿‘ä¼¼åŠ é€Ÿ\n",
    "\n",
    "é€šç”¨æŠ€å·§:\n",
    "1. è³‡æ–™å¢å¼·\n",
    "2. Task æ¡æ¨£ç­–ç•¥\n",
    "3. Episode è¨“ç·´\n",
    "```\n",
    "\n",
    "### å…ƒå­¸ç¿’æ‡‰ç”¨\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ å°‘æ¨£æœ¬åœ–åƒåˆ†é¡\n",
    "â”œâ”€â”€ è—¥ç‰©ç™¼ç¾ï¼ˆå°‘é‡å¯¦é©—æ•¸æ“šï¼‰\n",
    "â”œâ”€â”€ æ©Ÿå™¨äººå¿«é€Ÿé©æ‡‰\n",
    "â”œâ”€â”€ å€‹æ€§åŒ–æ¨è–¦\n",
    "â””â”€â”€ è·¨èªè¨€ NLP\n",
    "```\n",
    "\n",
    "### å®Œæˆï¼\n",
    "\n",
    "æ­å–œå®Œæˆæ‰€æœ‰æ·±åº¦å­¸ç¿’æ•™ç¨‹ï¼å›åˆ°ä¸» README æŸ¥çœ‹å®Œæ•´å­¸ç¿’è·¯ç·šåœ–ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## ğŸ‹ï¸ ç·´ç¿’\n\n### ç·´ç¿’ 1: æ¯”è¼ƒä¸åŒ N-way è¨­å®šçš„å½±éŸ¿\n\næ¸¬è©¦ N-way K-shot ä¸­ Nï¼ˆé¡åˆ¥æ•¸é‡ï¼‰å°åˆ†é¡é›£åº¦çš„å½±éŸ¿ã€‚",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ========== ç·´ç¿’ 1: æ¯”è¼ƒä¸åŒ N-way è¨­å®š ==========\n\ndef evaluate_n_way(model, dataset, n_ways, k_shot=5, n_episodes=100):\n    \"\"\"\n    è©•ä¼°ä¸åŒ N-way è¨­å®šçš„æ•ˆæœ\n    \"\"\"\n    model.eval()\n    results = {}\n    \n    print(\"ç·´ç¿’ 1: æ¯”è¼ƒä¸åŒ N-way è¨­å®š\")\n    print(\"=\" * 50)\n    \n    for n_way in n_ways:\n        accs = []\n        \n        for _ in range(n_episodes):\n            support_x, support_y, query_x, query_y = dataset.sample_task(n_way, k_shot, 15)\n            support_x, support_y = support_x.to(device), support_y.to(device)\n            query_x, query_y = query_x.to(device), query_y.to(device)\n            \n            with torch.no_grad():\n                prototypes = model.compute_prototypes(support_x, support_y, n_way)\n                logits = model.classify(query_x, prototypes)\n                acc = (logits.argmax(dim=1) == query_y).float().mean().item()\n                accs.append(acc)\n        \n        results[n_way] = {'mean': np.mean(accs), 'std': np.std(accs)}\n        \n        # è¨ˆç®—éš¨æ©ŸçŒœæ¸¬çš„æº–ç¢ºç‡ä½œç‚ºåƒè€ƒ\n        random_acc = 1.0 / n_way\n        print(f\"{n_way:2d}-way: {results[n_way]['mean']:.4f} Â± {results[n_way]['std']:.4f} \"\n              f\"(éš¨æ©Ÿ: {random_acc:.4f}, æå‡: {results[n_way]['mean']/random_acc:.1f}x)\")\n    \n    return results\n\n# è©•ä¼°ä¸åŒ N-way\nn_way_results = evaluate_n_way(protonet, dataset, n_ways=[2, 3, 5, 10, 15, 20], k_shot=5)\n\n# è¦–è¦ºåŒ–\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nn_ways = list(n_way_results.keys())\nmeans = [n_way_results[n]['mean'] for n in n_ways]\nstds = [n_way_results[n]['std'] for n in n_ways]\nrandom_accs = [1.0 / n for n in n_ways]\n\n# æº–ç¢ºç‡ vs N-way\naxes[0].errorbar(n_ways, means, yerr=stds, marker='o', capsize=5, linewidth=2, markersize=8, label='ProtoNet')\naxes[0].plot(n_ways, random_accs, 'r--', marker='x', label='Random Guess')\naxes[0].set_xlabel('N (number of ways)')\naxes[0].set_ylabel('Accuracy')\naxes[0].set_title('Accuracy vs Number of Classes')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# ç›¸å°æå‡\nimprovements = [m / r for m, r in zip(means, random_accs)]\naxes[1].bar(range(len(n_ways)), improvements, color='steelblue')\naxes[1].set_xticks(range(len(n_ways)))\naxes[1].set_xticklabels([f'{n}-way' for n in n_ways])\naxes[1].set_ylabel('Improvement over Random')\naxes[1].set_title('Relative Performance Improvement')\naxes[1].axhline(y=1, color='r', linestyle='--', alpha=0.5)\n\nfor i, imp in enumerate(improvements):\n    axes[1].text(i, imp + 0.1, f'{imp:.1f}x', ha='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nçµè«–ï¼š\")\nprint(\"- N å¢åŠ æ™‚ï¼Œæº–ç¢ºç‡ä¸‹é™ï¼ˆä»»å‹™æ›´é›£ï¼‰\")\nprint(\"- ä½†ç›¸å°æ–¼éš¨æ©ŸçŒœæ¸¬çš„æå‡ä¿æŒç©©å®š\")\nprint(\"- é€™è¡¨æ˜æ¨¡å‹å­¸åˆ°äº†æœ‰æ„ç¾©çš„è¡¨ç¤º\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}